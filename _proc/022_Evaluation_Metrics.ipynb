{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "description: '[Evaluation Metrics](https://manalelaidouni.github.io/Evaluating-Object-Detection-Models-Guide-to-PerformancecMetrics.html)'\n",
    "output-file: evaluation_metrics.html\n",
    "skip_exec: true\n",
    "skip_showdoc: true\n",
    "title: Evaluation Metrics\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d36c93f-803d-4deb-9f68-987e4a6c6412",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb629fc0-8169-4a28-89a3-b731843b8509",
   "metadata": {},
   "source": [
    "- Accuracy : is the percentage of correctly predicted examples out of all predictions, formally known as\n",
    "\n",
    "\n",
    "$Accuracy = \\frac{TP + TN}{TP + FP + TN + FN}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cffda9-e5a4-4282-aa11-20cdbc606850",
   "metadata": {},
   "source": [
    "## Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f144102-72c1-415e-af2f-20ea27ae4025",
   "metadata": {},
   "source": [
    "- Precision is the the probability of the predicted bounding boxes matching actual ground truth boxes, also referred to as the positive predictive value.\n",
    "\n",
    "$Precision = \\frac{TP}{TP + FP} = \\frac{true \\ object \\ detection}{all \\ detected \\ boxes}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072bee40-3958-4c40-b395-00d196f99ad6",
   "metadata": {},
   "source": [
    "## Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a0e022-4aad-4d68-a2cb-7e6cf93f32b0",
   "metadata": {},
   "source": [
    "- Recall is the true positive rate, also referred to as sensitivity, measures the probability of ground truth objects being correctly detected.\n",
    "\n",
    "$Recall = \\frac{TP}{TP + FN} = \\frac{true \\ object \\ detection}{all \\ ground \\ truth \\ boxes}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca833c75-9617-4f7a-8ebc-ecdf25cc67c4",
   "metadata": {},
   "source": [
    "## AP - Average Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec42d8fc-322b-4414-a802-0f9e58674d5f",
   "metadata": {},
   "source": [
    "- it is a single number metric that encapsulates both precision and recall and summarizes the Precision-Recall curve by averaging precision across recall values from 0 to 1\n",
    "\n",
    "$AP = \\dfrac{1}{11} \\sum_{r \\in \\{0,0.1,0.2,...,1\\}} p_{interp}(r)$\n",
    "\n",
    "where\n",
    "\n",
    "$p_{interp}(r) = \\max p(\\hat{r})$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccc67b3-4c92-4a9a-a0f3-f58802902fed",
   "metadata": {},
   "source": [
    "## mAP (Mean Average Precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ece226d-b73e-4289-b8ab-462b5ff8f5e9",
   "metadata": {},
   "source": [
    "- count the accumulated TP and the accumulated FP and compute the precision/recall at each line. Average Precision is computed as the average precision at 11 equally spaced recall levels."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
