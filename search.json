[
  {
    "objectID": "model_training_loop.html",
    "href": "model_training_loop.html",
    "title": "Model Training Loop",
    "section": "",
    "text": "import torch\nx = torch.randn(3, requires_grad = True)\nprint(x)\n\ntensor([-1.2561, -3.5029, -1.4459], requires_grad=True)\ny = x + 2\ny.retain_grad()\nz = y * y*2\nz.retain_grad()\nz = z.mean()\nprint(z)\n\ntensor(2.0794, grad_fn=&lt;MeanBackward0&gt;)\nz.backward(retain_graph=True)\nx,y, z\n\n(tensor([-1.2561, -3.5029, -1.4459], requires_grad=True),\n tensor([ 0.7439, -1.5029,  0.5541], grad_fn=&lt;AddBackward0&gt;),\n tensor(2.0794, grad_fn=&lt;MeanBackward0&gt;))\nx.grad, y.grad\n\n(tensor([ 0.9919, -2.0039,  0.7388]), tensor([ 0.9919, -2.0039,  0.7388]))",
    "crumbs": [
      "Blog",
      "Model Training Loop"
    ]
  },
  {
    "objectID": "model_training_loop.html#for-multiple-z-values",
    "href": "model_training_loop.html#for-multiple-z-values",
    "title": "Model Training Loop",
    "section": "For multiple z values",
    "text": "For multiple z values\n\nimport torch\n\n\nx = torch.randn(3, requires_grad = True)\n\n\nprint(x)\n\ntensor([-1.3521, -0.5026, -0.7557], requires_grad=True)\n\n\n\ny = x + 2\ny.retain_grad()\n\n\nz = y * y*2\nz.retain_grad()\n#z = z.mean()\nprint(z)\n\ntensor([0.8396, 4.4845, 3.0964], grad_fn=&lt;MulBackward0&gt;)\n\n\n\nv = torch.tensor([0.1, 1.0, 0.001], dtype=torch.float32)\nz.backward(v, retain_graph=True)\n\n\nx, y, z\n\n(tensor([-1.3521, -0.5026, -0.7557], requires_grad=True),\n tensor([0.6479, 1.4974, 1.2443], grad_fn=&lt;AddBackward0&gt;),\n tensor([0.8396, 4.4845, 3.0964], grad_fn=&lt;MulBackward0&gt;))\n\n\n\nx.grad, y.grad\n\n(tensor([2.5917e-01, 5.9897e+00, 4.9771e-03]),\n tensor([2.5917e-01, 5.9897e+00, 4.9771e-03]))",
    "crumbs": [
      "Blog",
      "Model Training Loop"
    ]
  },
  {
    "objectID": "model_training_loop.html#stopping-gradient-descent",
    "href": "model_training_loop.html#stopping-gradient-descent",
    "title": "Model Training Loop",
    "section": "Stopping gradient descent",
    "text": "Stopping gradient descent\n\nx.requires_grad_(False)\nprint(x)\n\ntensor([-1.3521, -0.5026, -0.7557])\n\n\n\ny = x.detach()\nprint(y)\n\ntensor([-1.3521, -0.5026, -0.7557])\n\n\n\nwith torch.no_grad():\n    print(x)\n\ntensor([-1.3521, -0.5026, -0.7557])",
    "crumbs": [
      "Blog",
      "Model Training Loop"
    ]
  },
  {
    "objectID": "model_training_loop.html#zeroing-gradients",
    "href": "model_training_loop.html#zeroing-gradients",
    "title": "Model Training Loop",
    "section": "Zeroing Gradients",
    "text": "Zeroing Gradients\n\nweights = torch.ones(4, requires_grad=True)\n\nfor epoch in range(5):\n    model_output = (weights * 3).sum()\n\n    model_output.backward()\n\n    print(weights.grad)\n\ntensor([3., 3., 3., 3.])\ntensor([6., 6., 6., 6.])\ntensor([9., 9., 9., 9.])\ntensor([12., 12., 12., 12.])\ntensor([15., 15., 15., 15.])\n\n\n\nweights = torch.ones(4, requires_grad=True)\n\nfor epoch in range(5):\n    model_output = (weights * 3).sum()\n\n    model_output.backward()\n\n    print(weights.grad)\n\n    weights.grad.zero_()\n\ntensor([3., 3., 3., 3.])\ntensor([3., 3., 3., 3.])\ntensor([3., 3., 3., 3.])\ntensor([3., 3., 3., 3.])\ntensor([3., 3., 3., 3.])",
    "crumbs": [
      "Blog",
      "Model Training Loop"
    ]
  },
  {
    "objectID": "model_training_loop.html#backpropagation",
    "href": "model_training_loop.html#backpropagation",
    "title": "Model Training Loop",
    "section": "Backpropagation",
    "text": "Backpropagation\n\nweights = torch.ones(4, requires_grad=True)\n\n\npip list| grep nbdevAuto\n\nnbdevAuto                 0.0.130        /home/ben/BENEDICT_Only/Benedict_Projects/Benedict_ML/nbdevAuto\nNote: you may need to restart the kernel to use updated packages.\n\n\n\nfrom graphviz import Digraph\nfrom nbdevAuto import functions\n\n\ndot = functions.graph()\n# Add nodes with different shapes and formatting\ndot.node('x', 'x')\ndot.node('a', 'a(x)', shape='circle')\ndot.node('y', 'y')\ndot.node('b', 'b(y)', shape='circle')\ndot.node('z', 'z')\n\n# Add edges with custom labels and formatting\ndot.edge('x', 'a')\ndot.edge('a', 'y')\ndot.edge('y', 'b')\ndot.edge('b', 'z')\n\n# Render the graph\ndot\n\n\n\n\n\n\n\n\nChain rule\n\\(\\dfrac{\\delta z}{\\delta x} =  \\dfrac{\\delta z}{\\delta y} \\cdot \\dfrac{\\delta y}{\\delta x}\\)",
    "crumbs": [
      "Blog",
      "Model Training Loop"
    ]
  },
  {
    "objectID": "model_training_loop.html#computational-graph",
    "href": "model_training_loop.html#computational-graph",
    "title": "Model Training Loop",
    "section": "Computational Graph",
    "text": "Computational Graph\n\ndot = functions.graph()\n# Add nodes with different shapes and formatting\ndot.node('x', 'x')\ndot.node('*', 'f=x*y', shape='circle')\ndot.node('y', 'y')\ndot.node('z', 'z')\n\n# Add edges with custom labels and formatting\ndot.edge('x', '*')\ndot.edge('y', '*')\ndot.edge('*', 'z')\n\n# Render the graph\ndot\n\n\n\n\n\n\n\n\n\\(\\dfrac{\\delta z}{\\delta x}  = \\dfrac{\\delta xy}{\\delta x} = y\\)\n\\(\\dfrac{\\delta z}{\\delta y}  = \\dfrac{\\delta xy}{\\delta y} = y\\)\n$ = $\n\nForward pass: Computer loss\nCompute local gradients\nBackward pass: Compute dLoss/dWeights using the Chain Rule\n\n\ndot = functions.graph()\n# Add nodes with different shapes and formatting\ndot.node('x', 'x')\ndot.node('w', 'w')\ndot.node('*', '*\\ny1=w*y', shape='circle')\n\ndot.node('y', 'y')\ndot.node('-', '-\\ns= y1-y')\n\ndot.node('^2', '^2\\n(y1-y)^2')\ndot.node('Loss', 'Loss')\n# Add edges with custom labels and formatting\ndot.edge('x', '*')\ndot.edge('w', '*')\n\ndot.edge('*', '-', label='y1')\ndot.edge('y', '-')\n\ndot.edge('-', '^2', label='s')\n\ndot.edge('^2', 'Loss')\n# Render the graph\ndot\n\n\n\n\n\n\n\n\n\\(Loss = (\\hat{y} - y)^2\\)\n\\(\\dfrac{\\delta loss}{\\delta s} = \\dfrac{s^2}{s} = 2s\\)\n\\(\\dfrac{\\delta s}{\\delta \\hat{y}} = \\dfrac{\\delta\\hat{y} - y}{\\delta \\hat{y}} = 1\\)\n\\(\\dfrac{\\delta \\hat{y}}{\\delta w} = \\dfrac{\\delta wx}{\\delta w} = x\\)\n\\(\\therefore \\dfrac{\\delta loss}{\\delta w} = \\dfrac{\\delta loss}{\\delta s} \\cdot \\dfrac{\\delta s}{\\delta y} \\cdot  \\dfrac{\\delta \\hat{y}}{\\delta w} = 2 \\cdot s \\cdot x = 2 \\cdot (-1) \\cdot (1) = -2\\)\n\nx = 1\ny = 2\nw = 1\n\ny1 = x * w \ns = y1-y\nloss = s**2\n\n\nprint(f'x:{x} w:{w} y1:{y1} y:{y} s:{s} loss:{loss}')\n\nx:1 w:1 y1:1 y:2 s:-1 loss:1\n\n\n\nimport torch\nx = torch.tensor(1.0)\ny = torch.tensor(2.0)\n\n\nw = torch.tensor(1.0, requires_grad = True)\n\nlr = 0.005\n\n\n#forward pass and compute the loss\ny1 = w * x\nloss = (y1-y)**2\n\nprint(y1)\nprint(loss)\n\ntensor(1., grad_fn=&lt;MulBackward0&gt;)\ntensor(1., grad_fn=&lt;PowBackward0&gt;)\n\n\n\n# backward pass\n\nloss.backward()\nprint(w.grad)\n\nw.grad.zero_()\n\ntensor(-2.)\n\n\ntensor(0.)",
    "crumbs": [
      "Blog",
      "Model Training Loop"
    ]
  },
  {
    "objectID": "model_training_loop.html#gradient-descent",
    "href": "model_training_loop.html#gradient-descent",
    "title": "Model Training Loop",
    "section": "Gradient Descent",
    "text": "Gradient Descent\n\nNumpy\n\nPrediction: Manually\n\n\nGradients Computation: Manually\n\n\nLoss Computation: Manually\n\n\nParameter updates: Manually\n\n\nimport numpy as np\n\n\nx = np.array([1,2,3,4], dtype=np.float32)\ny = np.array([2,4,6,8], dtype=np.float32)\n\nw = 0.0\n\n\n# model\ndef forward(x):\n    return w * x\n\n\ndef loss(y, y_predicted):\n    return ((y_predicted - y)**2).mean()\n\n\n# gradient\n\n# MSE = 1/N * (w*x - y)**2\n# dJ/dw = 1/N 2x (w*x - y)\n\ndef gradient (x, y, y_predicted):\n    return np.dot(2 * x, y_predicted-y).mean()\n\nprint(f'Prediction before training: f(5) = {forward(5):.3f}')\n\nPrediction before training: f(5) = 0.000\n\n\n\nlearning_rate = 0.01\nn_iters = 15\n\nfor epoch in range(n_iters):\n    # prediction = forward pass\n    y_pred = forward(x)\n\n    # loss \n    l = loss(y, y_pred)\n\n    # gradients\n    dw = gradient(x, y, y_pred)\n\n    # update weights\n    w -= learning_rate * dw\n\n    if epoch % 1 == 0:\n        print(f'epoc:{epoch}  w = {w:.3f} , y_pred={forward(5)}, y = {10}, loss = {l:.8f}, dw = {dw}')\n\nprint(f'Prediction after training: {forward(5):.3f}, y = {10}')\n\nepoc:0  w = 1.200 , y_pred=6.0, y = 10, loss = 30.00000000, dw = -120.0\nepoc:1  w = 1.680 , y_pred=8.399999809265136, y = 10, loss = 4.79999924, dw = -47.999996185302734\nepoc:2  w = 1.872 , y_pred=9.35999994277954, y = 10, loss = 0.76800019, dw = -19.200002670288086\nepoc:3  w = 1.949 , y_pred=9.743999934196472, y = 10, loss = 0.12288000, dw = -7.679999828338623\nepoc:4  w = 1.980 , y_pred=9.897600066661834, y = 10, loss = 0.01966083, dw = -3.072002649307251\nepoc:5  w = 1.992 , y_pred=9.95904014110565, y = 10, loss = 0.00314574, dw = -1.2288014888763428\nepoc:6  w = 1.997 , y_pred=9.983615934848784, y = 10, loss = 0.00050331, dw = -0.4915158748626709\nepoc:7  w = 1.999 , y_pred=9.993446409702301, y = 10, loss = 0.00008053, dw = -0.1966094970703125\nepoc:8  w = 1.999 , y_pred=9.997378492355345, y = 10, loss = 0.00001288, dw = -0.07864165306091309\nepoc:9  w = 2.000 , y_pred=9.998951268196105, y = 10, loss = 0.00000206, dw = -0.03145551681518555\nepoc:10  w = 2.000 , y_pred=9.999580299854276, y = 10, loss = 0.00000033, dw = -0.012580633163452148\nepoc:11  w = 2.000 , y_pred=9.999832069873808, y = 10, loss = 0.00000005, dw = -0.005035400390625\nepoc:12  w = 2.000 , y_pred=9.999932992458342, y = 10, loss = 0.00000001, dw = -0.002018451690673828\nepoc:13  w = 2.000 , y_pred=9.999973046779632, y = 10, loss = 0.00000000, dw = -0.00080108642578125\nepoc:14  w = 2.000 , y_pred=9.999989175796507, y = 10, loss = 0.00000000, dw = -0.00032258033752441406\nPrediction after training: 10.000, y = 10\n\n\n\n\nTorch\n\nPrediction: Manually\n\n\nGradients Computation: Autograd\n\n\nLoss Computation: Manually\n\n\nParameter updates: Manually\n\n\nx = torch.tensor([1,2,3,4], dtype=torch.float32)\ny = torch.tensor([2,4,6,8], dtype=torch.float32)\n\nw = torch.tensor([0.0], dtype=torch.float32, requires_grad=True)\n\n\n# model\ndef forward(x):\n    return w * x\n\n\ndef loss(y, y_predicted):\n    return ((y_predicted - y)**2).mean()\n\n\n# gradient\n\n# MSE = 1/N * (w*x - y)**2\n# dJ/dw = 1/N 2x (w*x - y)\n\nprint(f'Prediction before training: f(5) = {forward(5)}')\n\nPrediction before training: f(5) = tensor([0.], grad_fn=&lt;MulBackward0&gt;)\n\n\n\nlearning_rate = 0.01\nn_iters = 50\n\nfor epoch in range(n_iters):\n    # prediction = forward pass\n    y_pred = forward(x)\n\n    # loss \n    l = loss(y, y_pred)\n\n    # gradients\n    l.backward()\n\n    # update weights\n    with torch.no_grad():\n        w -= learning_rate * w.grad\n\n    if epoch % 2 == 0:\n        print(f'epoc:{epoch}  w = {w.item():.3f}, y_pred={forward(5).item():.3f}, y = {10}, loss = {l.item():.7f}, dw = {w.grad.item():.7f}')\n\n    w.grad.zero_()\n\nprint(f'Prediction after training: {forward(5)}, y = {10}')\n\nepoc:0  w = 0.300, y_pred=1.500, y = 10, loss = 30.0000000, dw = -30.0000000\nepoc:2  w = 0.772, y_pred=3.859, y = 10, loss = 15.6601877, dw = -21.6749992\nepoc:4  w = 1.113, y_pred=5.563, y = 10, loss = 8.1747169, dw = -15.6601877\nepoc:6  w = 1.359, y_pred=6.794, y = 10, loss = 4.2672529, dw = -11.3144855\nepoc:8  w = 1.537, y_pred=7.684, y = 10, loss = 2.2275321, dw = -8.1747150\nepoc:10  w = 1.665, y_pred=8.327, y = 10, loss = 1.1627856, dw = -5.9062314\nepoc:12  w = 1.758, y_pred=8.791, y = 10, loss = 0.6069812, dw = -4.2672515\nepoc:14  w = 1.825, y_pred=9.126, y = 10, loss = 0.3168478, dw = -3.0830884\nepoc:16  w = 1.874, y_pred=9.369, y = 10, loss = 0.1653965, dw = -2.2275314\nepoc:18  w = 1.909, y_pred=9.544, y = 10, loss = 0.0863381, dw = -1.6093917\nepoc:20  w = 1.934, y_pred=9.671, y = 10, loss = 0.0450689, dw = -1.1627841\nepoc:22  w = 1.952, y_pred=9.762, y = 10, loss = 0.0235263, dw = -0.8401127\nepoc:24  w = 1.966, y_pred=9.828, y = 10, loss = 0.0122808, dw = -0.6069803\nepoc:26  w = 1.975, y_pred=9.876, y = 10, loss = 0.0064107, dw = -0.4385428\nepoc:28  w = 1.982, y_pred=9.910, y = 10, loss = 0.0033464, dw = -0.3168479\nepoc:30  w = 1.987, y_pred=9.935, y = 10, loss = 0.0017469, dw = -0.2289228\nepoc:32  w = 1.991, y_pred=9.953, y = 10, loss = 0.0009119, dw = -0.1653977\nepoc:34  w = 1.993, y_pred=9.966, y = 10, loss = 0.0004760, dw = -0.1194997\nepoc:36  w = 1.995, y_pred=9.976, y = 10, loss = 0.0002485, dw = -0.0863385\nepoc:38  w = 1.996, y_pred=9.982, y = 10, loss = 0.0001297, dw = -0.0623794\nepoc:40  w = 1.997, y_pred=9.987, y = 10, loss = 0.0000677, dw = -0.0450683\nepoc:42  w = 1.998, y_pred=9.991, y = 10, loss = 0.0000353, dw = -0.0325624\nepoc:44  w = 1.999, y_pred=9.993, y = 10, loss = 0.0000184, dw = -0.0235248\nepoc:46  w = 1.999, y_pred=9.995, y = 10, loss = 0.0000096, dw = -0.0169984\nepoc:48  w = 1.999, y_pred=9.997, y = 10, loss = 0.0000050, dw = -0.0122809\nPrediction after training: tensor([9.9970], grad_fn=&lt;MulBackward0&gt;), y = 10\n\n\n\n\nPytorch Loss and Pytorch Optimizer\n\nPrediction: Manually\n\n\nGradients Computation: Autograd\n\n\nLoss Computation: Pytorch Loss\n\n\nParameter updates: Pytorch Optimizer\n\n\nDesign Model = (input, output, size, forward pass)\nConstruct loss and optimizer\nTraining loop\n\nforward pass: compute prediction\nbackward pass: gradients\nupdate weights\n\n\n\nimport torch\nimport torch.nn as nn\n\nx = torch.tensor([1,2,3,4], dtype=torch.float32)\ny = torch.tensor([2,4,6,8], dtype=torch.float32)\n\nw = torch.tensor([0.0], dtype=torch.float32, requires_grad=True)\n\n\n# model\ndef forward(x):\n    return w * x\n\n\nprint(f'Prediction before training: f(5) = {forward(5)}')\n\nPrediction before training: f(5) = tensor([0.], grad_fn=&lt;MulBackward0&gt;)\n\n\n\nlearning_rate = 0.01\nn_iters = 50\n\nloss = nn.MSELoss()\noptimizer = torch.optim.SGD([w], lr= learning_rate)\n\nfor epoch in range(n_iters):\n    # prediction = forward pass\n    y_pred = forward(x)\n\n    # loss \n    l = loss(y, y_pred)\n\n    # gradients\n    l.backward()\n\n    optimizer.step()\n\n\n    if epoch % 2 == 0:\n        print(f'epoc:{epoch}  w = {w.item():.3f}, y_pred={forward(5).item():.3f}, y = {10}, loss = {l.item():.7f}, dw = {w.grad.item():.7f}')\n\n    optimizer.zero_grad()\n\nprint(f'Prediction after training: {forward(5)}, y = {10}')\n\nepoc:0  w = 0.300, y_pred=1.500, y = 10, loss = 30.0000000, dw = -30.0000000\nepoc:2  w = 0.772, y_pred=3.859, y = 10, loss = 15.6601877, dw = -21.6749992\nepoc:4  w = 1.113, y_pred=5.563, y = 10, loss = 8.1747169, dw = -15.6601877\nepoc:6  w = 1.359, y_pred=6.794, y = 10, loss = 4.2672529, dw = -11.3144855\nepoc:8  w = 1.537, y_pred=7.684, y = 10, loss = 2.2275321, dw = -8.1747150\nepoc:10  w = 1.665, y_pred=8.327, y = 10, loss = 1.1627856, dw = -5.9062314\nepoc:12  w = 1.758, y_pred=8.791, y = 10, loss = 0.6069812, dw = -4.2672515\nepoc:14  w = 1.825, y_pred=9.126, y = 10, loss = 0.3168478, dw = -3.0830884\nepoc:16  w = 1.874, y_pred=9.369, y = 10, loss = 0.1653965, dw = -2.2275314\nepoc:18  w = 1.909, y_pred=9.544, y = 10, loss = 0.0863381, dw = -1.6093917\nepoc:20  w = 1.934, y_pred=9.671, y = 10, loss = 0.0450689, dw = -1.1627841\nepoc:22  w = 1.952, y_pred=9.762, y = 10, loss = 0.0235263, dw = -0.8401127\nepoc:24  w = 1.966, y_pred=9.828, y = 10, loss = 0.0122808, dw = -0.6069803\nepoc:26  w = 1.975, y_pred=9.876, y = 10, loss = 0.0064107, dw = -0.4385428\nepoc:28  w = 1.982, y_pred=9.910, y = 10, loss = 0.0033464, dw = -0.3168479\nepoc:30  w = 1.987, y_pred=9.935, y = 10, loss = 0.0017469, dw = -0.2289228\nepoc:32  w = 1.991, y_pred=9.953, y = 10, loss = 0.0009119, dw = -0.1653977\nepoc:34  w = 1.993, y_pred=9.966, y = 10, loss = 0.0004760, dw = -0.1194997\nepoc:36  w = 1.995, y_pred=9.976, y = 10, loss = 0.0002485, dw = -0.0863385\nepoc:38  w = 1.996, y_pred=9.982, y = 10, loss = 0.0001297, dw = -0.0623794\nepoc:40  w = 1.997, y_pred=9.987, y = 10, loss = 0.0000677, dw = -0.0450683\nepoc:42  w = 1.998, y_pred=9.991, y = 10, loss = 0.0000353, dw = -0.0325624\nepoc:44  w = 1.999, y_pred=9.993, y = 10, loss = 0.0000184, dw = -0.0235248\nepoc:46  w = 1.999, y_pred=9.995, y = 10, loss = 0.0000096, dw = -0.0169984\nepoc:48  w = 1.999, y_pred=9.997, y = 10, loss = 0.0000050, dw = -0.0122809\nPrediction after training: tensor([9.9970], grad_fn=&lt;MulBackward0&gt;), y = 10\n\n\n\n\nPytorch Automate\n\nPrediction: Manually\n\n\nGradients Computation: Autograd\n\n\nLoss Computation: Pytorch Loss\n\n\nParameter updates: Pytorch Optimizer\n\n\nimport torch\nimport torch.nn as nn\n\nx = torch.tensor([[1],[2],[3],[4]], dtype=torch.float32)\ny = torch.tensor([[2],[4],[6],[8]], dtype=torch.float32)\n\nx_test = torch.tensor([5], dtype = torch.float32)\nn_samples, n_features = x.shape\nn_samples, n_features\n\n(4, 1)\n\n\n\nmodel = nn.Linear(in_features = n_features, out_features = 1)\nmodel\n\nLinear(in_features=1, out_features=1, bias=True)\n\n\n\n[w,b] = model.parameters()\nw[0].item()\n\n-0.8376840353012085\n\n\n\nmodel.state_dict()['weight']\n\ntensor([[-0.8377]])\n\n\n\nprint(f'Prediction before training: f(5) = {model(x_test)}')\n\nPrediction before training: f(5) = tensor([-3.8722], grad_fn=&lt;ViewBackward0&gt;)\n\n\n\nlearning_rate = 0.1\nn_iters = 500\n\nloss = nn.MSELoss()\noptimizer = torch.optim.SGD(model.parameters(), lr= learning_rate)\n\nfor epoch in range(n_iters):\n    # prediction = forward pass\n    y_pred = model(x)\n\n    # loss \n    l = loss(y, y_pred)\n\n    # gradients\n    l.backward()\n\n    optimizer.step()\n\n\n    if epoch % 20 == 0:\n        [w,b] = model.parameters()\n        print(f'epoc:{epoch}  w = {w[0].item():.3f} {b[0].item():.3f}, y_pred={model(x_test).item():.3f}, y = {10}, loss = {l.item():.7f}, dw = {w.grad.item():.7f}')\n\n    optimizer.zero_grad()\n\nprint(f'Prediction after training: {model(x_test)}, y = {10}')\n\nepoc:0  w = 3.261 1.672, y_pred=17.975, y = 10, loss = 56.0067291, dw = -40.9841690\nepoc:20  w = 1.794 0.607, y_pred=9.578, y = 10, loss = 0.0653165, dw = -0.0772833\nepoc:40  w = 1.888 0.330, y_pred=9.769, y = 10, loss = 0.0193617, dw = -0.0346756\nepoc:60  w = 1.939 0.180, y_pred=9.874, y = 10, loss = 0.0057399, dw = -0.0188781\nepoc:80  w = 1.967 0.098, y_pred=9.931, y = 10, loss = 0.0017016, dw = -0.0102807\nepoc:100  w = 1.982 0.053, y_pred=9.963, y = 10, loss = 0.0005045, dw = -0.0055964\nepoc:120  w = 1.990 0.029, y_pred=9.980, y = 10, loss = 0.0001496, dw = -0.0030484\nepoc:140  w = 1.995 0.016, y_pred=9.989, y = 10, loss = 0.0000443, dw = -0.0016569\nepoc:160  w = 1.997 0.009, y_pred=9.994, y = 10, loss = 0.0000131, dw = -0.0009021\nepoc:180  w = 1.998 0.005, y_pred=9.997, y = 10, loss = 0.0000039, dw = -0.0004910\nepoc:200  w = 1.999 0.003, y_pred=9.998, y = 10, loss = 0.0000012, dw = -0.0002694\nepoc:220  w = 2.000 0.001, y_pred=9.999, y = 10, loss = 0.0000003, dw = -0.0001463\nepoc:240  w = 2.000 0.001, y_pred=9.999, y = 10, loss = 0.0000001, dw = -0.0000764\nepoc:260  w = 2.000 0.000, y_pred=10.000, y = 10, loss = 0.0000000, dw = -0.0000430\nepoc:280  w = 2.000 0.000, y_pred=10.000, y = 10, loss = 0.0000000, dw = -0.0000240\nepoc:300  w = 2.000 0.000, y_pred=10.000, y = 10, loss = 0.0000000, dw = -0.0000120\nepoc:320  w = 2.000 0.000, y_pred=10.000, y = 10, loss = 0.0000000, dw = -0.0000055\nepoc:340  w = 2.000 0.000, y_pred=10.000, y = 10, loss = 0.0000000, dw = -0.0000049\nepoc:360  w = 2.000 0.000, y_pred=10.000, y = 10, loss = 0.0000000, dw = -0.0000038\nepoc:380  w = 2.000 0.000, y_pred=10.000, y = 10, loss = 0.0000000, dw = -0.0000001\nepoc:400  w = 2.000 0.000, y_pred=10.000, y = 10, loss = 0.0000000, dw = -0.0000017\nepoc:420  w = 2.000 0.000, y_pred=10.000, y = 10, loss = 0.0000000, dw = -0.0000015\nepoc:440  w = 2.000 0.000, y_pred=10.000, y = 10, loss = 0.0000000, dw = 0.0000001\nepoc:460  w = 2.000 0.000, y_pred=10.000, y = 10, loss = 0.0000000, dw = -0.0000001\nepoc:480  w = 2.000 0.000, y_pred=10.000, y = 10, loss = 0.0000000, dw = 0.0000007\nPrediction after training: tensor([10.], grad_fn=&lt;ViewBackward0&gt;), y = 10\n\n\n\n\nPytorch Model\n\nPrediction: Manually\n\n\nGradients Computation: Autograd\n\n\nLoss Computation: Pytorch Loss\n\n\nParameter updates: Pytorch Optimizer\n\n\nDesign Model = (input, output, size, forward pass)\nConstruct loss and optimizer\nTraining loop\n\nforward pass: compute prediction\nbackward pass: gradients\nupdate weights\n\n\n\nimport torch\nimport torch.nn as nn\n\nx = torch.tensor([[1],[2],[3],[4]], dtype=torch.float32)\ny = torch.tensor([[2],[4],[6],[8]], dtype=torch.float32)\n\nx_test = torch.tensor([5], dtype = torch.float32)\nn_samples, n_features = x.shape\nn_samples, n_features\n\n(4, 1)\n\n\n\nmodel = nn.Linear(in_features = n_features, out_features = 1)\nmodel\n\nLinear(in_features=1, out_features=1, bias=True)\n\n\n\nclass LinearRegression(nn.Module):\n    def __init__(self, in_features, out_features):\n        super(LinearRegression, self).__init__()\n\n        self.lin = nn.Linear(in_features, out_features)\n\n    def forward(self, x):\n        return self.lin(x)\n\nmodel = LinearRegression(in_features = n_features, out_features = 1)\nmodel\n\nLinearRegression(\n  (lin): Linear(in_features=1, out_features=1, bias=True)\n)\n\n\n\n[w,b] = model.parameters()\nw[0].item()\n\n-0.08443880081176758\n\n\n\nmodel.state_dict()['lin.weight']\n\ntensor([[-0.0844]])\n\n\n\nprint(f'Prediction before training: f(5) = {model(x_test)}')\n\nPrediction before training: f(5) = tensor([-0.1386], grad_fn=&lt;ViewBackward0&gt;)\n\n\n\nlearning_rate = 0.1\nn_iters = 500\n\nloss = nn.MSELoss()\noptimizer = torch.optim.SGD(model.parameters(), lr= learning_rate)\n\nfor epoch in range(n_iters):\n    # prediction = forward pass\n    y_pred = model(x)\n\n    # loss \n    l = loss(y, y_pred)\n\n    # gradients\n    l.backward()\n\n    optimizer.step()\n\n\n    if epoch % 20 == 0:\n        [w,b] = model.parameters()\n        print(f'epoc:{epoch}  w = {w[0].item():.3f} {b[0].item():.3f}, y_pred={model(x_test).item():.3f}, y = {10}, loss = {l.item():.7f}, dw = {w.grad.item():.7f}')\n\n    optimizer.zero_grad()\n\nprint(f'Prediction after training: {model(x_test)}, y = {10}')\n\nepoc:0  w = 2.900 1.269, y_pred=15.771, y = 10, loss = 29.7110996, dw = -29.8484650\nepoc:20  w = 1.841 0.470, y_pred=9.673, y = 10, loss = 0.0391924, dw = -0.0592351\nepoc:40  w = 1.913 0.256, y_pred=9.821, y = 10, loss = 0.0116179, dw = -0.0268617\nepoc:60  w = 1.953 0.139, y_pred=9.902, y = 10, loss = 0.0034442, dw = -0.0146208\nepoc:80  w = 1.974 0.076, y_pred=9.947, y = 10, loss = 0.0010210, dw = -0.0079615\nepoc:100  w = 1.986 0.041, y_pred=9.971, y = 10, loss = 0.0003027, dw = -0.0043370\nepoc:120  w = 1.992 0.022, y_pred=9.984, y = 10, loss = 0.0000897, dw = -0.0023587\nepoc:140  w = 1.996 0.012, y_pred=9.991, y = 10, loss = 0.0000266, dw = -0.0012866\nepoc:160  w = 1.998 0.007, y_pred=9.995, y = 10, loss = 0.0000079, dw = -0.0007011\nepoc:180  w = 1.999 0.004, y_pred=9.997, y = 10, loss = 0.0000023, dw = -0.0003816\nepoc:200  w = 1.999 0.002, y_pred=9.999, y = 10, loss = 0.0000007, dw = -0.0002074\nepoc:220  w = 2.000 0.001, y_pred=9.999, y = 10, loss = 0.0000002, dw = -0.0001137\nepoc:240  w = 2.000 0.001, y_pred=10.000, y = 10, loss = 0.0000001, dw = -0.0000589\nepoc:260  w = 2.000 0.000, y_pred=10.000, y = 10, loss = 0.0000000, dw = -0.0000327\nepoc:280  w = 2.000 0.000, y_pred=10.000, y = 10, loss = 0.0000000, dw = -0.0000174\nepoc:300  w = 2.000 0.000, y_pred=10.000, y = 10, loss = 0.0000000, dw = -0.0000103\nepoc:320  w = 2.000 0.000, y_pred=10.000, y = 10, loss = 0.0000000, dw = -0.0000072\nepoc:340  w = 2.000 0.000, y_pred=10.000, y = 10, loss = 0.0000000, dw = -0.0000017\nepoc:360  w = 2.000 0.000, y_pred=10.000, y = 10, loss = 0.0000000, dw = -0.0000023\nepoc:380  w = 2.000 0.000, y_pred=10.000, y = 10, loss = 0.0000000, dw = -0.0000023\nepoc:400  w = 2.000 0.000, y_pred=10.000, y = 10, loss = 0.0000000, dw = -0.0000010\nepoc:420  w = 2.000 0.000, y_pred=10.000, y = 10, loss = 0.0000000, dw = -0.0000023\nepoc:440  w = 2.000 0.000, y_pred=10.000, y = 10, loss = 0.0000000, dw = 0.0000000\nepoc:460  w = 2.000 0.000, y_pred=10.000, y = 10, loss = 0.0000000, dw = -0.0000007\nepoc:480  w = 2.000 0.000, y_pred=10.000, y = 10, loss = 0.0000000, dw = -0.0000015\nPrediction after training: tensor([10.0000], grad_fn=&lt;ViewBackward0&gt;), y = 10",
    "crumbs": [
      "Blog",
      "Model Training Loop"
    ]
  },
  {
    "objectID": "model_training_loop.html#linear-regression",
    "href": "model_training_loop.html#linear-regression",
    "title": "Model Training Loop",
    "section": "Linear Regression",
    "text": "Linear Regression\n\nimport torch\nimport torch.nn as nn\nimport numpy as np\nfrom sklearn import datasets\nimport matplotlib.pyplot as plt\n\n\nPrepare data\nmodel\nloss and optimizer\ntraining loop\n\n\nx_numpy, y_numpy = datasets.make_regression(n_samples= 100, n_features=1, noise = 20, random_state = 1)\n\n\nx = torch.from_numpy(x_numpy.astype(np.float32))\ny = torch.from_numpy(y_numpy.astype(np.float32))\nx[:5], y[:5]\n\n(tensor([[-0.6118],\n         [-0.2494],\n         [ 0.4885],\n         [ 0.7620],\n         [ 1.5198]]),\n tensor([-55.5386, -10.6620,  22.7574, 101.0961, 144.3376]))\n\n\n\nx_test = x[4]\ny_test = y[4]\nx_test, y_test\n\n(tensor([1.5198]), tensor(144.3376))\n\n\n\ny =y.view(y.shape[0], 1)\nx[:5], y[:5]\n\n(tensor([[-0.6118],\n         [-0.2494],\n         [ 0.4885],\n         [ 0.7620],\n         [ 1.5198]]),\n tensor([[-55.5386],\n         [-10.6620],\n         [ 22.7574],\n         [101.0961],\n         [144.3376]]))\n\n\n\nn_samples, n_features = x.shape\nn_samples, n_features\n\n(100, 1)\n\n\n\n#1.model\ninput_size = n_features\noutput_size = 1\nmodel = nn.Linear(input_size, output_size)\nmodel\n\nLinear(in_features=1, out_features=1, bias=True)\n\n\n\n[a,b] = model.parameters()\na,b\n\n(Parameter containing:\n tensor([[-0.3357]], requires_grad=True),\n Parameter containing:\n tensor([0.3514], requires_grad=True))\n\n\n\n#2. loss and optimizer\nlearning_rate = 0.01\ncriterion = nn.MSELoss()\noptimizer = torch.optim.SGD(model.parameters(), lr= learning_rate)\n\n\noptimizer\n\nSGD (\nParameter Group 0\n    dampening: 0\n    differentiable: False\n    foreach: None\n    lr: 0.01\n    maximize: False\n    momentum: 0\n    nesterov: False\n    weight_decay: 0\n)\n\n\n\n#3. training loop\nnum_epochs = 1000\n\nfor epoch in range(num_epochs):\n    #forward pass and loss\n    y_predicted = model(x)\n    loss=criterion(y_predicted, y)\n\n    # backward pass\n    loss.backward()\n\n    #update\n    optimizer.step()\n\n    if (epoch + 1) % 50 == 0:\n        [w,b] = model.parameters()\n        print(f'[epoc:{epoch}] (y = {w[0].item():.3f}x + {b[0].item():.3f}) y_pred:{model(x_test).item():.3f}, y:{y_test}, loss :{loss.item():.7f}, dw:{w.grad.item():.7f} db:{b.grad.item():.7f}')\n\n    optimizer.zero_grad()\n\n[epoc:49] (y = 45.054x + 4.782) y_pred:73.255, y:144.33755493164062, loss :1468.2329102, dw:-59.7857857 db:-3.2169607\n[epoc:99] (y = 65.507x + 5.259) y_pred:104.818, y:144.33755493164062, loss :565.6588745, dw:-27.0066071 db:0.3266662\n[epoc:149] (y = 74.762x + 4.920) y_pred:118.545, y:144.33755493164062, loss :380.9410095, dw:-12.2441845 db:0.7970295\n[epoc:199] (y = 78.964x + 4.564) y_pred:124.575, y:144.33755493164062, loss :342.6767883, dw:-5.5675182 db:0.5981486\n[epoc:249] (y = 80.877x + 4.328) y_pred:127.246, y:144.33755493164062, loss :334.6894531, dw:-2.5375218 db:0.3580039\n[epoc:299] (y = 81.749x + 4.195) y_pred:128.438, y:144.33755493164062, loss :333.0141296, dw:-1.1586771 db:0.1943260\n[epoc:349] (y = 82.148x + 4.124) y_pred:128.973, y:144.33755493164062, loss :332.6617126, dw:-0.5298302 db:0.0999891\n[epoc:399] (y = 82.330x + 4.088) y_pred:129.215, y:144.33755493164062, loss :332.5874329, dw:-0.2425606 db:0.0497854\n[epoc:449] (y = 82.414x + 4.070) y_pred:129.324, y:144.33755493164062, loss :332.5717468, dw:-0.1111394 db:0.0242567\n[epoc:499] (y = 82.452x + 4.062) y_pred:129.374, y:144.33755493164062, loss :332.5684509, dw:-0.0509445 db:0.0116392\n[epoc:549] (y = 82.470x + 4.058) y_pred:129.396, y:144.33755493164062, loss :332.5677490, dw:-0.0233506 db:0.0055273\n[epoc:599] (y = 82.478x + 4.056) y_pred:129.407, y:144.33755493164062, loss :332.5675659, dw:-0.0107345 db:0.0026025\n[epoc:649] (y = 82.481x + 4.055) y_pred:129.411, y:144.33755493164062, loss :332.5675659, dw:-0.0049275 db:0.0012180\n[epoc:699] (y = 82.483x + 4.054) y_pred:129.414, y:144.33755493164062, loss :332.5675659, dw:-0.0022745 db:0.0005686\n[epoc:749] (y = 82.484x + 4.054) y_pred:129.415, y:144.33755493164062, loss :332.5675659, dw:-0.0010500 db:0.0002624\n[epoc:799] (y = 82.484x + 4.054) y_pred:129.415, y:144.33755493164062, loss :332.5675659, dw:-0.0004619 db:0.0001247\n[epoc:849] (y = 82.484x + 4.054) y_pred:129.415, y:144.33755493164062, loss :332.5675659, dw:-0.0003704 db:0.0000531\n[epoc:899] (y = 82.484x + 4.054) y_pred:129.415, y:144.33755493164062, loss :332.5675659, dw:-0.0003721 db:0.0000235\n[epoc:949] (y = 82.484x + 4.054) y_pred:129.415, y:144.33755493164062, loss :332.5675659, dw:-0.0003721 db:0.0000235\n[epoc:999] (y = 82.484x + 4.054) y_pred:129.415, y:144.33755493164062, loss :332.5675659, dw:-0.0003721 db:0.0000235\n\n\n\npredicted = model(x).detach().numpy()\n\n\nplt.plot(x_numpy, y_numpy, 'ro')\nplt.plot(x_numpy, predicted, 'b')\nplt.show()\n\n\n\n\n\n\n\n\n\nfrom bokeh.io import output_notebook\n\n\noutput_notebook()\n\n    \n    \n        \n        Loading BokehJS ...\n    \n\n\n\n\n\n\nfrom bokeh.plotting import figure, show\n\nfrom bokeh.io import curdoc\n# apply theme to current document\ncurdoc().theme = \"dark_minimal\"\n\n\n# create a new plot with a title and axis labels\np = figure(title=\"Real data vs Model\",\n           x_axis_label='x',\n           y_axis_label='y',\n           sizing_mode=\"stretch_width\",\n           max_width=1000,\n           height=500,)\n\n\n# add a line renderer with legend and line thickness to the plot\np.circle(x_numpy.flatten(), y_numpy.flatten(), legend_label=\"Original\", line_width=2, color=\"red\", radius=0.02)\np.line(x_numpy.flatten(), predicted.flatten(), legend_label=\"Predicted\", line_width=2)\n\np.legend.location = \"top_left\"\np.legend.click_policy=\"mute\"\n\n# show the results\nshow(p)",
    "crumbs": [
      "Blog",
      "Model Training Loop"
    ]
  },
  {
    "objectID": "model_training_loop.html#logistic-regression",
    "href": "model_training_loop.html#logistic-regression",
    "title": "Model Training Loop",
    "section": "Logistic Regression",
    "text": "Logistic Regression\n\nimport torch\nimport torch.nn as nn\nimport numpy as np\nfrom sklearn import datasets\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\n\nbc = datasets.load_breast_cancer()\nbc.keys()\n\ndict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename', 'data_module'])\n\n\n\nx, y = bc.data, bc.target\nn_samples, n_features = x.shape\nn_samples, n_features\n\n(569, 30)\n\n\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 1234)\nx_train.shape, x_test.shape, y_train.shape, y_test.shape\n\n((455, 30), (114, 30), (455,), (114,))\n\n\n\nsc = StandardScaler()\nx_train = sc.fit_transform(x_train)\nx_test = sc.transform(x_test)\n\n\nx_train = torch.from_numpy(x_train.astype(np.float32))\nx_test = torch.from_numpy(x_test.astype(np.float32))\ny_train = torch.from_numpy(y_train.astype(np.float32))\ny_test = torch.from_numpy(y_test.astype(np.float32))\n\n\ny_train = y_train.view(y_train.shape[0], 1)\n\n\n#1. model\n\nclass LogisticRegression(nn.Module):\n    def __init__(self, n_input):\n        super(LogisticRegression, self).__init__()\n\n        self.linear = nn.Linear(n_input, 1)\n\n    def forward(self, x):\n        y_pred = torch.sigmoid(self.linear(x))\n\n        return y_pred\n\nmodel = LogisticRegression(n_features)\n\n\n#2. loss and optimizer\nlearning_rate = 0.01\ncriterion = nn.BCELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n\n\n#3. train loop\nnum_epochs = 1000\nfor epoch in range(num_epochs):\n    #forward pass and loss\n    y_predicted = model(x_train)\n    loss=criterion(y_predicted, y_train)\n\n    # backward pass\n    loss.backward()\n\n    #update\n    optimizer.step()\n\n    if (epoch + 1) % 50 == 0:\n        with torch.no_grad():\n            [w,b] = model.parameters()\n            y_predicted = model(x_test)\n            y_predicted_cls = y_predicted.round().flatten()\n            acc = (y_predicted_cls == y_test).float().mean() * 100\n            error = (100 - acc)\n            print(f'[epoc:{epoch + 1}] (y = {w.mean().item():.3f}x + {b.mean().item():.3f}) \\\n            loss:{loss.item():.5f}, accuracy: {acc:.2f}%, error: {error:.2f}%, \\\n            dw:{w.grad.mean().item():.5f} db:{b.grad.mean().item():.5f}')\n\n    optimizer.zero_grad()\n\n[epoc:50] (y = -0.171x + 0.301)             loss:0.11539, accuracy: 93.86%, error: 6.14%,             dw:0.01317 db:-0.03279\n[epoc:100] (y = -0.222x + 0.488)             loss:0.07955, accuracy: 94.74%, error: 5.26%,             dw:0.00604 db:-0.01464\n[epoc:150] (y = -0.257x + 0.602)             loss:0.06487, accuracy: 95.61%, error: 4.39%,             dw:0.00404 db:-0.00820\n[epoc:200] (y = -0.286x + 0.681)             loss:0.05622, accuracy: 96.49%, error: 3.51%,             dw:0.00301 db:-0.00513\n[epoc:250] (y = -0.311x + 0.739)             loss:0.05038, accuracy: 95.61%, error: 4.39%,             dw:0.00238 db:-0.00343\n[epoc:300] (y = -0.334x + 0.783)             loss:0.04613, accuracy: 95.61%, error: 4.39%,             dw:0.00197 db:-0.00240\n[epoc:350] (y = -0.355x + 0.817)             loss:0.04288, accuracy: 95.61%, error: 4.39%,             dw:0.00167 db:-0.00173\n[epoc:400] (y = -0.375x + 0.844)             loss:0.04031, accuracy: 95.61%, error: 4.39%,             dw:0.00144 db:-0.00128\n[epoc:450] (y = -0.394x + 0.866)             loss:0.03821, accuracy: 95.61%, error: 4.39%,             dw:0.00126 db:-0.00096\n[epoc:500] (y = -0.412x + 0.883)             loss:0.03648, accuracy: 95.61%, error: 4.39%,             dw:0.00112 db:-0.00073\n[epoc:550] (y = -0.429x + 0.898)             loss:0.03501, accuracy: 95.61%, error: 4.39%,             dw:0.00101 db:-0.00056\n[epoc:600] (y = -0.445x + 0.909)             loss:0.03374, accuracy: 95.61%, error: 4.39%,             dw:0.00091 db:-0.00042\n[epoc:650] (y = -0.460x + 0.918)             loss:0.03264, accuracy: 95.61%, error: 4.39%,             dw:0.00083 db:-0.00032\n[epoc:700] (y = -0.475x + 0.925)             loss:0.03167, accuracy: 95.61%, error: 4.39%,             dw:0.00076 db:-0.00024\n[epoc:750] (y = -0.489x + 0.931)             loss:0.03080, accuracy: 95.61%, error: 4.39%,             dw:0.00070 db:-0.00017\n[epoc:800] (y = -0.503x + 0.935)             loss:0.03003, accuracy: 95.61%, error: 4.39%,             dw:0.00065 db:-0.00012\n[epoc:850] (y = -0.516x + 0.938)             loss:0.02932, accuracy: 95.61%, error: 4.39%,             dw:0.00060 db:-0.00008\n[epoc:900] (y = -0.529x + 0.940)             loss:0.02868, accuracy: 95.61%, error: 4.39%,             dw:0.00056 db:-0.00004\n[epoc:950] (y = -0.542x + 0.941)             loss:0.02809, accuracy: 95.61%, error: 4.39%,             dw:0.00052 db:-0.00002\n[epoc:1000] (y = -0.554x + 0.942)             loss:0.02754, accuracy: 95.61%, error: 4.39%,             dw:0.00049 db:0.00001\n\n\n\nwith torch.no_grad():\n    [w,b] = model.parameters()\n    y_predicted = model(x_test)\n    y_predicted_cls = y_predicted.round().flatten()\n    acc = (y_predicted_cls == y_test).float().mean() * 100\n    error = (100 - acc)\n    print(f'[epoc:{epoch + 1}] (y = {w.mean().item():.3f}x + {b.mean().item():.3f}) \\\n    loss:{loss.item():.5f}, accuracy: {acc:.2f}%, error: {error:.2f}%')\n\n[epoc:1000] (y = -0.554x + 0.942)     loss:0.02754, accuracy: 95.61%, error: 4.39%",
    "crumbs": [
      "Blog",
      "Model Training Loop"
    ]
  },
  {
    "objectID": "model_training_loop.html#softmax-and-cross-entropy",
    "href": "model_training_loop.html#softmax-and-cross-entropy",
    "title": "Model Training Loop",
    "section": "Softmax and Cross-Entropy",
    "text": "Softmax and Cross-Entropy\n\nSoftmax\n\n\\(S(y_i) = \\frac{e^{y_i}}{\\sum e^{y_i}}\\)\n\n\\(Linear = [2.0, 1.0, 0.1]\\)\n\\(Softmax = [0.7, 0.2, 0.1]\\)\nAdds to 1\n\ndef softmax(x):\n    return np.exp(x) / np.sum(np.exp(x), axis = 0)\n\nx = np.array([2.0, 1.0, 0.1])\noutputs = softmax(x)\noutputs\n\narray([0.65900114, 0.24243297, 0.09856589])\n\n\n\nx = torch.from_numpy(x)\nx\n\ntensor([2.0000, 1.0000, 0.1000], dtype=torch.float64)\n\n\n\noutputs = torch.softmax(x, dim = 0)\noutputs\n\ntensor([0.6590, 0.2424, 0.0986], dtype=torch.float64)",
    "crumbs": [
      "Blog",
      "Model Training Loop"
    ]
  },
  {
    "objectID": "model_training_loop.html#cross-entropy",
    "href": "model_training_loop.html#cross-entropy",
    "title": "Model Training Loop",
    "section": "Cross Entropy",
    "text": "Cross Entropy\n\n\\(D(\\hat{Y}, Y) = \\dfrac{1}{N} \\cdot \\displaystyle\\sum_{i=1}^{N} Y_i \\cdot \\log{\\hat{Y_i}}\\)\n\n\\(Y = [1, 0, 0]\\)\n\\(\\hat{Y} = [0.7, 0.2, 0.1]  --&gt; D(\\hat{Y}, Y) = 0.35\\)\n\\(Y = [1, 0, 0]\\)\n\\(\\hat{Y} = [0.7, 0.2, 0.1]  --&gt; D(\\hat{Y}, Y) = 2.30\\)\n\ndef cross_entropy(actual, predicted):\n    loss = -np.sum(actual * np.log(predicted))\n    return loss\n\n\nY_actual = np.array([1,0,0])\n\nY_pred_good = np.array([0.7, 0.2, 0.1])\nY_pred_bad = np.array([0.1, 0.3, 0.6])\n\nl1 = cross_entropy(Y_actual, Y_pred_good)\nl2 = cross_entropy(Y_actual, Y_pred_bad)\n\n\nprint(f'good pred:{l1:4f}, bad pred:{l2:.4f}')\n\ngood pred:0.356675, bad pred:2.3026\n\n\n\nnn.CrossEntropyLoss()\n\napplies nn.LogSoftmax + nn.NLLLoss(negative log likelihood loss)\n\n\ny has class labels, not One-Hot!\n\n\nY_pred has raw scores(logits), no softmax\n\n\nloss = nn.CrossEntropyLoss()\n\n\nY = torch.tensor([0])\n\nY_pred_good = torch.tensor([[2.0, 1.0, 0.1]])\nY_pred_bad = torch.tensor([[0.5, 2.0, 0.3]])\n\nl1 = loss(Y_pred_good, Y)\nl2 = loss(Y_pred_bad, Y)\n\nprint(f'good pred:{l1:4f}, bad pred:{l2:.4f}')\n\ngood pred:0.417030, bad pred:1.8406\n\n\n\n_, predictions1 = torch.max(Y_pred_good, 1)\n_, predictions2 = torch.max(Y_pred_bad, 1)\n\nprint(f'good pred:{predictions1}, bad pred:{predictions2}')\n\ngood pred:tensor([0]), bad pred:tensor([1])\n\n\n\n#Multiclass Problem\nclass NeuralNet2(nn.Module):\n    def __init__(self, input_size, hidden_size, num_classes):\n        super(NeuralNet2, self).__init__()\n        self.linear1 = nn.Linear(input_size, hidden_size)\n        self.rely = nn.ReLU()\n        self.linear2 = nn.Linear(hidden_size, num_classes)\n        \n\n    def forward(self, x):\n        out = self.linear1(x)\n        out = self.relu(out)\n        out = self.linear2(out)\n\n        return out\n\n\nmodel = NeuralNet2(input_size = 28 * 28, hidden_size = 5, num_classes = 3)\ncrioterion = nn.CrossEntropyLoss()",
    "crumbs": [
      "Blog",
      "Model Training Loop"
    ]
  },
  {
    "objectID": "model_training_loop.html#activation-functions",
    "href": "model_training_loop.html#activation-functions",
    "title": "Model Training Loop",
    "section": "Activation Functions",
    "text": "Activation Functions\n\nWithout activation functions, our network is basically just a stacked linear regression model\n\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\n#Option 1 (create nn modules)\nclass NeuralNet2(nn.Module):\n    def __init__(self, input_size, hidden_size, num_classes):\n        super(NeuralNet2, self).__init__()\n        self.linear1 = nn.Linear(input_size, hidden_size)\n        self.relu = nn.ReLU()\n        self.linear2 = nn.Linear(hidden_size, num_classes)\n        self.sigmoid = nn.Sigmoid()\n        \n\n    def forward(self, x):\n        out = self.linear1(x)\n        out = self.relu(out)\n        out = self.linear2(out)\n        out = self.sigmoid(out)\n\n        return out\n\n\n#Option 2 (use activation functions directly in forward pass)\nclass NeuralNet2(nn.Module):\n    def __init__(self, input_size, hidden_size, num_classes):\n        super(NeuralNet2, self).__init__()\n        self.linear1 = nn.Linear(input_size, hidden_size)\n        self.linear2 = nn.Linear(hidden_size, num_classes)\n        \n\n    def forward(self, x):\n        out = torch.relu(self.linear1(x))\n        out = torch.sigmoid(self.linear2(out))\n\n        return out",
    "crumbs": [
      "Blog",
      "Model Training Loop"
    ]
  },
  {
    "objectID": "model_training_loop.html#mlp-on-mnist",
    "href": "model_training_loop.html#mlp-on-mnist",
    "title": "Model Training Loop",
    "section": "MLP on MNIST",
    "text": "MLP on MNIST\n\nMNIST\nDataLoader, Transformation\nMultilayer Neural Net, activation function\nLoss and Optimizer\nTraining Loop (batch training)\nModel evaluation\nGPU Support\n\n\nimport torch\nimport torch.nn as nn\nimport torchvision\nimport torchvision.transforms as transforms \nfrom torchvision.transforms import ToPILImage\nimport matplotlib.pyplot as plt\n\n\ntorch.cuda.is_available()\n\nTrue\n\n\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n\ninput_size = 28 * 28\nhidden_size = 100\nnum_classes = 10\nnum_epochs = 2\nbatch_size = 100\n\n\ntrain_dataset = torchvision.datasets.MNIST(root=\"./data\",\n                                           download=True,\n                                           train=True,\n                                           transform=transforms.ToTensor())\n\ntest_dataset = torchvision.datasets.MNIST(root=\"./data\",\n                                          download=True,\n                                          train=False,\n                                          transform=transforms.ToTensor())\n\ntrain_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n                                           batch_size = batch_size,\n                                           shuffle = True)\n\ntest_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n                                          batch_size = batch_size,\n                                          shuffle = False)\n\n\nlen(train_dataset), len(test_dataset)\n\n(60000, 10000)\n\n\n\nimage, label = train_dataset[1]\nplt.imshow(transforms.ToPILImage()(image), cmap='gray')\nplt.axis('off')\nplt.show()\n\n\n\n\n\n\n\n\n\nexamples = iter(train_loader)\nimages, labels = next(examples)\n\n\nimages.shape, labels.shape\n\n(torch.Size([100, 1, 28, 28]), torch.Size([100]))\n\n\n\nfor i in range(6):\n    plt.subplot(2, 3, i + 1)\n    plt.imshow(images[i][0], cmap = 'gray')\n    plt.axis('off')\n\nplt.show()\n\n\n\n\n\n\n\n\n\n# model\nclass NeuralNet(nn.Module):\n    def __init__(self, input_size, hidden_size, num_classes):\n        super(NeuralNet, self).__init__()\n        self.l1 = nn.Linear(input_size, hidden_size)\n        self.relu = nn.ReLU()\n        self.l2 = nn.Linear(hidden_size, num_classes)\n\n    def forward(self, x):\n        out = self.l1(x)\n        out = self.relu(out)\n        out = self.l2(out)\n        \n        return out\n\nmodel = NeuralNet(input_size, hidden_size, num_classes).to(device)\n\n\n# loss and optimizer\nlearning_rate = 0.001\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n\n\n# Train loop\nn_total_steps = len(train_loader)\nrunning_loss = 0.0\nprint_stat = 100\n\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):\n        # 100, 1, 28, 28 --&gt; 100 , 28 * 28\n        images = images.reshape(-1, 28 * 28).to(device)\n        labels = labels.to(device)\n\n        # forward\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n\n        #backwards\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        if (i + 1) % print_stat == 0:\n            with torch.no_grad():\n                n_correct = 0\n                n_samples = 0\n                for images, labels in test_loader:\n                    images = images.reshape(-1, 28 * 28).to(device)\n                    labels = labels.to(device)\n                    outputs = model(images)\n\n                    _, predictions = torch.max(outputs, 1)\n                    n_samples += labels.shape[0]\n                    n_correct += (predictions == labels).sum().item()\n            \n                acc = 100.0 * n_correct / n_samples\n            \n            print(f'[epoch:{epoch+1}/{num_epochs}, [step:{i+1}/{n_total_steps}] loss:{(running_loss/print_stat):.4f} accuracy:{acc}')\n            running_loss = 0.0\n\n[epoch:1/2, [step:100/600] loss:0.9543 accuracy:88.51333333333334\n[epoch:1/2, [step:200/600] loss:0.3956 accuracy:90.49166666666666\n[epoch:1/2, [step:300/600] loss:0.3070 accuracy:91.86666666666666\n[epoch:1/2, [step:400/600] loss:0.2970 accuracy:92.62\n[epoch:1/2, [step:500/600] loss:0.2614 accuracy:93.28833333333333\n[epoch:1/2, [step:600/600] loss:0.2401 accuracy:93.665\n[epoch:2/2, [step:100/600] loss:0.2229 accuracy:94.08166666666666\n[epoch:2/2, [step:200/600] loss:0.2201 accuracy:94.43333333333334\n[epoch:2/2, [step:300/600] loss:0.1986 accuracy:94.75833333333334\n[epoch:2/2, [step:400/600] loss:0.1918 accuracy:94.61666666666666\n[epoch:2/2, [step:500/600] loss:0.1879 accuracy:95.115\n[epoch:2/2, [step:600/600] loss:0.1609 accuracy:95.52166666666666\n\n\n\n# Test\nwith torch.no_grad():\n    n_correct = 0\n    n_samples = 0\n    for images, labels in test_loader:\n        images = images.reshape(-1, 28 * 28).to(device)\n        labels = labels.to(device)\n        outputs = model(images)\n\n        #value, index\n        _, predictions = torch.max(outputs, 1)\n        n_samples += labels.shape[0]\n        n_correct += (predictions == labels).sum().item()\n\n    acc = 100.0 * n_correct / n_samples\n    print(f'accuracy = {acc}')\n\naccuracy = 95.52166666666666",
    "crumbs": [
      "Blog",
      "Model Training Loop"
    ]
  },
  {
    "objectID": "model_training_loop.html#cnn-on-cifar-10",
    "href": "model_training_loop.html#cnn-on-cifar-10",
    "title": "Model Training Loop",
    "section": "CNN on Cifar-10",
    "text": "CNN on Cifar-10\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.transforms as transforms \nfrom torchvision.transforms import ToPILImage\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\ntorch.cuda.is_available()\n\nTrue\n\n\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n\nnum_epochs = 4\nbatch_size = 100\n\ntransform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5),(0.5, 0.5, 0.5))])\n\n\ntrain_dataset = torchvision.datasets.CIFAR10(root=\"./data\", \n                                             download=True,\n                                             train=True,\n                                             transform=transform)\n\ntest_dataset = torchvision.datasets.CIFAR10(root=\"./data\",\n                                            download=True,\n                                            train=False,\n                                            transform=transform)\n\ntrain_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n                                           batch_size = batch_size,\n                                           shuffle = True)\n\ntest_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n                                          batch_size = batch_size,\n                                          shuffle = False)\n\nFiles already downloaded and verified\nFiles already downloaded and verified\n\n\n\nlen(train_dataset), len(test_dataset)\n\n(50000, 10000)\n\n\n\nclasses = train_dataset.class_to_idx\n\n\nclasses = list(train_dataset.class_to_idx)\n\n\nlist(classes)\n\n['airplane',\n 'automobile',\n 'bird',\n 'cat',\n 'deer',\n 'dog',\n 'frog',\n 'horse',\n 'ship',\n 'truck']\n\n\n\nexamples = iter(train_loader)\nimages, labels = next(examples)\n\n\nimages.shape, labels.shape\n\n(torch.Size([100, 3, 32, 32]), torch.Size([100]))\n\n\n\nclass ConvNet(nn.Module):\n    def __init__(self):\n        super(ConvNet, self).__init__()\n        # input size: 3 colour channels\n        self.conv1 = nn.Conv2d(3, 6, 5)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        self.fc1 = nn.Linear(16*5*5, 120)\n        self.fc2 = nn.Linear(120, 40)\n        self.fc3 = nn.Linear(40, 10)\n        \n\n    def forward(self, x):\n        out = self.pool(F.relu(self.conv1(x)))\n        out = self.pool(F.relu(self.conv2(out)))\n        out = out.view(-1, 16*5*5)\n        out = F.relu(self.fc1(out))\n        out = F.relu(self.fc2(out))\n        out = self.fc3(out)\n\n        return out\n        \n\nmodel = ConvNet().to(device)\n\n\nlearning_rate = 0.001\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n\n\n# Image Classifier Neural Network\nclass ImageClassifier(nn.Module): \n    def __init__(self):\n        super().__init__()\n        self.model = nn.Sequential(\n            nn.Conv2d(3, 32, (3,3)), \n            nn.ReLU(),\n            nn.Conv2d(32, 64, (3,3)), \n            nn.ReLU(),\n            nn.Conv2d(64, 64, (3,3)), \n            nn.ReLU(),\n            nn.Flatten(), \n            nn.Linear(64*(28-2)*(28-2), 10)  \n        )\n\n    def forward(self, x): \n        return self.model(x)\n\nmodel = ImageClassifier().to(device)\n\n\n# Train loop\nn_total_steps = len(train_loader)\nrunning_loss = 0.0\nprint_stat = 100\n\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):\n        # 100, 1, 28, 28 --&gt; 100 , 28 * 28\n        images = images.to(device)\n        labels = labels.to(device)\n\n        # forward\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n\n        #backwards\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        if (i + 1) % print_stat == 0:\n            with torch.no_grad():\n                n_correct = 0\n                n_samples = 0\n                for images, labels in test_loader:\n                    images = images.to(device)\n                    labels = labels.to(device)\n                    outputs = model(images)\n\n                    _, predictions = torch.max(outputs, 1)\n                    n_samples += labels.shape[0]\n                    n_correct += (predictions == labels).sum().item()\n\n                acc = 100.0 * n_correct / n_samples\n            \n            print(f'[epoch:{epoch+1}/{num_epochs}, [step:{i+1}/{n_total_steps}] loss:{(running_loss/print_stat):.4f} accuracy:{acc}')\n            running_loss = 0.0\n\n[epoch:1/4, [step:100/500] loss:2.3045 accuracy:6.866\n[epoch:1/4, [step:200/500] loss:2.3044 accuracy:6.866\n[epoch:1/4, [step:300/500] loss:2.3047 accuracy:6.866\n[epoch:1/4, [step:400/500] loss:2.3047 accuracy:6.866\n[epoch:1/4, [step:500/500] loss:2.3048 accuracy:6.866\n[epoch:2/4, [step:100/500] loss:2.3046 accuracy:6.866\n[epoch:2/4, [step:200/500] loss:2.3045 accuracy:6.866\n[epoch:2/4, [step:300/500] loss:2.3046 accuracy:6.866\n[epoch:2/4, [step:400/500] loss:2.3046 accuracy:6.866\n[epoch:2/4, [step:500/500] loss:2.3048 accuracy:6.866\n[epoch:3/4, [step:100/500] loss:2.3044 accuracy:6.866\n[epoch:3/4, [step:200/500] loss:2.3045 accuracy:6.866\n[epoch:3/4, [step:300/500] loss:2.3047 accuracy:6.866\n[epoch:3/4, [step:400/500] loss:2.3051 accuracy:6.866\n[epoch:3/4, [step:500/500] loss:2.3044 accuracy:6.866\n[epoch:4/4, [step:100/500] loss:2.3046 accuracy:6.866\n[epoch:4/4, [step:200/500] loss:2.3046 accuracy:6.866\n[epoch:4/4, [step:300/500] loss:2.3048 accuracy:6.866\n[epoch:4/4, [step:400/500] loss:2.3048 accuracy:6.866\n[epoch:4/4, [step:500/500] loss:2.3044 accuracy:6.866",
    "crumbs": [
      "Blog",
      "Model Training Loop"
    ]
  },
  {
    "objectID": "simple_cnn_in_pytorch.html",
    "href": "simple_cnn_in_pytorch.html",
    "title": "CNN",
    "section": "",
    "text": "import torch\n\n\ntorch.cuda.is_available()\n\nTrue\n\n\n\nfrom tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt\n\n\n# Import dependencies\nimport torch \nfrom PIL import Image\nfrom torch import nn, save, load\nfrom torch.optim import Adam\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets\nfrom torchvision.transforms import ToTensor\n\n# Get data \ntrain = datasets.MNIST(root=\"data\", download=True, train=True, transform=ToTensor())\ndataset = DataLoader(train, 32)\n#1,28,28 - classes 0-9\n\n# Image Classifier Neural Network\nclass ImageClassifier(nn.Module): \n    def __init__(self):\n        super().__init__()\n        self.model = nn.Sequential(\n            nn.Conv2d(1, 32, (3,3)), \n            nn.ReLU(),\n            nn.Conv2d(32, 64, (3,3)), \n            nn.ReLU(),\n            nn.Conv2d(64, 64, (3,3)), \n            nn.ReLU(),\n            nn.Flatten(), \n            nn.Linear(64*(28-6)*(28-6), 10)  \n        )\n\n    def forward(self, x): \n        return self.model(x)\n\n# Instance of the neural network, loss, optimizer \nclf = ImageClassifier().to('cuda')\nopt = Adam(clf.parameters(), lr=1e-3)\nloss_fn = nn.CrossEntropyLoss()\n\n\n# Training flow \nfor epoch in range(10): # train for 10 epochs\n    for batch in tqdm(dataset): \n        X,y = batch \n        X, y = X.to('cuda'), y.to('cuda') \n        yhat = clf(X) \n        loss = loss_fn(yhat, y) \n\n        # Apply backprop \n        opt.zero_grad()\n        loss.backward() \n        opt.step() \n\n    print(f\"Epoch:{epoch} loss is {loss.item()}\")\n\nwith open('data/model_state.pt', 'wb') as f: \n    save(clf.state_dict(), f)\n\n\n\n\nEpoch:0 loss is 0.02930162288248539\nEpoch:1 loss is 0.015122702345252037\nEpoch:2 loss is 0.0010034267324954271\nEpoch:3 loss is 0.0003837654658127576\nEpoch:4 loss is 4.9363912694389e-05\nEpoch:5 loss is 0.00015089042426552624\nEpoch:6 loss is 3.805131927947514e-05\nEpoch:7 loss is 3.0979390430729836e-05\nEpoch:8 loss is 7.227040441648569e-07\nEpoch:9 loss is 8.898725354811177e-06\ntensor(9, device='cuda:0')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nwith open('data/model_state.pt', 'rb') as f: \n    clf.load_state_dict(load(f))  \n\nimg = Image.open('data/img_3.jpg') \nplt.imshow(img, cmap = 'gray')\nimg_tensor = ToTensor()(img).unsqueeze(0).to('cuda')\n\nprint(torch.argmax(clf(img_tensor)))\n\ntensor(9, device='cuda:0')\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Blog",
      "CNN"
    ]
  },
  {
    "objectID": "torch_basics.html",
    "href": "torch_basics.html",
    "title": "Pytorch Basics",
    "section": "",
    "text": "import torch\nimport numpy as np\ntorch.cuda.is_available()\n\nTrue",
    "crumbs": [
      "Blog",
      "Pytorch Basics"
    ]
  },
  {
    "objectID": "torch_basics.html#data-types",
    "href": "torch_basics.html#data-types",
    "title": "Pytorch Basics",
    "section": "Data Types",
    "text": "Data Types\n\nfloat_tensor = torch.ones(1, dtype=torch.float)\nfloat_tensor.dtype\n\ntorch.float32\n\n\n\ndouble_tensor = torch.ones(1, dtype=torch.double)\ndouble_tensor.dtype\n\ntorch.float64\n\n\n\ncomplex_float_tensor = torch.ones(1, dtype=torch.complex64)\ncomplex_float_tensor.dtype\n\ntorch.complex64\n\n\n\ncomplex_double_tensor = torch.ones(1, dtype=torch.complex128)\ncomplex_double_tensor.dtype\n\ntorch.complex128\n\n\n\nint_tensor = torch.ones(1, dtype=torch.int)\nint_tensor.dtype\n\ntorch.int32\n\n\n\nlong_tensor = torch.ones(1, dtype=torch.long)\nlong_tensor.dtype\n\ntorch.int64\n\n\n\nuint_tensor = torch.ones(1, dtype=torch.uint8)\nuint_tensor.dtype\n\ntorch.uint8\n\n\n\ndouble_tensor = torch.ones(1, dtype=torch.double)\ndouble_tensor.dtype\n\ntorch.float64\n\n\n\nbool_tensor = torch.ones(1, dtype=torch.bool)\nbool_tensor.dtype\n\ntorch.bool",
    "crumbs": [
      "Blog",
      "Pytorch Basics"
    ]
  },
  {
    "objectID": "torch_basics.html#creation-operations",
    "href": "torch_basics.html#creation-operations",
    "title": "Pytorch Basics",
    "section": "Creation Operations",
    "text": "Creation Operations\n\ntorch.is_tensor\n\nx = torch.tensor([1, 2, 3])\nx, torch.is_tensor(x)\n\n(tensor([1, 2, 3]), True)\n\n\n\n\ntorch.set_default_device\n\ntorch.tensor([1.2, 3]).device\n\ndevice(type='cpu')\n\n\n\ntorch.set_default_device('cuda')  # current device is 0\ntorch.tensor([1.2, 3]).device\n\ndevice(type='cuda', index=0)\n\n\n\ntorch.set_default_device('cpu')\na = torch.arange(1000000)\na\n\ntensor([     0,      1,      2,  ..., 999997, 999998, 999999])\n\n\n\na + 1\n\nCPU times: user 20.7 ms, sys: 25.4 ms, total: 46.1 ms\nWall time: 7.37 ms\n\n\ntensor([      1,       2,       3,  ...,  999998,  999999, 1000000])\n\n\n\ntorch.set_default_device('cuda')\na = torch.arange(1000000)\na\n\ntensor([     0,      1,      2,  ..., 999997, 999998, 999999], device='cuda:0')\n\n\n\na + 1\n\nCPU times: user 6.29 ms, sys: 0 ns, total: 6.29 ms\nWall time: 912 µs\n\n\ntensor([      1,       2,       3,  ...,  999998,  999999, 1000000],\n       device='cuda:0')\n\n\n\n\ntorch.get_default_dtype\n\ntorch.get_default_dtype()  # initial default for floating point is torch.float32\n\ntorch.float32\n\n\n\ntorch.set_default_dtype(torch.float64)\ntorch.get_default_dtype()  # default is now changed to torch.float64\n\ntorch.float64\n\n\n\n\ntorch.set_printoptions\n\n# Limit the precision of elements\ntorch.set_printoptions(precision=2)\ntorch.tensor([1.12345])\n\ntensor([1.12], device='cuda:0')\n\n\n\n# Limit the number of elements shown\ntorch.set_printoptions(threshold=5)\ntorch.arange(10)\n\ntensor([0, 1, 2,  ..., 7, 8, 9], device='cuda:0')\n\n\n\n# Restore defaults\ntorch.set_printoptions(profile='default')\ntorch.tensor([1.12345])\n\ntensor([1.1235], device='cuda:0')\n\n\n\ntorch.set_default_device('cpu')\ntorch.arange(10)\n\ntensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n\n\n\n\ntorch.as_tensor\n\na = np.array([1, 2, 3])\nt = torch.as_tensor(a)\nt\n\ntensor([1, 2, 3])\n\n\n\nt[0] = -1\na\n\narray([-1,  2,  3])\n\n\n\na = np.array([1, 2, 3])\nt = torch.as_tensor(a, device=torch.device('cuda'))\nt\n\ntensor([1, 2, 3], device='cuda:0')\n\n\n\nt[0] = -1\na\n\narray([1, 2, 3])\n\n\n\nt\n\ntensor([-1,  2,  3], device='cuda:0')\n\n\n\n\ntorch.zeros\n\ntorch.empty((2,2))\n\ntensor([[1.3554e-20, 3.0851e-41],\n        [1.3552e-20, 3.0851e-41]])\n\n\n\ntorch.zeros(2, 3)\n\ntensor([[0., 0., 0.],\n        [0., 0., 0.]])\n\n\n\ntorch.zeros(5)\n\ntensor([0., 0., 0., 0., 0.])\n\n\n\ntorch.ones(2, 3)\n\ntensor([[1., 1., 1.],\n        [1., 1., 1.]])\n\n\n\ntorch.ones(5)\n\ntensor([1., 1., 1., 1., 1.])\n\n\n\n\ntorch.range\n\ntorch.arange(5), torch.arange(1, 4), torch.arange(1, 2.5, 0.5)\n\n(tensor([0, 1, 2, 3, 4]), tensor([1, 2, 3]), tensor([1.0000, 1.5000, 2.0000]))\n\n\n\ntorch.linspace(3, 10, steps=5),\\\ntorch.linspace(-10, 10, steps=5),\\\ntorch.linspace(start=-10, end=10, steps=5),\\\ntorch.linspace(start=-10, end=10, steps=1)\n\n(tensor([ 3.0000,  4.7500,  6.5000,  8.2500, 10.0000]),\n tensor([-10.,  -5.,   0.,   5.,  10.]),\n tensor([-10.,  -5.,   0.,   5.,  10.]),\n tensor([-10.]))\n\n\n\ntorch.logspace(start=-10, end=10, steps=5),\\\ntorch.logspace(start=0.1, end=1.0, steps=5),\\\ntorch.logspace(start=0.1, end=1.0, steps=1),\\\ntorch.logspace(start=2, end=2, steps=1, base=2)\n\n(tensor([1.0000e-10, 1.0000e-05, 1.0000e+00, 1.0000e+05, 1.0000e+10]),\n tensor([ 1.2589,  2.1135,  3.5481,  5.9566, 10.0000]),\n tensor([1.2589]),\n tensor([4.]))\n\n\n\ntorch.empty((2,3), dtype=torch.int64)\n\ntensor([[    140634107035024,     140634107035024, 7454421801564381752],\n        [2322206376936961119, 7310597164893758754,                 145]])\n\n\n\ntorch.full((2, 3), 3.141592)\n\ntensor([[3.1416, 3.1416, 3.1416],\n        [3.1416, 3.1416, 3.1416]])\n\n\n\n\ntorch.quantize_per_tensor\n\ntorch.set_default_dtype(torch.float32)\n\n\ntorch.quantize_per_tensor(torch.tensor([-1.0, 0.0, 1.0, 2.0]), 0.1, 10, torch.quint8),\\\ntorch.quantize_per_tensor(torch.tensor([-1.0, 0.0, 1.0, 2.0]), 0.1, 10, torch.quint8).int_repr(),\\\ntorch.quantize_per_tensor(torch.tensor([-1.0, 0.0, 1.0, 2.0]), torch.tensor(0.1), torch.tensor(10), torch.quint8)\n\n(tensor([-1.,  0.,  1.,  2.], size=(4,), dtype=torch.quint8,\n        quantization_scheme=torch.per_tensor_affine, scale=0.1, zero_point=10),\n tensor([ 0, 10, 20, 30], dtype=torch.uint8),\n tensor([-1.,  0.,  1.,  2.], size=(4,), dtype=torch.quint8,\n        quantization_scheme=torch.per_tensor_affine, scale=0.10000000149011612,\n        zero_point=10))\n\n\n\n\ntorch.complex\n\nreal = torch.tensor([1, 2], dtype=torch.float32)\nimag = torch.tensor([3, 4], dtype=torch.float32)\nz = torch.complex(real, imag)\nz.dtype\n\ntorch.complex64\n\n\n\nreal, imag, z\n\n(tensor([1., 2.]), tensor([3., 4.]), tensor([1.+3.j, 2.+4.j]))\n\n\n\n\ntorch.polar\n\nimport numpy as np\nabs = torch.tensor([1, 2], dtype=torch.float64)\nangle = torch.tensor([np.pi / 2, 5 * np.pi / 4], dtype=torch.float64)\nz = torch.polar(abs, angle)\nz\n\ntensor([ 6.1232e-17+1.0000j, -1.4142e+00-1.4142j], dtype=torch.complex128)",
    "crumbs": [
      "Blog",
      "Pytorch Basics"
    ]
  },
  {
    "objectID": "torch_basics.html#indexing-slicing-joining-mutating-ops",
    "href": "torch_basics.html#indexing-slicing-joining-mutating-ops",
    "title": "Pytorch Basics",
    "section": "Indexing, Slicing, Joining, Mutating Ops",
    "text": "Indexing, Slicing, Joining, Mutating Ops\n\ntorch.cat\n\nx = torch.randn(2, 3)\nx\n\ntensor([[-0.1340,  0.5254, -0.3770],\n        [-2.0310, -0.8961, -0.6459]])\n\n\n\ntorch.cat((x, x, x), 0)\n\ntensor([[-0.1340,  0.5254, -0.3770],\n        [-2.0310, -0.8961, -0.6459],\n        [-0.1340,  0.5254, -0.3770],\n        [-2.0310, -0.8961, -0.6459],\n        [-0.1340,  0.5254, -0.3770],\n        [-2.0310, -0.8961, -0.6459]])\n\n\n\ntorch.cat((x, x), 1)\n\ntensor([[-0.1340,  0.5254, -0.3770, -0.1340,  0.5254, -0.3770],\n        [-2.0310, -0.8961, -0.6459, -2.0310, -0.8961, -0.6459]])\n\n\n\n\ntorch.conj\n\nx = torch.tensor([-1 + 1j, -2 + 2j, 3 - 3j])\nx, x.is_conj()\n\n(tensor([-1.+1.j, -2.+2.j,  3.-3.j]), False)\n\n\n\ny = torch.conj(x)\ny, y.is_conj()\n\n(tensor([-1.-1.j, -2.-2.j,  3.+3.j]), True)\n\n\n\n\ntorch.permute\n\nx = torch.randn(2, 3, 5)\nx.size()\n\ntorch.Size([2, 3, 5])\n\n\n\ntorch.permute(x, (2, 0, 1)).size()\n\ntorch.Size([5, 2, 3])\n\n\n\n\ntorch.reshape\n\na = torch.arange(4.)\na\n\ntensor([0., 1., 2., 3.])\n\n\n\ntorch.reshape(a, (2, 2))\n\ntensor([[0., 1.],\n        [2., 3.]])\n\n\n\nb = torch.tensor([[0, 1], [2, 3]])\nb\n\ntensor([[0, 1],\n        [2, 3]])\n\n\n\ntorch.reshape(b, (-1,))\n\ntensor([0, 1, 2, 3])\n\n\n\n\ntorch.movedim\n\nt = torch.randn(2,3,5)\nt.size()\n\ntorch.Size([2, 3, 5])\n\n\n\ntorch.movedim(t, 1, 0).shape\n\ntorch.Size([3, 2, 5])\n\n\n\ntorch.movedim(t, 1, 0)\n\ntensor([[[-1.0370, -0.2811,  0.2693,  0.5935, -0.1354],\n         [-2.7575, -2.4650,  0.8077, -0.2873, -1.2993]],\n\n        [[-0.6173, -0.0460, -0.6329,  1.0519, -0.1674],\n         [-0.8958, -0.2828,  1.2355, -1.1782, -1.3597]],\n\n        [[-2.0996, -0.6692, -0.7840,  0.1171,  0.0334],\n         [ 0.4422,  0.2438,  1.0947, -1.2390, -1.4378]]])\n\n\n\ntorch.movedim(t, (1, 2), (0, 1)).shape\n\ntorch.Size([3, 5, 2])\n\n\n\n\ntorch.split\n\na = torch.arange(10).reshape(5, 2)\na\n\ntensor([[0, 1],\n        [2, 3],\n        [4, 5],\n        [6, 7],\n        [8, 9]])\n\n\n\ntorch.split(a, 2)\n\n(tensor([[0, 1],\n         [2, 3]]),\n tensor([[4, 5],\n         [6, 7]]),\n tensor([[8, 9]]))\n\n\n\ntorch.split(a, [1, 4])\n\n(tensor([[0, 1]]),\n tensor([[2, 3],\n         [4, 5],\n         [6, 7],\n         [8, 9]]))\n\n\n\n\ntorch.t\n\nx = torch.randn(())\nx, torch.t(x)\n\n(tensor(-0.6261), tensor(-0.6261))\n\n\n\nx = torch.randn(3)\nx, torch.t(x)\n\n(tensor([-1.2207, -0.6549, -0.0028]), tensor([-1.2207, -0.6549, -0.0028]))\n\n\n\nx = torch.randn(2, 3)\nx, torch.t(x)\n\n(tensor([[-2.0021,  1.3072, -0.9742],\n         [-1.8025,  0.5369,  0.2517]]),\n tensor([[-2.0021, -1.8025],\n         [ 1.3072,  0.5369],\n         [-0.9742,  0.2517]]))",
    "crumbs": [
      "Blog",
      "Pytorch Basics"
    ]
  },
  {
    "objectID": "torch_basics.html#rand",
    "href": "torch_basics.html#rand",
    "title": "Pytorch Basics",
    "section": "Rand",
    "text": "Rand\n\ntorch.rand\n\ntorch.rand(4)\n\ntensor([0.7377, 0.8273, 0.2958, 0.8372])\n\n\n\ntorch.rand(2, 3)\n\ntensor([[0.5028, 0.1841, 0.1133],\n        [0.4431, 0.0016, 0.8662]])\n\n\n\n\ntorch.randint\n\ntorch.randint(3, 5, (3,))\n\ntensor([4, 3, 3])\n\n\n\ntorch.randint(10, (2, 2))\n\ntensor([[5, 3],\n        [8, 1]])\n\n\n\ntorch.randint(3, 10, (2, 2))\n\ntensor([[8, 3],\n        [8, 3]])\n\n\n\n\ntorch.randn\n\ntorch.randn(4)\n\ntensor([0.2036, 0.3526, 0.7444, 1.0029])\n\n\n\ntorch.randn(2, 3)\n\ntensor([[-0.3317,  3.1649,  2.7242],\n        [ 0.2243,  1.2105,  0.9819]])\n\n\n\n\ntorch.normal\n\ntorch.normal(mean=torch.arange(1., 11.), std=torch.arange(1, 0, -0.1))\n\ntensor([ 0.3426,  0.6076,  3.0707,  3.1876,  5.1126,  6.5160,  6.9380,  8.2192,\n         8.9378, 10.0406])\n\n\n\ntorch.normal(mean=0.5, std=torch.arange(1., 6.))\n\ntensor([1.4364, 1.6189, 0.1503, 2.7895, 3.0156])\n\n\n\ntorch.normal(mean=torch.arange(1., 6.))\n\ntensor([0.8270, 0.5561, 2.5076, 2.7576, 2.9344])\n\n\n\ntorch.normal(2, 3, size=(1, 4))\n\ntensor([[-1.9720,  2.4059,  3.3461,  0.6155]])",
    "crumbs": [
      "Blog",
      "Pytorch Basics"
    ]
  },
  {
    "objectID": "torch_basics.html#save-and-load",
    "href": "torch_basics.html#save-and-load",
    "title": "Pytorch Basics",
    "section": "Save and Load",
    "text": "Save and Load\n\n# Save to file\nimport io\nx = torch.tensor([0, 1, 2, 3, 4])\ntorch.save(x, 'Data/tensor.pt')\n# Save to io.BytesIO buffer\nbuffer = io.BytesIO()\ntorch.save(x, buffer)\n\ntorch.load('tensors.pt', weights_only=True)\n\ntorch.load('tensors.pt', map_location=torch.device('cpu'), weights_only=True)\ntorch.load('tensors.pt', map_location=lambda storage, loc: storage, weights_only=True)\ntorch.load('tensors.pt', map_location=lambda storage, loc: storage.cuda(1), weights_only=True)\ntorch.load('tensors.pt', map_location={'cuda:1': 'cuda:0'}, weights_only=True)\nwith open('tensor.pt', 'rb') as f:\n    buffer = io.BytesIO(f.read())\ntorch.load(buffer, weights_only=False)\ntorch.load('module.pt', encoding='ascii', weights_only=False)",
    "crumbs": [
      "Blog",
      "Pytorch Basics"
    ]
  },
  {
    "objectID": "torch_basics.html#parallelism",
    "href": "torch_basics.html#parallelism",
    "title": "Pytorch Basics",
    "section": "Parallelism",
    "text": "Parallelism\n\ntorch.get_num_threads()\n\n6\n\n\n\ntorch.set_num_threads(12)\n\n\ntorch.get_num_interop_threads()\n\n6\n\n\n\ntorch.set_num_interop_threads(12)",
    "crumbs": [
      "Blog",
      "Pytorch Basics"
    ]
  },
  {
    "objectID": "torch_basics.html#locally-disabling-gradient-computation",
    "href": "torch_basics.html#locally-disabling-gradient-computation",
    "title": "Pytorch Basics",
    "section": "Locally disabling gradient computation",
    "text": "Locally disabling gradient computation\n\nx = torch.zeros(1, requires_grad=True)\nwith torch.no_grad():\n    y = x * 2\ny.requires_grad\n\nFalse\n\n\n\nis_train = False\nwith torch.set_grad_enabled(is_train):\n    y = x * 2\ny.requires_grad\n\nFalse\n\n\n\ntorch.set_grad_enabled(True)  # this can also be used as a function\ny = x * 2\ny.requires_grad\n\nTrue\n\n\n\ntorch.set_grad_enabled(False)\ny = x * 2\ny.requires_grad\n\nFalse\n\n\n\nno_grad\n\nx = torch.tensor([1.], requires_grad=True)\nwith torch.no_grad():\n    y = x * 2\ny.requires_grad\n\nFalse\n\n\n\n@torch.no_grad()\ndef doubler(x):\n    return x * 2\nz = doubler(x)\nz.requires_grad\n\nFalse\n\n\n\n@torch.no_grad()\ndef tripler(x):\n    return x * 3\nz = tripler(x)\nz.requires_grad\n# factory function exception\nwith torch.no_grad():\n    a = torch.nn.Parameter(torch.rand(10))\na.requires_grad\n\nTrue",
    "crumbs": [
      "Blog",
      "Pytorch Basics"
    ]
  },
  {
    "objectID": "torch_basics.html#math-operations",
    "href": "torch_basics.html#math-operations",
    "title": "Pytorch Basics",
    "section": "Math operations",
    "text": "Math operations\n\ntorch.abs(torch.tensor([-1, -2, 3]))\n\ntensor([1, 2, 3])\n\n\n\na = torch.randn(4)\na\n\ntensor([ 0.1512, -0.5116,  1.4073, -0.9758])\n\n\n\ntorch.add(a, 20)\n\ntensor([20.1512, 19.4884, 21.4073, 19.0242])\n\n\n\nb = torch.randn(4)\nc = torch.randn(4, 1)\nb,c, torch.add(b, c, alpha=10)\n\n(tensor([-0.5080, -0.2541,  0.7946, -0.7497]),\n tensor([[ 0.3399],\n         [-0.8642],\n         [-1.4262],\n         [ 0.3894]]),\n tensor([[  2.8912,   3.1451,   4.1939,   2.6495],\n         [ -9.1502,  -8.8963,  -7.8475,  -9.3918],\n         [-14.7701, -14.5162, -13.4674, -15.0117],\n         [  3.3861,   3.6400,   4.6887,   3.1444]]))\n\n\n\ntorch.asin(a)\n\ntensor([ 0.1518, -0.5370,     nan, -1.3503])\n\n\n\ntorch.bitwise_not(torch.tensor([-1, -2, 3], dtype=torch.int8))\n\ntensor([ 0,  1, -4], dtype=torch.int8)\n\n\n\ntorch.ceil(a)\n\ntensor([1., -0., 2., -0.])\n\n\n\ntorch.clamp(a, min=-0.5, max=0.5)\n\ntensor([ 0.1512, -0.5000,  0.5000, -0.5000])\n\n\n\nmin = torch.linspace(-1, 1, steps=4)\nmin, torch.clamp(a, min=min)\n\n(tensor([-1.0000, -0.3333,  0.3333,  1.0000]),\n tensor([ 0.1512, -0.3333,  1.4073,  1.0000]))\n\n\n\nGradient\n\n# Estimates the gradient of f(x)=x^2 at points [-2, -1, 2, 4]\ncoordinates = (torch.tensor([-2., -1., 1., 4.]),)\nvalues = torch.tensor([4., 1., 1., 16.], )\ntorch.gradient(values, spacing = coordinates)\n\n(tensor([-3., -2.,  2.,  5.]),)\n\n\n\n# Estimates the gradient of the R^2 -&gt; R function whose samples are\n# described by the tensor t. Implicit coordinates are [0, 1] for the outermost\n# dimension and [0, 1, 2, 3] for the innermost dimension, and function estimates\n# partial derivative for both dimensions.\nt = torch.tensor([[1, 2, 4, 8], [10, 20, 40, 80]])\ntorch.gradient(t)\n\n(tensor([[ 9., 18., 36., 72.],\n         [ 9., 18., 36., 72.]]),\n tensor([[ 1.0000,  1.5000,  3.0000,  4.0000],\n         [10.0000, 15.0000, 30.0000, 40.0000]]))\n\n\n\n# A scalar value for spacing modifies the relationship between tensor indices\n# and input coordinates by multiplying the indices to find the\n# coordinates. For example, below the indices of the innermost\n# 0, 1, 2, 3 translate to coordinates of [0, 2, 4, 6], and the indices of\n# the outermost dimension 0, 1 translate to coordinates of [0, 2].\ntorch.gradient(t, spacing = 2.0) # dim = None (implicitly [0, 1])\n# doubling the spacing between samples halves the estimated partial gradients.\n\n(tensor([[ 4.5000,  9.0000, 18.0000, 36.0000],\n         [ 4.5000,  9.0000, 18.0000, 36.0000]]),\n tensor([[ 0.5000,  0.7500,  1.5000,  2.0000],\n         [ 5.0000,  7.5000, 15.0000, 20.0000]]))\n\n\n\n# Estimates only the partial derivative for dimension 1\ntorch.gradient(t, dim = 1) # spacing = None (implicitly 1.)\n\n(tensor([[ 1.0000,  1.5000,  3.0000,  4.0000],\n         [10.0000, 15.0000, 30.0000, 40.0000]]),)\n\n\n\n# When spacing is a list of scalars, the relationship between the tensor\n# indices and input coordinates changes based on dimension.\n# For example, below, the indices of the innermost dimension 0, 1, 2, 3 translate\n# to coordinates of [0, 3, 6, 9], and the indices of the outermost dimension\n# 0, 1 translate to coordinates of [0, 2].\ntorch.gradient(t, spacing = [3., 2.])\n\n(tensor([[ 3.,  6., 12., 24.],\n         [ 3.,  6., 12., 24.]]),\n tensor([[ 0.5000,  0.7500,  1.5000,  2.0000],\n         [ 5.0000,  7.5000, 15.0000, 20.0000]]))\n\n\n\n# The following example is a replication of the previous one with explicit\n# coordinates.\ncoords = (torch.tensor([0, 2]), torch.tensor([0, 3, 6, 9]))\ntorch.gradient(t, spacing = coords)\n\n(tensor([[ 4.5000,  9.0000, 18.0000, 36.0000],\n         [ 4.5000,  9.0000, 18.0000, 36.0000]]),\n tensor([[ 0.3333,  0.5000,  1.0000,  1.3333],\n         [ 3.3333,  5.0000, 10.0000, 13.3333]]))\n\n\n\n\nReduction Ops\n\na = torch.randn(4, 4)\na\n\ntensor([[ 0.5653, -1.6250, -1.7234,  1.6898],\n        [ 0.0964,  0.4920, -0.8990,  0.2050],\n        [-0.0388, -0.3062, -2.7269,  2.2214],\n        [-0.4256, -1.3614,  1.6437,  1.3073]])\n\n\n\ntorch.argmax(a), torch.argmin(a)\n\n(tensor(11), tensor(10))\n\n\n\ntorch.argmax(a, dim=1), torch.argmin(a, dim=1)\n\n(tensor([3, 1, 3, 2]), tensor([2, 2, 2, 1]))\n\n\n\ntorch.argmin(a, dim=1, keepdim=True)\n\ntensor([[2],\n        [2],\n        [2],\n        [1]])\n\n\n\ntorch.amax(a, 0)\n\ntensor([0.5653, 0.4920, 1.6437, 2.2214])\n\n\n\ntorch.amin(a, 0)\n\ntensor([-0.4256, -1.6250, -2.7269,  0.2050])\n\n\nTests if all element in input evaluates to True.\n\ntorch.all(a)\n\ntensor(True)\n\n\nTests if any element in input evaluates to True.\n\ntorch.any(a)\n\ntensor(True)\n\n\n\nx = torch.randn(4)\ny = torch.randn(4)\nx,y\n\n(tensor([-0.8039, -1.4679,  0.4484, -0.5348]),\n tensor([-0.5979, -1.1656, -1.2298, -0.2573]))\n\n\n\ntorch.dist(x, y, 3.5)\n\ntensor(1.6806)\n\n\n\ntorch.dist(x, y, 3)\n\ntensor(1.6850)\n\n\n\ntorch.dist(x, y, 0)\n\ntensor(4.)\n\n\n\ntorch.dist(x, y, 2)\n\ntensor(1.7399)\n\n\n\na = torch.randn(1, 3)\na\n\ntensor([[-0.7396, -1.3435,  1.0013]])\n\n\n\ntorch.mean(a)\n\ntensor(-0.3606)\n\n\n\ntorch.prod(a)\n\ntensor(0.9949)\n\n\n\na = torch.randn(4, 2)\na\n\ntensor([[-0.0451, -0.4858],\n        [-0.1007,  0.4423],\n        [-0.2149,  0.4494],\n        [ 0.7059,  0.0417]])\n\n\n\ntorch.prod(a, 1)\n\ntensor([ 0.0219, -0.0446, -0.0966,  0.0295])\n\n\n\ntorch.std(a, dim=1, keepdim=True)\n\ntensor([[0.3116],\n        [0.3840],\n        [0.4697],\n        [0.4696]])\n\n\n\ntorch.sum(a)\n\ntensor(0.7927)\n\n\n\ntorch.sum(a, 0)\n\ntensor([0.3451, 0.4476])\n\n\n\ntorch.var(a, dim=0, keepdim=True)\n\ntensor([[0.1756, 0.1951]])\n\n\n\ntorch.var_mean(a, dim=0, keepdim=True)\n\n(tensor([[0.1756, 0.1951]]), tensor([[0.0863, 0.1119]]))\n\n\n\n\nComparison Ops\n\ntorch.eq(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]]))\n\ntensor([[ True, False],\n        [False,  True]])\n\n\n\ntorch.allclose(torch.tensor([10000., 1e-07]), torch.tensor([10000.1, 1e-08]))\n\nFalse\n\n\n\ntorch.isnan(torch.tensor([1, float('nan'), 2]))\n\ntensor([False,  True, False])\n\n\n\n\nBroadcast\n\nx = torch.tensor([1, 2, 3])\ntorch.broadcast_to(x, (3, 3))\n\ntensor([[1, 2, 3],\n        [1, 2, 3],\n        [1, 2, 3]])\n\n\n\ntorch.broadcast_shapes((2,), (3, 1), (1, 1, 1))\n\ntorch.Size([1, 3, 2])\n\n\n\nx = torch.tensor([[0, 2], [1, 1], [2, 0]]).T\nx\nx, torch.cov(x)\n\n(tensor([[0, 1, 2],\n         [2, 1, 0]]),\n tensor([[ 1., -1.],\n         [-1.,  1.]]))\n\n\n\n\nCovolution\n\ntorch.cov(x, correction=0)\n\ntensor([[ 0.6667, -0.6667],\n        [-0.6667,  0.6667]])\n\n\n\nfw = torch.randint(1, 10, (3,))\naw = torch.rand(3)\nfw, aw, torch.cov(x, fweights=fw, aweights=aw)\n\n(tensor([2, 2, 9]),\n tensor([0.1281, 0.5609, 0.1982]),\n tensor([[ 0.4583, -0.4583],\n         [-0.4583,  0.4583]]))\n\n\n\n\nDiagonal\n\na = torch.randn(3)\na, torch.diag(a)\n\n(tensor([-0.0159, -0.2550, -0.3652]),\n tensor([[-0.0159,  0.0000,  0.0000],\n         [ 0.0000, -0.2550,  0.0000],\n         [ 0.0000,  0.0000, -0.3652]]))\n\n\n\ntorch.diag(a, 1)\n\ntensor([[ 0.0000, -0.0159,  0.0000,  0.0000],\n        [ 0.0000,  0.0000, -0.2550,  0.0000],\n        [ 0.0000,  0.0000,  0.0000, -0.3652],\n        [ 0.0000,  0.0000,  0.0000,  0.0000]])\n\n\n\na = torch.randn(3, 3)\na\n\ntensor([[ 0.8558, -0.8227,  1.5082],\n        [-0.6674, -0.0815, -0.5271],\n        [-0.1011, -0.3513,  0.1919]])\n\n\n\ntorch.diag(a, 0)\n\ntensor([ 0.8558, -0.0815,  0.1919])\n\n\n\ntorch.diag(a, 1)\n\ntensor([-0.8227, -0.5271])\n\n\n\n\nDiff\n\na = torch.tensor([1, 3, 2])\na, torch.diff(a)\n\n(tensor([1, 3, 2]), tensor([ 2, -1]))\n\n\n\ntorch.gradient(a)\n\n(tensor([ 2.0000,  0.5000, -1.0000]),)\n\n\n\nb = torch.tensor([4, 5])\ntorch.diff(a, append=b)\n\ntensor([ 2, -1,  2,  1])\n\n\n\nc = torch.tensor([[1, 2, 3], [3, 4, 5]])\nc\n\ntensor([[1, 2, 3],\n        [3, 4, 5]])\n\n\n\ntorch.diff(c, dim=0)\n\ntensor([[2, 2, 2]])\n\n\n\ntorch.diff(c, dim=1)\n\ntensor([[1, 1],\n        [1, 1]])\n\n\n\n\nEinsum\n\n# trace\ntorch.einsum('ii', torch.randn(4, 4))\n\ntensor(2.8873)\n\n\n\n# diagonal\ntorch.einsum('ii-&gt;i', torch.randn(4, 4))\n\ntensor([ 0.1727, -1.2934,  0.1134,  0.6699])\n\n\n\n# outer product\nx = torch.randn(5)\ny = torch.randn(4)\nx,y\n\n(tensor([ 0.8261,  2.2608,  0.5666, -2.3195, -1.1706]),\n tensor([-0.1575,  1.3682, -1.6248, -0.4177]))\n\n\n\ntorch.einsum('i,j-&gt;ij', x, y)\n\ntensor([[-0.1301,  1.1302, -1.3423, -0.3451],\n        [-0.3561,  3.0932, -3.6735, -0.9444],\n        [-0.0892,  0.7751, -0.9206, -0.2367],\n        [ 0.3653, -3.1735,  3.7688,  0.9689],\n        [ 0.1844, -1.6016,  1.9021,  0.4890]])\n\n\n\n# batch matrix multiplication\nAs = torch.randn(3, 2, 5)\nBs = torch.randn(3, 5, 4)\nAs, Bs\n\n(tensor([[[ 1.7602,  1.3836, -0.8395,  1.2415,  0.2106],\n          [-1.0765, -1.6569,  1.0785,  2.6039, -0.2173]],\n \n         [[-0.3606,  0.7737, -0.3265, -0.3982, -0.1795],\n          [ 0.4787,  0.3987,  0.5030,  2.0617, -1.2417]],\n \n         [[ 1.2606, -1.7532,  1.2267,  0.2588, -0.6794],\n          [ 0.4158,  0.3457, -0.3235, -1.4921, -1.0168]]]),\n tensor([[[-1.1582, -0.5702, -0.5820,  1.4487],\n          [ 1.6068,  0.6026,  1.4152,  0.7676],\n          [-1.3955, -0.1634, -0.9673, -1.1214],\n          [ 0.8568, -1.2561, -1.0304,  0.8901],\n          [ 0.1840,  0.1531,  1.4496,  1.6163]],\n \n         [[-0.0585, -0.2001,  0.1084, -0.9644],\n          [-0.6568,  1.1768, -1.4877,  0.0249],\n          [-0.1339,  0.2070,  1.4734,  0.0627],\n          [ 0.3322,  0.5430, -0.5031,  0.1328],\n          [ 0.0214,  1.1490,  0.4914,  0.2774]],\n \n         [[ 0.3791,  0.3227, -2.6659,  0.6282],\n          [ 1.6384,  0.2688, -2.1299,  1.0743],\n          [ 0.1086, -0.0778, -0.2566,  0.0419],\n          [ 0.0142,  1.9122, -1.7333,  1.9668],\n          [ 0.1300,  0.8281, -1.0565, -1.4234]]]))\n\n\n\ntorch.einsum('bij,bjk-&gt;bik', As, Bs)\n\ntensor([[[ 2.4586, -1.5598,  0.7718,  5.9990],\n         [-0.7296, -3.8648, -5.7596, -2.0744]],\n\n        [[-0.5795,  0.4926, -1.5591,  0.2439],\n         [ 0.3011,  0.1702, -1.4475, -0.4909]],\n\n        [[-2.3460, -0.2277,  0.3280,  0.4358],\n         [ 0.5354, -3.4429,  1.8989, -0.8684]]])\n\n\n\n# with sublist format and ellipsis\ntorch.einsum(As, [..., 0, 1], Bs, [..., 1, 2], [..., 0, 2])\n\ntensor([[[ 2.4586, -1.5598,  0.7718,  5.9990],\n         [-0.7296, -3.8648, -5.7596, -2.0744]],\n\n        [[-0.5795,  0.4926, -1.5591,  0.2439],\n         [ 0.3011,  0.1702, -1.4475, -0.4909]],\n\n        [[-2.3460, -0.2277,  0.3280,  0.4358],\n         [ 0.5354, -3.4429,  1.8989, -0.8684]]])\n\n\n\n# batch permute\nA = torch.randn(2, 3, 4, 5)\ntorch.einsum('...ij-&gt;...ji', A).shape\n\ntorch.Size([2, 3, 5, 4])\n\n\n\n# equivalent to torch.nn.functional.bilinear\nA = torch.randn(3, 5, 4)\nl = torch.randn(2, 5)\nr = torch.randn(2, 4)\nA,l,r\n\n(tensor([[[-0.5127, -0.0817, -0.5872, -2.0090],\n          [-0.3169, -1.0569, -0.2818,  1.8631],\n          [-1.5130, -0.7615, -0.3052,  0.7982],\n          [-0.3297,  1.6522,  0.9849, -1.5223],\n          [-0.5275,  0.1215, -0.5165, -0.4254]],\n \n         [[ 0.7555, -0.9271,  2.2486, -0.5548],\n          [ 0.0759, -0.3391, -1.3095, -0.2525],\n          [-0.2529, -1.0799,  0.5418,  0.4821],\n          [ 0.8987, -0.0494, -0.5371, -1.5568],\n          [-0.2188,  0.9023,  0.8624,  0.7310]],\n \n         [[ 2.1191,  0.3084, -0.8052, -0.2008],\n          [-0.3211, -0.4985,  0.3982, -1.0806],\n          [ 0.6403, -0.8154, -2.6253,  2.4096],\n          [ 0.5290, -1.3181, -0.8800,  0.9082],\n          [-0.8891, -1.0462,  1.2305, -0.7983]]]),\n tensor([[ 1.1455, -1.6298,  0.2697,  1.6974, -1.9843],\n         [ 0.5881, -0.7756,  0.5787, -0.1486, -1.0844]]),\n tensor([[-0.4899,  0.0820, -0.1346, -0.0343],\n         [-0.6835,  0.4302, -0.0725,  0.6343]]))\n\n\n\ntorch.einsum('bn,anm,bm-&gt;ba', l, A, r)\n\ntensor([[ 0.2351, -1.6662, -2.1157],\n        [-0.7526, -1.4344,  0.7918]])\n\n\n\n\ntorch.flatten\n\nt = torch.tensor([[[1, 2],\n                   [3, 4]],\n                  [[5, 6],\n                   [7, 8]]])\ntorch.flatten(t)\n\ntensor([1, 2, 3, 4, 5, 6, 7, 8])\n\n\n\ntorch.flatten(t, start_dim=1)\n\ntensor([[1, 2, 3, 4],\n        [5, 6, 7, 8]])\n\n\n\n\ntorch.histogram\n\ntorch.histogram(torch.tensor([1., 2, 1]),\n                bins=4,\n                range=(0., 3.),\n                weight=torch.tensor([1., 2., 4.]))\n\ntorch.return_types.histogram(\nhist=tensor([0., 5., 2., 0.]),\nbin_edges=tensor([0.0000, 0.7500, 1.5000, 2.2500, 3.0000]))\n\n\n\ntorch.histogram(torch.tensor([1., 2, 1]),\n                bins=4, range=(0., 3.),\n                weight=torch.tensor([1., 2., 4.]),\n                density=True)\n\ntorch.return_types.histogram(\nhist=tensor([0.0000, 0.9524, 0.3810, 0.0000]),\nbin_edges=tensor([0.0000, 0.7500, 1.5000, 2.2500, 3.0000]))\n\n\n\n\ntorch.meshgrid\n\nx = torch.tensor([1, 2, 3])\ny = torch.tensor([4, 5, 6])\nx,y\n\n(tensor([1, 2, 3]), tensor([4, 5, 6]))\n\n\n\ngrid_x, grid_y = torch.meshgrid(x, y, indexing='ij')\ngrid_x,grid_y\n\n(tensor([[1, 1, 1],\n         [2, 2, 2],\n         [3, 3, 3]]),\n tensor([[4, 5, 6],\n         [4, 5, 6],\n         [4, 5, 6]]))\n\n\n\ntorch.equal(torch.cat(tuple(torch.dstack([grid_x, grid_y]))),\n            torch.cartesian_prod(x, y))\n\nimport matplotlib.pyplot as plt\nxs = torch.linspace(-5, 5, steps=100)\nys = torch.linspace(-5, 5, steps=100)\nx, y = torch.meshgrid(xs, ys, indexing='xy')\nz = torch.sin(torch.sqrt(x * x + y * y))\nax = plt.axes(projection='3d')\nax.plot_surface(x.numpy(), y.numpy(), z.numpy())\nplt.show()",
    "crumbs": [
      "Blog",
      "Pytorch Basics"
    ]
  },
  {
    "objectID": "torch_basics.html#torch.matmul",
    "href": "torch_basics.html#torch.matmul",
    "title": "Pytorch Basics",
    "section": "torch.matmul",
    "text": "torch.matmul\n\n# vector x vector\ntensor1 = torch.randn(3)\ntensor2 = torch.randn(3)\ntensor1, tensor2\n\n(tensor([0.6342, 0.6817, 0.5164]), tensor([-0.5737,  1.6013,  0.5605]))\n\n\n\ntorch.matmul(tensor1, tensor2)\n\ntensor(1.0173)\n\n\n\n# matrix x vector\ntensor1 = torch.randn(3, 4)\ntensor2 = torch.randn(4)\ntensor1, tensor2\n\n(tensor([[-0.7989, -0.2968, -0.5672, -0.3673],\n         [ 0.3948,  0.9695, -0.9593,  0.5856],\n         [-0.6744,  1.4336, -1.3985, -0.2974]]),\n tensor([-2.0189, -1.3822, -0.2220,  0.0440]))\n\n\n\ntorch.matmul(tensor1, tensor2).size(), torch.matmul(tensor1, tensor2)\n\n(torch.Size([3]), tensor([ 2.1329, -1.8983, -0.3226]))\n\n\n\n# batched matrix x broadcasted vector\ntensor1 = torch.randn(10, 3, 4)\ntensor2 = torch.randn(4)\ntorch.matmul(tensor1, tensor2).size()\n\ntorch.Size([10, 3])\n\n\n\n# batched matrix x batched matrix\ntensor1 = torch.randn(10, 3, 4)\ntensor2 = torch.randn(10, 4, 5)\ntorch.matmul(tensor1, tensor2).size()\n\ntorch.Size([10, 3, 5])\n\n\n\n# batched matrix x broadcasted matrix\ntensor1 = torch.randn(10, 3, 4)\ntensor2 = torch.randn(4, 5)\ntorch.matmul(tensor1, tensor2).size()\n\ntorch.Size([10, 3, 5])",
    "crumbs": [
      "Blog",
      "Pytorch Basics"
    ]
  },
  {
    "objectID": "timm_models_torch.html",
    "href": "timm_models_torch.html",
    "title": "Timm - Loading Models",
    "section": "",
    "text": "!pip list | grep timm\n\ntimm                      1.0.3\n\n\n\nimport timm\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\ntimm??\n\n\nType:        module\nString form: &lt;module 'timm' from '/home/ben/miniconda3/envs/pfast/lib/python3.12/site-packages/timm/__init__.py'&gt;\nFile:        ~/miniconda3/envs/pfast/lib/python3.12/site-packages/timm/__init__.py\nSource:     \nfrom .version import __version__\nfrom .layers import is_scriptable, is_exportable, set_scriptable, set_exportable\nfrom .models import create_model, list_models, list_pretrained, is_model, list_modules, model_entrypoint, \\\n    is_model_pretrained, get_pretrained_cfg, get_pretrained_cfg_value",
    "crumbs": [
      "Blog",
      "Timm - Loading Models"
    ]
  },
  {
    "objectID": "timm_models_torch.html#using-timm",
    "href": "timm_models_torch.html#using-timm",
    "title": "Timm - Loading Models",
    "section": "",
    "text": "!pip list | grep timm\n\ntimm                      1.0.3\n\n\n\nimport timm\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\ntimm??\n\n\nType:        module\nString form: &lt;module 'timm' from '/home/ben/miniconda3/envs/pfast/lib/python3.12/site-packages/timm/__init__.py'&gt;\nFile:        ~/miniconda3/envs/pfast/lib/python3.12/site-packages/timm/__init__.py\nSource:     \nfrom .version import __version__\nfrom .layers import is_scriptable, is_exportable, set_scriptable, set_exportable\nfrom .models import create_model, list_models, list_pretrained, is_model, list_modules, model_entrypoint, \\\n    is_model_pretrained, get_pretrained_cfg, get_pretrained_cfg_value",
    "crumbs": [
      "Blog",
      "Timm - Loading Models"
    ]
  },
  {
    "objectID": "timm_models_torch.html#list-models",
    "href": "timm_models_torch.html#list-models",
    "title": "Timm - Loading Models",
    "section": "List Models",
    "text": "List Models\n\nprint(timm.list_models(\"resnet*\"))\n\n['resnet10t', 'resnet14t', 'resnet18', 'resnet18d', 'resnet26', 'resnet26d', 'resnet26t', 'resnet32ts', 'resnet33ts', 'resnet34', 'resnet34d', 'resnet50', 'resnet50_gn', 'resnet50c', 'resnet50d', 'resnet50s', 'resnet50t', 'resnet51q', 'resnet61q', 'resnet101', 'resnet101c', 'resnet101d', 'resnet101s', 'resnet152', 'resnet152c', 'resnet152d', 'resnet152s', 'resnet200', 'resnet200d', 'resnetaa34d', 'resnetaa50', 'resnetaa50d', 'resnetaa101d', 'resnetblur18', 'resnetblur50', 'resnetblur50d', 'resnetblur101d', 'resnetrs50', 'resnetrs101', 'resnetrs152', 'resnetrs200', 'resnetrs270', 'resnetrs350', 'resnetrs420', 'resnetv2_50', 'resnetv2_50d', 'resnetv2_50d_evos', 'resnetv2_50d_frn', 'resnetv2_50d_gn', 'resnetv2_50t', 'resnetv2_50x1_bit', 'resnetv2_50x3_bit', 'resnetv2_101', 'resnetv2_101d', 'resnetv2_101x1_bit', 'resnetv2_101x3_bit', 'resnetv2_152', 'resnetv2_152d', 'resnetv2_152x2_bit', 'resnetv2_152x4_bit']\n\n\n\nlen(timm.list_models())\n\n1063",
    "crumbs": [
      "Blog",
      "Timm - Loading Models"
    ]
  },
  {
    "objectID": "timm_models_torch.html#create-model",
    "href": "timm_models_torch.html#create-model",
    "title": "Timm - Loading Models",
    "section": "Create Model",
    "text": "Create Model\n\nmodel = timm.create_model('resnet18', pretrained=True)",
    "crumbs": [
      "Blog",
      "Timm - Loading Models"
    ]
  },
  {
    "objectID": "timm_models_torch.html#list-pretrained",
    "href": "timm_models_torch.html#list-pretrained",
    "title": "Timm - Loading Models",
    "section": "List Pretrained",
    "text": "List Pretrained\n\nprint(timm.list_pretrained(\"resnet*\"))\n\n['resnet10t.c3_in1k', 'resnet14t.c3_in1k', 'resnet18.a1_in1k', 'resnet18.a2_in1k', 'resnet18.a3_in1k', 'resnet18.fb_ssl_yfcc100m_ft_in1k', 'resnet18.fb_swsl_ig1b_ft_in1k', 'resnet18.gluon_in1k', 'resnet18.tv_in1k', 'resnet18d.ra2_in1k', 'resnet26.bt_in1k', 'resnet26d.bt_in1k', 'resnet26t.ra2_in1k', 'resnet32ts.ra2_in1k', 'resnet33ts.ra2_in1k', 'resnet34.a1_in1k', 'resnet34.a2_in1k', 'resnet34.a3_in1k', 'resnet34.bt_in1k', 'resnet34.gluon_in1k', 'resnet34.tv_in1k', 'resnet34d.ra2_in1k', 'resnet50.a1_in1k', 'resnet50.a1h_in1k', 'resnet50.a2_in1k', 'resnet50.a3_in1k', 'resnet50.am_in1k', 'resnet50.b1k_in1k', 'resnet50.b2k_in1k', 'resnet50.bt_in1k', 'resnet50.c1_in1k', 'resnet50.c2_in1k', 'resnet50.d_in1k', 'resnet50.fb_ssl_yfcc100m_ft_in1k', 'resnet50.fb_swsl_ig1b_ft_in1k', 'resnet50.gluon_in1k', 'resnet50.ra_in1k', 'resnet50.ram_in1k', 'resnet50.tv2_in1k', 'resnet50.tv_in1k', 'resnet50_gn.a1h_in1k', 'resnet50c.gluon_in1k', 'resnet50d.a1_in1k', 'resnet50d.a2_in1k', 'resnet50d.a3_in1k', 'resnet50d.gluon_in1k', 'resnet50d.ra2_in1k', 'resnet50s.gluon_in1k', 'resnet51q.ra2_in1k', 'resnet61q.ra2_in1k', 'resnet101.a1_in1k', 'resnet101.a1h_in1k', 'resnet101.a2_in1k', 'resnet101.a3_in1k', 'resnet101.gluon_in1k', 'resnet101.tv2_in1k', 'resnet101.tv_in1k', 'resnet101c.gluon_in1k', 'resnet101d.gluon_in1k', 'resnet101d.ra2_in1k', 'resnet101s.gluon_in1k', 'resnet152.a1_in1k', 'resnet152.a1h_in1k', 'resnet152.a2_in1k', 'resnet152.a3_in1k', 'resnet152.gluon_in1k', 'resnet152.tv2_in1k', 'resnet152.tv_in1k', 'resnet152c.gluon_in1k', 'resnet152d.gluon_in1k', 'resnet152d.ra2_in1k', 'resnet152s.gluon_in1k', 'resnet200d.ra2_in1k', 'resnetaa50.a1h_in1k', 'resnetaa50d.d_in12k', 'resnetaa50d.sw_in12k', 'resnetaa50d.sw_in12k_ft_in1k', 'resnetaa101d.sw_in12k', 'resnetaa101d.sw_in12k_ft_in1k', 'resnetblur50.bt_in1k', 'resnetrs50.tf_in1k', 'resnetrs101.tf_in1k', 'resnetrs152.tf_in1k', 'resnetrs200.tf_in1k', 'resnetrs270.tf_in1k', 'resnetrs350.tf_in1k', 'resnetrs420.tf_in1k', 'resnetv2_50.a1h_in1k', 'resnetv2_50d_evos.ah_in1k', 'resnetv2_50d_gn.ah_in1k', 'resnetv2_50x1_bit.goog_distilled_in1k', 'resnetv2_50x1_bit.goog_in21k', 'resnetv2_50x1_bit.goog_in21k_ft_in1k', 'resnetv2_50x3_bit.goog_in21k', 'resnetv2_50x3_bit.goog_in21k_ft_in1k', 'resnetv2_101.a1h_in1k', 'resnetv2_101x1_bit.goog_in21k', 'resnetv2_101x1_bit.goog_in21k_ft_in1k', 'resnetv2_101x3_bit.goog_in21k', 'resnetv2_101x3_bit.goog_in21k_ft_in1k', 'resnetv2_152x2_bit.goog_in21k', 'resnetv2_152x2_bit.goog_in21k_ft_in1k', 'resnetv2_152x2_bit.goog_teacher_in21k_ft_in1k', 'resnetv2_152x2_bit.goog_teacher_in21k_ft_in1k_384', 'resnetv2_152x4_bit.goog_in21k', 'resnetv2_152x4_bit.goog_in21k_ft_in1k']",
    "crumbs": [
      "Blog",
      "Timm - Loading Models"
    ]
  },
  {
    "objectID": "timm_models_torch.html#using-torchvision-models",
    "href": "timm_models_torch.html#using-torchvision-models",
    "title": "Timm - Loading Models",
    "section": "Using torchvision models",
    "text": "Using torchvision models\n\nimport torchvision.models\n\n\nprint(torchvision.models.list_models())\n\n['alexnet', 'convnext_base', 'convnext_large', 'convnext_small', 'convnext_tiny', 'deeplabv3_mobilenet_v3_large', 'deeplabv3_resnet101', 'deeplabv3_resnet50', 'densenet121', 'densenet161', 'densenet169', 'densenet201', 'efficientnet_b0', 'efficientnet_b1', 'efficientnet_b2', 'efficientnet_b3', 'efficientnet_b4', 'efficientnet_b5', 'efficientnet_b6', 'efficientnet_b7', 'efficientnet_v2_l', 'efficientnet_v2_m', 'efficientnet_v2_s', 'fasterrcnn_mobilenet_v3_large_320_fpn', 'fasterrcnn_mobilenet_v3_large_fpn', 'fasterrcnn_resnet50_fpn', 'fasterrcnn_resnet50_fpn_v2', 'fcn_resnet101', 'fcn_resnet50', 'fcos_resnet50_fpn', 'googlenet', 'inception_v3', 'keypointrcnn_resnet50_fpn', 'lraspp_mobilenet_v3_large', 'maskrcnn_resnet50_fpn', 'maskrcnn_resnet50_fpn_v2', 'maxvit_t', 'mc3_18', 'mnasnet0_5', 'mnasnet0_75', 'mnasnet1_0', 'mnasnet1_3', 'mobilenet_v2', 'mobilenet_v3_large', 'mobilenet_v3_small', 'mvit_v1_b', 'mvit_v2_s', 'quantized_googlenet', 'quantized_inception_v3', 'quantized_mobilenet_v2', 'quantized_mobilenet_v3_large', 'quantized_resnet18', 'quantized_resnet50', 'quantized_resnext101_32x8d', 'quantized_resnext101_64x4d', 'quantized_shufflenet_v2_x0_5', 'quantized_shufflenet_v2_x1_0', 'quantized_shufflenet_v2_x1_5', 'quantized_shufflenet_v2_x2_0', 'r2plus1d_18', 'r3d_18', 'raft_large', 'raft_small', 'regnet_x_16gf', 'regnet_x_1_6gf', 'regnet_x_32gf', 'regnet_x_3_2gf', 'regnet_x_400mf', 'regnet_x_800mf', 'regnet_x_8gf', 'regnet_y_128gf', 'regnet_y_16gf', 'regnet_y_1_6gf', 'regnet_y_32gf', 'regnet_y_3_2gf', 'regnet_y_400mf', 'regnet_y_800mf', 'regnet_y_8gf', 'resnet101', 'resnet152', 'resnet18', 'resnet34', 'resnet50', 'resnext101_32x8d', 'resnext101_64x4d', 'resnext50_32x4d', 'retinanet_resnet50_fpn', 'retinanet_resnet50_fpn_v2', 's3d', 'shufflenet_v2_x0_5', 'shufflenet_v2_x1_0', 'shufflenet_v2_x1_5', 'shufflenet_v2_x2_0', 'squeezenet1_0', 'squeezenet1_1', 'ssd300_vgg16', 'ssdlite320_mobilenet_v3_large', 'swin3d_b', 'swin3d_s', 'swin3d_t', 'swin_b', 'swin_s', 'swin_t', 'swin_v2_b', 'swin_v2_s', 'swin_v2_t', 'vgg11', 'vgg11_bn', 'vgg13', 'vgg13_bn', 'vgg16', 'vgg16_bn', 'vgg19', 'vgg19_bn', 'vit_b_16', 'vit_b_32', 'vit_h_14', 'vit_l_16', 'vit_l_32', 'wide_resnet101_2', 'wide_resnet50_2']",
    "crumbs": [
      "Blog",
      "Timm - Loading Models"
    ]
  },
  {
    "objectID": "timm_models_torch.html#model-summary",
    "href": "timm_models_torch.html#model-summary",
    "title": "Timm - Loading Models",
    "section": "Model Summary",
    "text": "Model Summary\n\nfrom torchinfo import summary\nimport numpy as np\n\n\ntimm.create_model?\n\n\nmodel = timm.create_model('resnet50', pretrained=True, num_classes=10)\nrandom_input = np.random.rand(1, 3, 100, 100)\n\n\n\n\n\nmodel\n\nResNet(\n  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (act1): ReLU(inplace=True)\n  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (layer1): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act1): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act2): ReLU(inplace=True)\n      (aa): Identity()\n      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act3): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act1): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act2): ReLU(inplace=True)\n      (aa): Identity()\n      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act3): ReLU(inplace=True)\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act1): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act2): ReLU(inplace=True)\n      (aa): Identity()\n      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act3): ReLU(inplace=True)\n    )\n  )\n  (layer2): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act1): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act2): ReLU(inplace=True)\n      (aa): Identity()\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act3): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act1): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act2): ReLU(inplace=True)\n      (aa): Identity()\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act3): ReLU(inplace=True)\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act1): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act2): ReLU(inplace=True)\n      (aa): Identity()\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act3): ReLU(inplace=True)\n    )\n    (3): Bottleneck(\n      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act1): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act2): ReLU(inplace=True)\n      (aa): Identity()\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act3): ReLU(inplace=True)\n    )\n  )\n  (layer3): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act1): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act2): ReLU(inplace=True)\n      (aa): Identity()\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act3): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act1): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act2): ReLU(inplace=True)\n      (aa): Identity()\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act3): ReLU(inplace=True)\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act1): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act2): ReLU(inplace=True)\n      (aa): Identity()\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act3): ReLU(inplace=True)\n    )\n    (3): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act1): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act2): ReLU(inplace=True)\n      (aa): Identity()\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act3): ReLU(inplace=True)\n    )\n    (4): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act1): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act2): ReLU(inplace=True)\n      (aa): Identity()\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act3): ReLU(inplace=True)\n    )\n    (5): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act1): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act2): ReLU(inplace=True)\n      (aa): Identity()\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act3): ReLU(inplace=True)\n    )\n  )\n  (layer4): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act1): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act2): ReLU(inplace=True)\n      (aa): Identity()\n      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act3): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act1): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act2): ReLU(inplace=True)\n      (aa): Identity()\n      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act3): ReLU(inplace=True)\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act1): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act2): ReLU(inplace=True)\n      (aa): Identity()\n      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act3): ReLU(inplace=True)\n    )\n  )\n  (global_pool): SelectAdaptivePool2d(pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))\n  (fc): Linear(in_features=2048, out_features=10, bias=True)\n)\n\n\n\nsummary(model, input_size=random_input.shape, verbose = 0, depth  = 5, col_names = (\n                \"input_size\",\n                \"output_size\",\n                \"num_params\",\n                \"params_percent\",\n                \"kernel_size\",\n                \"mult_adds\",\n                \"trainable\",\n            ))\n\n=======================================================================================================================================================================================================================\nLayer (type:depth-idx)                   Input Shape               Output Shape              Param #                   Param %                   Kernel Shape              Mult-Adds                 Trainable\n=======================================================================================================================================================================================================================\nResNet                                   [1, 3, 100, 100]          [1, 10]                   --                             --                   --                        --                        True\n├─Conv2d: 1-1                            [1, 3, 100, 100]          [1, 64, 50, 50]           9,408                       0.04%                   [7, 7]                    23,520,000                True\n├─BatchNorm2d: 1-2                       [1, 64, 50, 50]           [1, 64, 50, 50]           128                         0.00%                   --                        128                       True\n├─ReLU: 1-3                              [1, 64, 50, 50]           [1, 64, 50, 50]           --                             --                   --                        --                        --\n├─MaxPool2d: 1-4                         [1, 64, 50, 50]           [1, 64, 25, 25]           --                             --                   3                         --                        --\n├─Sequential: 1-5                        [1, 64, 25, 25]           [1, 256, 25, 25]          --                             --                   --                        --                        True\n│    └─Bottleneck: 2-1                   [1, 64, 25, 25]           [1, 256, 25, 25]          --                             --                   --                        --                        True\n│    │    └─Conv2d: 3-1                  [1, 64, 25, 25]           [1, 64, 25, 25]           4,096                       0.02%                   [1, 1]                    2,560,000                 True\n│    │    └─BatchNorm2d: 3-2             [1, 64, 25, 25]           [1, 64, 25, 25]           128                         0.00%                   --                        128                       True\n│    │    └─ReLU: 3-3                    [1, 64, 25, 25]           [1, 64, 25, 25]           --                             --                   --                        --                        --\n│    │    └─Conv2d: 3-4                  [1, 64, 25, 25]           [1, 64, 25, 25]           36,864                      0.16%                   [3, 3]                    23,040,000                True\n│    │    └─BatchNorm2d: 3-5             [1, 64, 25, 25]           [1, 64, 25, 25]           128                         0.00%                   --                        128                       True\n│    │    └─Identity: 3-6                [1, 64, 25, 25]           [1, 64, 25, 25]           --                             --                   --                        --                        --\n│    │    └─ReLU: 3-7                    [1, 64, 25, 25]           [1, 64, 25, 25]           --                             --                   --                        --                        --\n│    │    └─Identity: 3-8                [1, 64, 25, 25]           [1, 64, 25, 25]           --                             --                   --                        --                        --\n│    │    └─Conv2d: 3-9                  [1, 64, 25, 25]           [1, 256, 25, 25]          16,384                      0.07%                   [1, 1]                    10,240,000                True\n│    │    └─BatchNorm2d: 3-10            [1, 256, 25, 25]          [1, 256, 25, 25]          512                         0.00%                   --                        512                       True\n│    │    └─Sequential: 3-11             [1, 64, 25, 25]           [1, 256, 25, 25]          --                             --                   --                        --                        True\n│    │    │    └─Conv2d: 4-1             [1, 64, 25, 25]           [1, 256, 25, 25]          16,384                      0.07%                   [1, 1]                    10,240,000                True\n│    │    │    └─BatchNorm2d: 4-2        [1, 256, 25, 25]          [1, 256, 25, 25]          512                         0.00%                   --                        512                       True\n│    │    └─ReLU: 3-12                   [1, 256, 25, 25]          [1, 256, 25, 25]          --                             --                   --                        --                        --\n│    └─Bottleneck: 2-2                   [1, 256, 25, 25]          [1, 256, 25, 25]          --                             --                   --                        --                        True\n│    │    └─Conv2d: 3-13                 [1, 256, 25, 25]          [1, 64, 25, 25]           16,384                      0.07%                   [1, 1]                    10,240,000                True\n│    │    └─BatchNorm2d: 3-14            [1, 64, 25, 25]           [1, 64, 25, 25]           128                         0.00%                   --                        128                       True\n│    │    └─ReLU: 3-15                   [1, 64, 25, 25]           [1, 64, 25, 25]           --                             --                   --                        --                        --\n│    │    └─Conv2d: 3-16                 [1, 64, 25, 25]           [1, 64, 25, 25]           36,864                      0.16%                   [3, 3]                    23,040,000                True\n│    │    └─BatchNorm2d: 3-17            [1, 64, 25, 25]           [1, 64, 25, 25]           128                         0.00%                   --                        128                       True\n│    │    └─Identity: 3-18               [1, 64, 25, 25]           [1, 64, 25, 25]           --                             --                   --                        --                        --\n│    │    └─ReLU: 3-19                   [1, 64, 25, 25]           [1, 64, 25, 25]           --                             --                   --                        --                        --\n│    │    └─Identity: 3-20               [1, 64, 25, 25]           [1, 64, 25, 25]           --                             --                   --                        --                        --\n│    │    └─Conv2d: 3-21                 [1, 64, 25, 25]           [1, 256, 25, 25]          16,384                      0.07%                   [1, 1]                    10,240,000                True\n│    │    └─BatchNorm2d: 3-22            [1, 256, 25, 25]          [1, 256, 25, 25]          512                         0.00%                   --                        512                       True\n│    │    └─ReLU: 3-23                   [1, 256, 25, 25]          [1, 256, 25, 25]          --                             --                   --                        --                        --\n│    └─Bottleneck: 2-3                   [1, 256, 25, 25]          [1, 256, 25, 25]          --                             --                   --                        --                        True\n│    │    └─Conv2d: 3-24                 [1, 256, 25, 25]          [1, 64, 25, 25]           16,384                      0.07%                   [1, 1]                    10,240,000                True\n│    │    └─BatchNorm2d: 3-25            [1, 64, 25, 25]           [1, 64, 25, 25]           128                         0.00%                   --                        128                       True\n│    │    └─ReLU: 3-26                   [1, 64, 25, 25]           [1, 64, 25, 25]           --                             --                   --                        --                        --\n│    │    └─Conv2d: 3-27                 [1, 64, 25, 25]           [1, 64, 25, 25]           36,864                      0.16%                   [3, 3]                    23,040,000                True\n│    │    └─BatchNorm2d: 3-28            [1, 64, 25, 25]           [1, 64, 25, 25]           128                         0.00%                   --                        128                       True\n│    │    └─Identity: 3-29               [1, 64, 25, 25]           [1, 64, 25, 25]           --                             --                   --                        --                        --\n│    │    └─ReLU: 3-30                   [1, 64, 25, 25]           [1, 64, 25, 25]           --                             --                   --                        --                        --\n│    │    └─Identity: 3-31               [1, 64, 25, 25]           [1, 64, 25, 25]           --                             --                   --                        --                        --\n│    │    └─Conv2d: 3-32                 [1, 64, 25, 25]           [1, 256, 25, 25]          16,384                      0.07%                   [1, 1]                    10,240,000                True\n│    │    └─BatchNorm2d: 3-33            [1, 256, 25, 25]          [1, 256, 25, 25]          512                         0.00%                   --                        512                       True\n│    │    └─ReLU: 3-34                   [1, 256, 25, 25]          [1, 256, 25, 25]          --                             --                   --                        --                        --\n├─Sequential: 1-6                        [1, 256, 25, 25]          [1, 512, 13, 13]          --                             --                   --                        --                        True\n│    └─Bottleneck: 2-4                   [1, 256, 25, 25]          [1, 512, 13, 13]          --                             --                   --                        --                        True\n│    │    └─Conv2d: 3-35                 [1, 256, 25, 25]          [1, 128, 25, 25]          32,768                      0.14%                   [1, 1]                    20,480,000                True\n│    │    └─BatchNorm2d: 3-36            [1, 128, 25, 25]          [1, 128, 25, 25]          256                         0.00%                   --                        256                       True\n│    │    └─ReLU: 3-37                   [1, 128, 25, 25]          [1, 128, 25, 25]          --                             --                   --                        --                        --\n│    │    └─Conv2d: 3-38                 [1, 128, 25, 25]          [1, 128, 13, 13]          147,456                     0.63%                   [3, 3]                    24,920,064                True\n│    │    └─BatchNorm2d: 3-39            [1, 128, 13, 13]          [1, 128, 13, 13]          256                         0.00%                   --                        256                       True\n│    │    └─Identity: 3-40               [1, 128, 13, 13]          [1, 128, 13, 13]          --                             --                   --                        --                        --\n│    │    └─ReLU: 3-41                   [1, 128, 13, 13]          [1, 128, 13, 13]          --                             --                   --                        --                        --\n│    │    └─Identity: 3-42               [1, 128, 13, 13]          [1, 128, 13, 13]          --                             --                   --                        --                        --\n│    │    └─Conv2d: 3-43                 [1, 128, 13, 13]          [1, 512, 13, 13]          65,536                      0.28%                   [1, 1]                    11,075,584                True\n│    │    └─BatchNorm2d: 3-44            [1, 512, 13, 13]          [1, 512, 13, 13]          1,024                       0.00%                   --                        1,024                     True\n│    │    └─Sequential: 3-45             [1, 256, 25, 25]          [1, 512, 13, 13]          --                             --                   --                        --                        True\n│    │    │    └─Conv2d: 4-3             [1, 256, 25, 25]          [1, 512, 13, 13]          131,072                     0.56%                   [1, 1]                    22,151,168                True\n│    │    │    └─BatchNorm2d: 4-4        [1, 512, 13, 13]          [1, 512, 13, 13]          1,024                       0.00%                   --                        1,024                     True\n│    │    └─ReLU: 3-46                   [1, 512, 13, 13]          [1, 512, 13, 13]          --                             --                   --                        --                        --\n│    └─Bottleneck: 2-5                   [1, 512, 13, 13]          [1, 512, 13, 13]          --                             --                   --                        --                        True\n│    │    └─Conv2d: 3-47                 [1, 512, 13, 13]          [1, 128, 13, 13]          65,536                      0.28%                   [1, 1]                    11,075,584                True\n│    │    └─BatchNorm2d: 3-48            [1, 128, 13, 13]          [1, 128, 13, 13]          256                         0.00%                   --                        256                       True\n│    │    └─ReLU: 3-49                   [1, 128, 13, 13]          [1, 128, 13, 13]          --                             --                   --                        --                        --\n│    │    └─Conv2d: 3-50                 [1, 128, 13, 13]          [1, 128, 13, 13]          147,456                     0.63%                   [3, 3]                    24,920,064                True\n│    │    └─BatchNorm2d: 3-51            [1, 128, 13, 13]          [1, 128, 13, 13]          256                         0.00%                   --                        256                       True\n│    │    └─Identity: 3-52               [1, 128, 13, 13]          [1, 128, 13, 13]          --                             --                   --                        --                        --\n│    │    └─ReLU: 3-53                   [1, 128, 13, 13]          [1, 128, 13, 13]          --                             --                   --                        --                        --\n│    │    └─Identity: 3-54               [1, 128, 13, 13]          [1, 128, 13, 13]          --                             --                   --                        --                        --\n│    │    └─Conv2d: 3-55                 [1, 128, 13, 13]          [1, 512, 13, 13]          65,536                      0.28%                   [1, 1]                    11,075,584                True\n│    │    └─BatchNorm2d: 3-56            [1, 512, 13, 13]          [1, 512, 13, 13]          1,024                       0.00%                   --                        1,024                     True\n│    │    └─ReLU: 3-57                   [1, 512, 13, 13]          [1, 512, 13, 13]          --                             --                   --                        --                        --\n│    └─Bottleneck: 2-6                   [1, 512, 13, 13]          [1, 512, 13, 13]          --                             --                   --                        --                        True\n│    │    └─Conv2d: 3-58                 [1, 512, 13, 13]          [1, 128, 13, 13]          65,536                      0.28%                   [1, 1]                    11,075,584                True\n│    │    └─BatchNorm2d: 3-59            [1, 128, 13, 13]          [1, 128, 13, 13]          256                         0.00%                   --                        256                       True\n│    │    └─ReLU: 3-60                   [1, 128, 13, 13]          [1, 128, 13, 13]          --                             --                   --                        --                        --\n│    │    └─Conv2d: 3-61                 [1, 128, 13, 13]          [1, 128, 13, 13]          147,456                     0.63%                   [3, 3]                    24,920,064                True\n│    │    └─BatchNorm2d: 3-62            [1, 128, 13, 13]          [1, 128, 13, 13]          256                         0.00%                   --                        256                       True\n│    │    └─Identity: 3-63               [1, 128, 13, 13]          [1, 128, 13, 13]          --                             --                   --                        --                        --\n│    │    └─ReLU: 3-64                   [1, 128, 13, 13]          [1, 128, 13, 13]          --                             --                   --                        --                        --\n│    │    └─Identity: 3-65               [1, 128, 13, 13]          [1, 128, 13, 13]          --                             --                   --                        --                        --\n│    │    └─Conv2d: 3-66                 [1, 128, 13, 13]          [1, 512, 13, 13]          65,536                      0.28%                   [1, 1]                    11,075,584                True\n│    │    └─BatchNorm2d: 3-67            [1, 512, 13, 13]          [1, 512, 13, 13]          1,024                       0.00%                   --                        1,024                     True\n│    │    └─ReLU: 3-68                   [1, 512, 13, 13]          [1, 512, 13, 13]          --                             --                   --                        --                        --\n│    └─Bottleneck: 2-7                   [1, 512, 13, 13]          [1, 512, 13, 13]          --                             --                   --                        --                        True\n│    │    └─Conv2d: 3-69                 [1, 512, 13, 13]          [1, 128, 13, 13]          65,536                      0.28%                   [1, 1]                    11,075,584                True\n│    │    └─BatchNorm2d: 3-70            [1, 128, 13, 13]          [1, 128, 13, 13]          256                         0.00%                   --                        256                       True\n│    │    └─ReLU: 3-71                   [1, 128, 13, 13]          [1, 128, 13, 13]          --                             --                   --                        --                        --\n│    │    └─Conv2d: 3-72                 [1, 128, 13, 13]          [1, 128, 13, 13]          147,456                     0.63%                   [3, 3]                    24,920,064                True\n│    │    └─BatchNorm2d: 3-73            [1, 128, 13, 13]          [1, 128, 13, 13]          256                         0.00%                   --                        256                       True\n│    │    └─Identity: 3-74               [1, 128, 13, 13]          [1, 128, 13, 13]          --                             --                   --                        --                        --\n│    │    └─ReLU: 3-75                   [1, 128, 13, 13]          [1, 128, 13, 13]          --                             --                   --                        --                        --\n│    │    └─Identity: 3-76               [1, 128, 13, 13]          [1, 128, 13, 13]          --                             --                   --                        --                        --\n│    │    └─Conv2d: 3-77                 [1, 128, 13, 13]          [1, 512, 13, 13]          65,536                      0.28%                   [1, 1]                    11,075,584                True\n│    │    └─BatchNorm2d: 3-78            [1, 512, 13, 13]          [1, 512, 13, 13]          1,024                       0.00%                   --                        1,024                     True\n│    │    └─ReLU: 3-79                   [1, 512, 13, 13]          [1, 512, 13, 13]          --                             --                   --                        --                        --\n├─Sequential: 1-7                        [1, 512, 13, 13]          [1, 1024, 7, 7]           --                             --                   --                        --                        True\n│    └─Bottleneck: 2-8                   [1, 512, 13, 13]          [1, 1024, 7, 7]           --                             --                   --                        --                        True\n│    │    └─Conv2d: 3-80                 [1, 512, 13, 13]          [1, 256, 13, 13]          131,072                     0.56%                   [1, 1]                    22,151,168                True\n│    │    └─BatchNorm2d: 3-81            [1, 256, 13, 13]          [1, 256, 13, 13]          512                         0.00%                   --                        512                       True\n│    │    └─ReLU: 3-82                   [1, 256, 13, 13]          [1, 256, 13, 13]          --                             --                   --                        --                        --\n│    │    └─Conv2d: 3-83                 [1, 256, 13, 13]          [1, 256, 7, 7]            589,824                     2.51%                   [3, 3]                    28,901,376                True\n│    │    └─BatchNorm2d: 3-84            [1, 256, 7, 7]            [1, 256, 7, 7]            512                         0.00%                   --                        512                       True\n│    │    └─Identity: 3-85               [1, 256, 7, 7]            [1, 256, 7, 7]            --                             --                   --                        --                        --\n│    │    └─ReLU: 3-86                   [1, 256, 7, 7]            [1, 256, 7, 7]            --                             --                   --                        --                        --\n│    │    └─Identity: 3-87               [1, 256, 7, 7]            [1, 256, 7, 7]            --                             --                   --                        --                        --\n│    │    └─Conv2d: 3-88                 [1, 256, 7, 7]            [1, 1024, 7, 7]           262,144                     1.11%                   [1, 1]                    12,845,056                True\n│    │    └─BatchNorm2d: 3-89            [1, 1024, 7, 7]           [1, 1024, 7, 7]           2,048                       0.01%                   --                        2,048                     True\n│    │    └─Sequential: 3-90             [1, 512, 13, 13]          [1, 1024, 7, 7]           --                             --                   --                        --                        True\n│    │    │    └─Conv2d: 4-5             [1, 512, 13, 13]          [1, 1024, 7, 7]           524,288                     2.23%                   [1, 1]                    25,690,112                True\n│    │    │    └─BatchNorm2d: 4-6        [1, 1024, 7, 7]           [1, 1024, 7, 7]           2,048                       0.01%                   --                        2,048                     True\n│    │    └─ReLU: 3-91                   [1, 1024, 7, 7]           [1, 1024, 7, 7]           --                             --                   --                        --                        --\n│    └─Bottleneck: 2-9                   [1, 1024, 7, 7]           [1, 1024, 7, 7]           --                             --                   --                        --                        True\n│    │    └─Conv2d: 3-92                 [1, 1024, 7, 7]           [1, 256, 7, 7]            262,144                     1.11%                   [1, 1]                    12,845,056                True\n│    │    └─BatchNorm2d: 3-93            [1, 256, 7, 7]            [1, 256, 7, 7]            512                         0.00%                   --                        512                       True\n│    │    └─ReLU: 3-94                   [1, 256, 7, 7]            [1, 256, 7, 7]            --                             --                   --                        --                        --\n│    │    └─Conv2d: 3-95                 [1, 256, 7, 7]            [1, 256, 7, 7]            589,824                     2.51%                   [3, 3]                    28,901,376                True\n│    │    └─BatchNorm2d: 3-96            [1, 256, 7, 7]            [1, 256, 7, 7]            512                         0.00%                   --                        512                       True\n│    │    └─Identity: 3-97               [1, 256, 7, 7]            [1, 256, 7, 7]            --                             --                   --                        --                        --\n│    │    └─ReLU: 3-98                   [1, 256, 7, 7]            [1, 256, 7, 7]            --                             --                   --                        --                        --\n│    │    └─Identity: 3-99               [1, 256, 7, 7]            [1, 256, 7, 7]            --                             --                   --                        --                        --\n│    │    └─Conv2d: 3-100                [1, 256, 7, 7]            [1, 1024, 7, 7]           262,144                     1.11%                   [1, 1]                    12,845,056                True\n│    │    └─BatchNorm2d: 3-101           [1, 1024, 7, 7]           [1, 1024, 7, 7]           2,048                       0.01%                   --                        2,048                     True\n│    │    └─ReLU: 3-102                  [1, 1024, 7, 7]           [1, 1024, 7, 7]           --                             --                   --                        --                        --\n│    └─Bottleneck: 2-10                  [1, 1024, 7, 7]           [1, 1024, 7, 7]           --                             --                   --                        --                        True\n│    │    └─Conv2d: 3-103                [1, 1024, 7, 7]           [1, 256, 7, 7]            262,144                     1.11%                   [1, 1]                    12,845,056                True\n│    │    └─BatchNorm2d: 3-104           [1, 256, 7, 7]            [1, 256, 7, 7]            512                         0.00%                   --                        512                       True\n│    │    └─ReLU: 3-105                  [1, 256, 7, 7]            [1, 256, 7, 7]            --                             --                   --                        --                        --\n│    │    └─Conv2d: 3-106                [1, 256, 7, 7]            [1, 256, 7, 7]            589,824                     2.51%                   [3, 3]                    28,901,376                True\n│    │    └─BatchNorm2d: 3-107           [1, 256, 7, 7]            [1, 256, 7, 7]            512                         0.00%                   --                        512                       True\n│    │    └─Identity: 3-108              [1, 256, 7, 7]            [1, 256, 7, 7]            --                             --                   --                        --                        --\n│    │    └─ReLU: 3-109                  [1, 256, 7, 7]            [1, 256, 7, 7]            --                             --                   --                        --                        --\n│    │    └─Identity: 3-110              [1, 256, 7, 7]            [1, 256, 7, 7]            --                             --                   --                        --                        --\n│    │    └─Conv2d: 3-111                [1, 256, 7, 7]            [1, 1024, 7, 7]           262,144                     1.11%                   [1, 1]                    12,845,056                True\n│    │    └─BatchNorm2d: 3-112           [1, 1024, 7, 7]           [1, 1024, 7, 7]           2,048                       0.01%                   --                        2,048                     True\n│    │    └─ReLU: 3-113                  [1, 1024, 7, 7]           [1, 1024, 7, 7]           --                             --                   --                        --                        --\n│    └─Bottleneck: 2-11                  [1, 1024, 7, 7]           [1, 1024, 7, 7]           --                             --                   --                        --                        True\n│    │    └─Conv2d: 3-114                [1, 1024, 7, 7]           [1, 256, 7, 7]            262,144                     1.11%                   [1, 1]                    12,845,056                True\n│    │    └─BatchNorm2d: 3-115           [1, 256, 7, 7]            [1, 256, 7, 7]            512                         0.00%                   --                        512                       True\n│    │    └─ReLU: 3-116                  [1, 256, 7, 7]            [1, 256, 7, 7]            --                             --                   --                        --                        --\n│    │    └─Conv2d: 3-117                [1, 256, 7, 7]            [1, 256, 7, 7]            589,824                     2.51%                   [3, 3]                    28,901,376                True\n│    │    └─BatchNorm2d: 3-118           [1, 256, 7, 7]            [1, 256, 7, 7]            512                         0.00%                   --                        512                       True\n│    │    └─Identity: 3-119              [1, 256, 7, 7]            [1, 256, 7, 7]            --                             --                   --                        --                        --\n│    │    └─ReLU: 3-120                  [1, 256, 7, 7]            [1, 256, 7, 7]            --                             --                   --                        --                        --\n│    │    └─Identity: 3-121              [1, 256, 7, 7]            [1, 256, 7, 7]            --                             --                   --                        --                        --\n│    │    └─Conv2d: 3-122                [1, 256, 7, 7]            [1, 1024, 7, 7]           262,144                     1.11%                   [1, 1]                    12,845,056                True\n│    │    └─BatchNorm2d: 3-123           [1, 1024, 7, 7]           [1, 1024, 7, 7]           2,048                       0.01%                   --                        2,048                     True\n│    │    └─ReLU: 3-124                  [1, 1024, 7, 7]           [1, 1024, 7, 7]           --                             --                   --                        --                        --\n│    └─Bottleneck: 2-12                  [1, 1024, 7, 7]           [1, 1024, 7, 7]           --                             --                   --                        --                        True\n│    │    └─Conv2d: 3-125                [1, 1024, 7, 7]           [1, 256, 7, 7]            262,144                     1.11%                   [1, 1]                    12,845,056                True\n│    │    └─BatchNorm2d: 3-126           [1, 256, 7, 7]            [1, 256, 7, 7]            512                         0.00%                   --                        512                       True\n│    │    └─ReLU: 3-127                  [1, 256, 7, 7]            [1, 256, 7, 7]            --                             --                   --                        --                        --\n│    │    └─Conv2d: 3-128                [1, 256, 7, 7]            [1, 256, 7, 7]            589,824                     2.51%                   [3, 3]                    28,901,376                True\n│    │    └─BatchNorm2d: 3-129           [1, 256, 7, 7]            [1, 256, 7, 7]            512                         0.00%                   --                        512                       True\n│    │    └─Identity: 3-130              [1, 256, 7, 7]            [1, 256, 7, 7]            --                             --                   --                        --                        --\n│    │    └─ReLU: 3-131                  [1, 256, 7, 7]            [1, 256, 7, 7]            --                             --                   --                        --                        --\n│    │    └─Identity: 3-132              [1, 256, 7, 7]            [1, 256, 7, 7]            --                             --                   --                        --                        --\n│    │    └─Conv2d: 3-133                [1, 256, 7, 7]            [1, 1024, 7, 7]           262,144                     1.11%                   [1, 1]                    12,845,056                True\n│    │    └─BatchNorm2d: 3-134           [1, 1024, 7, 7]           [1, 1024, 7, 7]           2,048                       0.01%                   --                        2,048                     True\n│    │    └─ReLU: 3-135                  [1, 1024, 7, 7]           [1, 1024, 7, 7]           --                             --                   --                        --                        --\n│    └─Bottleneck: 2-13                  [1, 1024, 7, 7]           [1, 1024, 7, 7]           --                             --                   --                        --                        True\n│    │    └─Conv2d: 3-136                [1, 1024, 7, 7]           [1, 256, 7, 7]            262,144                     1.11%                   [1, 1]                    12,845,056                True\n│    │    └─BatchNorm2d: 3-137           [1, 256, 7, 7]            [1, 256, 7, 7]            512                         0.00%                   --                        512                       True\n│    │    └─ReLU: 3-138                  [1, 256, 7, 7]            [1, 256, 7, 7]            --                             --                   --                        --                        --\n│    │    └─Conv2d: 3-139                [1, 256, 7, 7]            [1, 256, 7, 7]            589,824                     2.51%                   [3, 3]                    28,901,376                True\n│    │    └─BatchNorm2d: 3-140           [1, 256, 7, 7]            [1, 256, 7, 7]            512                         0.00%                   --                        512                       True\n│    │    └─Identity: 3-141              [1, 256, 7, 7]            [1, 256, 7, 7]            --                             --                   --                        --                        --\n│    │    └─ReLU: 3-142                  [1, 256, 7, 7]            [1, 256, 7, 7]            --                             --                   --                        --                        --\n│    │    └─Identity: 3-143              [1, 256, 7, 7]            [1, 256, 7, 7]            --                             --                   --                        --                        --\n│    │    └─Conv2d: 3-144                [1, 256, 7, 7]            [1, 1024, 7, 7]           262,144                     1.11%                   [1, 1]                    12,845,056                True\n│    │    └─BatchNorm2d: 3-145           [1, 1024, 7, 7]           [1, 1024, 7, 7]           2,048                       0.01%                   --                        2,048                     True\n│    │    └─ReLU: 3-146                  [1, 1024, 7, 7]           [1, 1024, 7, 7]           --                             --                   --                        --                        --\n├─Sequential: 1-8                        [1, 1024, 7, 7]           [1, 2048, 4, 4]           --                             --                   --                        --                        True\n│    └─Bottleneck: 2-14                  [1, 1024, 7, 7]           [1, 2048, 4, 4]           --                             --                   --                        --                        True\n│    │    └─Conv2d: 3-147                [1, 1024, 7, 7]           [1, 512, 7, 7]            524,288                     2.23%                   [1, 1]                    25,690,112                True\n│    │    └─BatchNorm2d: 3-148           [1, 512, 7, 7]            [1, 512, 7, 7]            1,024                       0.00%                   --                        1,024                     True\n│    │    └─ReLU: 3-149                  [1, 512, 7, 7]            [1, 512, 7, 7]            --                             --                   --                        --                        --\n│    │    └─Conv2d: 3-150                [1, 512, 7, 7]            [1, 512, 4, 4]            2,359,296                  10.03%                   [3, 3]                    37,748,736                True\n│    │    └─BatchNorm2d: 3-151           [1, 512, 4, 4]            [1, 512, 4, 4]            1,024                       0.00%                   --                        1,024                     True\n│    │    └─Identity: 3-152              [1, 512, 4, 4]            [1, 512, 4, 4]            --                             --                   --                        --                        --\n│    │    └─ReLU: 3-153                  [1, 512, 4, 4]            [1, 512, 4, 4]            --                             --                   --                        --                        --\n│    │    └─Identity: 3-154              [1, 512, 4, 4]            [1, 512, 4, 4]            --                             --                   --                        --                        --\n│    │    └─Conv2d: 3-155                [1, 512, 4, 4]            [1, 2048, 4, 4]           1,048,576                   4.46%                   [1, 1]                    16,777,216                True\n│    │    └─BatchNorm2d: 3-156           [1, 2048, 4, 4]           [1, 2048, 4, 4]           4,096                       0.02%                   --                        4,096                     True\n│    │    └─Sequential: 3-157            [1, 1024, 7, 7]           [1, 2048, 4, 4]           --                             --                   --                        --                        True\n│    │    │    └─Conv2d: 4-7             [1, 1024, 7, 7]           [1, 2048, 4, 4]           2,097,152                   8.91%                   [1, 1]                    33,554,432                True\n│    │    │    └─BatchNorm2d: 4-8        [1, 2048, 4, 4]           [1, 2048, 4, 4]           4,096                       0.02%                   --                        4,096                     True\n│    │    └─ReLU: 3-158                  [1, 2048, 4, 4]           [1, 2048, 4, 4]           --                             --                   --                        --                        --\n│    └─Bottleneck: 2-15                  [1, 2048, 4, 4]           [1, 2048, 4, 4]           --                             --                   --                        --                        True\n│    │    └─Conv2d: 3-159                [1, 2048, 4, 4]           [1, 512, 4, 4]            1,048,576                   4.46%                   [1, 1]                    16,777,216                True\n│    │    └─BatchNorm2d: 3-160           [1, 512, 4, 4]            [1, 512, 4, 4]            1,024                       0.00%                   --                        1,024                     True\n│    │    └─ReLU: 3-161                  [1, 512, 4, 4]            [1, 512, 4, 4]            --                             --                   --                        --                        --\n│    │    └─Conv2d: 3-162                [1, 512, 4, 4]            [1, 512, 4, 4]            2,359,296                  10.03%                   [3, 3]                    37,748,736                True\n│    │    └─BatchNorm2d: 3-163           [1, 512, 4, 4]            [1, 512, 4, 4]            1,024                       0.00%                   --                        1,024                     True\n│    │    └─Identity: 3-164              [1, 512, 4, 4]            [1, 512, 4, 4]            --                             --                   --                        --                        --\n│    │    └─ReLU: 3-165                  [1, 512, 4, 4]            [1, 512, 4, 4]            --                             --                   --                        --                        --\n│    │    └─Identity: 3-166              [1, 512, 4, 4]            [1, 512, 4, 4]            --                             --                   --                        --                        --\n│    │    └─Conv2d: 3-167                [1, 512, 4, 4]            [1, 2048, 4, 4]           1,048,576                   4.46%                   [1, 1]                    16,777,216                True\n│    │    └─BatchNorm2d: 3-168           [1, 2048, 4, 4]           [1, 2048, 4, 4]           4,096                       0.02%                   --                        4,096                     True\n│    │    └─ReLU: 3-169                  [1, 2048, 4, 4]           [1, 2048, 4, 4]           --                             --                   --                        --                        --\n│    └─Bottleneck: 2-16                  [1, 2048, 4, 4]           [1, 2048, 4, 4]           --                             --                   --                        --                        True\n│    │    └─Conv2d: 3-170                [1, 2048, 4, 4]           [1, 512, 4, 4]            1,048,576                   4.46%                   [1, 1]                    16,777,216                True\n│    │    └─BatchNorm2d: 3-171           [1, 512, 4, 4]            [1, 512, 4, 4]            1,024                       0.00%                   --                        1,024                     True\n│    │    └─ReLU: 3-172                  [1, 512, 4, 4]            [1, 512, 4, 4]            --                             --                   --                        --                        --\n│    │    └─Conv2d: 3-173                [1, 512, 4, 4]            [1, 512, 4, 4]            2,359,296                  10.03%                   [3, 3]                    37,748,736                True\n│    │    └─BatchNorm2d: 3-174           [1, 512, 4, 4]            [1, 512, 4, 4]            1,024                       0.00%                   --                        1,024                     True\n│    │    └─Identity: 3-175              [1, 512, 4, 4]            [1, 512, 4, 4]            --                             --                   --                        --                        --\n│    │    └─ReLU: 3-176                  [1, 512, 4, 4]            [1, 512, 4, 4]            --                             --                   --                        --                        --\n│    │    └─Identity: 3-177              [1, 512, 4, 4]            [1, 512, 4, 4]            --                             --                   --                        --                        --\n│    │    └─Conv2d: 3-178                [1, 512, 4, 4]            [1, 2048, 4, 4]           1,048,576                   4.46%                   [1, 1]                    16,777,216                True\n│    │    └─BatchNorm2d: 3-179           [1, 2048, 4, 4]           [1, 2048, 4, 4]           4,096                       0.02%                   --                        4,096                     True\n│    │    └─ReLU: 3-180                  [1, 2048, 4, 4]           [1, 2048, 4, 4]           --                             --                   --                        --                        --\n├─SelectAdaptivePool2d: 1-9              [1, 2048, 4, 4]           [1, 2048]                 --                             --                   --                        --                        --\n│    └─AdaptiveAvgPool2d: 2-17           [1, 2048, 4, 4]           [1, 2048, 1, 1]           --                             --                   --                        --                        --\n│    └─Flatten: 2-18                     [1, 2048, 1, 1]           [1, 2048]                 --                             --                   --                        --                        --\n├─Linear: 1-10                           [1, 2048]                 [1, 10]                   20,490                      0.09%                   --                        20,490                    True\n=======================================================================================================================================================================================================================\nTotal params: 23,528,522\nTrainable params: 23,528,522\nNon-trainable params: 0\nTotal mult-adds (Units.MEGABYTES): 995.48\n=======================================================================================================================================================================================================================\nInput size (MB): 0.12\nForward/backward pass size (MB): 38.94\nParams size (MB): 94.11\nEstimated Total Size (MB): 133.17\n=======================================================================================================================================================================================================================\n\n\n\nmodel = timm.create_model('resnetv2_50', pretrained=True, num_classes=10)\nrandom_input = np.random.rand(1, 3, 100, 100)\n\nsummary(model, input_size=random_input.shape, verbose = 0, depth  = 5, col_names = (\n                \"input_size\",\n                \"output_size\",\n                \"num_params\",\n                \"params_percent\",\n                \"kernel_size\",\n                \"mult_adds\",\n                \"trainable\",\n            ))\n\n\n\n\n=================================================================================================================================================================================================================================\nLayer (type:depth-idx)                             Input Shape               Output Shape              Param #                   Param %                   Kernel Shape              Mult-Adds                 Trainable\n=================================================================================================================================================================================================================================\nResNetV2                                           [1, 3, 100, 100]          [1, 10]                   --                             --                   --                        --                        True\n├─Sequential: 1-1                                  [1, 3, 100, 100]          [1, 64, 25, 25]           --                             --                   --                        --                        True\n│    └─Conv2d: 2-1                                 [1, 3, 100, 100]          [1, 64, 50, 50]           9,408                       0.04%                   [7, 7]                    23,520,000                True\n│    └─MaxPool2d: 2-2                              [1, 64, 50, 50]           [1, 64, 25, 25]           --                             --                   3                         --                        --\n├─Sequential: 1-2                                  [1, 64, 25, 25]           [1, 2048, 4, 4]           --                             --                   --                        --                        True\n│    └─ResNetStage: 2-3                            [1, 64, 25, 25]           [1, 256, 25, 25]          --                             --                   --                        --                        True\n│    │    └─Sequential: 3-1                        [1, 64, 25, 25]           [1, 256, 25, 25]          --                             --                   --                        --                        True\n│    │    │    └─PreActBottleneck: 4-1             [1, 64, 25, 25]           [1, 256, 25, 25]          --                             --                   --                        --                        True\n│    │    │    │    └─BatchNormAct2d: 5-1          [1, 64, 25, 25]           [1, 64, 25, 25]           128                         0.00%                   --                        0                         True\n│    │    │    │    └─DownsampleConv: 5-2          [1, 64, 25, 25]           [1, 256, 25, 25]          16,384                      0.07%                   --                        10,240,000                True\n│    │    │    │    └─Conv2d: 5-3                  [1, 64, 25, 25]           [1, 64, 25, 25]           4,096                       0.02%                   [1, 1]                    2,560,000                 True\n│    │    │    │    └─BatchNormAct2d: 5-4          [1, 64, 25, 25]           [1, 64, 25, 25]           128                         0.00%                   --                        0                         True\n│    │    │    │    └─Conv2d: 5-5                  [1, 64, 25, 25]           [1, 64, 25, 25]           36,864                      0.16%                   [3, 3]                    23,040,000                True\n│    │    │    │    └─BatchNormAct2d: 5-6          [1, 64, 25, 25]           [1, 64, 25, 25]           128                         0.00%                   --                        0                         True\n│    │    │    │    └─Conv2d: 5-7                  [1, 64, 25, 25]           [1, 256, 25, 25]          16,384                      0.07%                   [1, 1]                    10,240,000                True\n│    │    │    │    └─Identity: 5-8                [1, 256, 25, 25]          [1, 256, 25, 25]          --                             --                   --                        --                        --\n│    │    │    └─PreActBottleneck: 4-2             [1, 256, 25, 25]          [1, 256, 25, 25]          --                             --                   --                        --                        True\n│    │    │    │    └─BatchNormAct2d: 5-9          [1, 256, 25, 25]          [1, 256, 25, 25]          512                         0.00%                   --                        0                         True\n│    │    │    │    └─Conv2d: 5-10                 [1, 256, 25, 25]          [1, 64, 25, 25]           16,384                      0.07%                   [1, 1]                    10,240,000                True\n│    │    │    │    └─BatchNormAct2d: 5-11         [1, 64, 25, 25]           [1, 64, 25, 25]           128                         0.00%                   --                        0                         True\n│    │    │    │    └─Conv2d: 5-12                 [1, 64, 25, 25]           [1, 64, 25, 25]           36,864                      0.16%                   [3, 3]                    23,040,000                True\n│    │    │    │    └─BatchNormAct2d: 5-13         [1, 64, 25, 25]           [1, 64, 25, 25]           128                         0.00%                   --                        0                         True\n│    │    │    │    └─Conv2d: 5-14                 [1, 64, 25, 25]           [1, 256, 25, 25]          16,384                      0.07%                   [1, 1]                    10,240,000                True\n│    │    │    │    └─Identity: 5-15               [1, 256, 25, 25]          [1, 256, 25, 25]          --                             --                   --                        --                        --\n│    │    │    └─PreActBottleneck: 4-3             [1, 256, 25, 25]          [1, 256, 25, 25]          --                             --                   --                        --                        True\n│    │    │    │    └─BatchNormAct2d: 5-16         [1, 256, 25, 25]          [1, 256, 25, 25]          512                         0.00%                   --                        0                         True\n│    │    │    │    └─Conv2d: 5-17                 [1, 256, 25, 25]          [1, 64, 25, 25]           16,384                      0.07%                   [1, 1]                    10,240,000                True\n│    │    │    │    └─BatchNormAct2d: 5-18         [1, 64, 25, 25]           [1, 64, 25, 25]           128                         0.00%                   --                        0                         True\n│    │    │    │    └─Conv2d: 5-19                 [1, 64, 25, 25]           [1, 64, 25, 25]           36,864                      0.16%                   [3, 3]                    23,040,000                True\n│    │    │    │    └─BatchNormAct2d: 5-20         [1, 64, 25, 25]           [1, 64, 25, 25]           128                         0.00%                   --                        0                         True\n│    │    │    │    └─Conv2d: 5-21                 [1, 64, 25, 25]           [1, 256, 25, 25]          16,384                      0.07%                   [1, 1]                    10,240,000                True\n│    │    │    │    └─Identity: 5-22               [1, 256, 25, 25]          [1, 256, 25, 25]          --                             --                   --                        --                        --\n│    └─ResNetStage: 2-4                            [1, 256, 25, 25]          [1, 512, 13, 13]          --                             --                   --                        --                        True\n│    │    └─Sequential: 3-2                        [1, 256, 25, 25]          [1, 512, 13, 13]          --                             --                   --                        --                        True\n│    │    │    └─PreActBottleneck: 4-4             [1, 256, 25, 25]          [1, 512, 13, 13]          --                             --                   --                        --                        True\n│    │    │    │    └─BatchNormAct2d: 5-23         [1, 256, 25, 25]          [1, 256, 25, 25]          512                         0.00%                   --                        0                         True\n│    │    │    │    └─DownsampleConv: 5-24         [1, 256, 25, 25]          [1, 512, 13, 13]          131,072                     0.56%                   --                        22,151,168                True\n│    │    │    │    └─Conv2d: 5-25                 [1, 256, 25, 25]          [1, 128, 25, 25]          32,768                      0.14%                   [1, 1]                    20,480,000                True\n│    │    │    │    └─BatchNormAct2d: 5-26         [1, 128, 25, 25]          [1, 128, 25, 25]          256                         0.00%                   --                        0                         True\n│    │    │    │    └─Conv2d: 5-27                 [1, 128, 25, 25]          [1, 128, 13, 13]          147,456                     0.63%                   [3, 3]                    24,920,064                True\n│    │    │    │    └─BatchNormAct2d: 5-28         [1, 128, 13, 13]          [1, 128, 13, 13]          256                         0.00%                   --                        0                         True\n│    │    │    │    └─Conv2d: 5-29                 [1, 128, 13, 13]          [1, 512, 13, 13]          65,536                      0.28%                   [1, 1]                    11,075,584                True\n│    │    │    │    └─Identity: 5-30               [1, 512, 13, 13]          [1, 512, 13, 13]          --                             --                   --                        --                        --\n│    │    │    └─PreActBottleneck: 4-5             [1, 512, 13, 13]          [1, 512, 13, 13]          --                             --                   --                        --                        True\n│    │    │    │    └─BatchNormAct2d: 5-31         [1, 512, 13, 13]          [1, 512, 13, 13]          1,024                       0.00%                   --                        0                         True\n│    │    │    │    └─Conv2d: 5-32                 [1, 512, 13, 13]          [1, 128, 13, 13]          65,536                      0.28%                   [1, 1]                    11,075,584                True\n│    │    │    │    └─BatchNormAct2d: 5-33         [1, 128, 13, 13]          [1, 128, 13, 13]          256                         0.00%                   --                        0                         True\n│    │    │    │    └─Conv2d: 5-34                 [1, 128, 13, 13]          [1, 128, 13, 13]          147,456                     0.63%                   [3, 3]                    24,920,064                True\n│    │    │    │    └─BatchNormAct2d: 5-35         [1, 128, 13, 13]          [1, 128, 13, 13]          256                         0.00%                   --                        0                         True\n│    │    │    │    └─Conv2d: 5-36                 [1, 128, 13, 13]          [1, 512, 13, 13]          65,536                      0.28%                   [1, 1]                    11,075,584                True\n│    │    │    │    └─Identity: 5-37               [1, 512, 13, 13]          [1, 512, 13, 13]          --                             --                   --                        --                        --\n│    │    │    └─PreActBottleneck: 4-6             [1, 512, 13, 13]          [1, 512, 13, 13]          --                             --                   --                        --                        True\n│    │    │    │    └─BatchNormAct2d: 5-38         [1, 512, 13, 13]          [1, 512, 13, 13]          1,024                       0.00%                   --                        0                         True\n│    │    │    │    └─Conv2d: 5-39                 [1, 512, 13, 13]          [1, 128, 13, 13]          65,536                      0.28%                   [1, 1]                    11,075,584                True\n│    │    │    │    └─BatchNormAct2d: 5-40         [1, 128, 13, 13]          [1, 128, 13, 13]          256                         0.00%                   --                        0                         True\n│    │    │    │    └─Conv2d: 5-41                 [1, 128, 13, 13]          [1, 128, 13, 13]          147,456                     0.63%                   [3, 3]                    24,920,064                True\n│    │    │    │    └─BatchNormAct2d: 5-42         [1, 128, 13, 13]          [1, 128, 13, 13]          256                         0.00%                   --                        0                         True\n│    │    │    │    └─Conv2d: 5-43                 [1, 128, 13, 13]          [1, 512, 13, 13]          65,536                      0.28%                   [1, 1]                    11,075,584                True\n│    │    │    │    └─Identity: 5-44               [1, 512, 13, 13]          [1, 512, 13, 13]          --                             --                   --                        --                        --\n│    │    │    └─PreActBottleneck: 4-7             [1, 512, 13, 13]          [1, 512, 13, 13]          --                             --                   --                        --                        True\n│    │    │    │    └─BatchNormAct2d: 5-45         [1, 512, 13, 13]          [1, 512, 13, 13]          1,024                       0.00%                   --                        0                         True\n│    │    │    │    └─Conv2d: 5-46                 [1, 512, 13, 13]          [1, 128, 13, 13]          65,536                      0.28%                   [1, 1]                    11,075,584                True\n│    │    │    │    └─BatchNormAct2d: 5-47         [1, 128, 13, 13]          [1, 128, 13, 13]          256                         0.00%                   --                        0                         True\n│    │    │    │    └─Conv2d: 5-48                 [1, 128, 13, 13]          [1, 128, 13, 13]          147,456                     0.63%                   [3, 3]                    24,920,064                True\n│    │    │    │    └─BatchNormAct2d: 5-49         [1, 128, 13, 13]          [1, 128, 13, 13]          256                         0.00%                   --                        0                         True\n│    │    │    │    └─Conv2d: 5-50                 [1, 128, 13, 13]          [1, 512, 13, 13]          65,536                      0.28%                   [1, 1]                    11,075,584                True\n│    │    │    │    └─Identity: 5-51               [1, 512, 13, 13]          [1, 512, 13, 13]          --                             --                   --                        --                        --\n│    └─ResNetStage: 2-5                            [1, 512, 13, 13]          [1, 1024, 7, 7]           --                             --                   --                        --                        True\n│    │    └─Sequential: 3-3                        [1, 512, 13, 13]          [1, 1024, 7, 7]           --                             --                   --                        --                        True\n│    │    │    └─PreActBottleneck: 4-8             [1, 512, 13, 13]          [1, 1024, 7, 7]           --                             --                   --                        --                        True\n│    │    │    │    └─BatchNormAct2d: 5-52         [1, 512, 13, 13]          [1, 512, 13, 13]          1,024                       0.00%                   --                        0                         True\n│    │    │    │    └─DownsampleConv: 5-53         [1, 512, 13, 13]          [1, 1024, 7, 7]           524,288                     2.23%                   --                        25,690,112                True\n│    │    │    │    └─Conv2d: 5-54                 [1, 512, 13, 13]          [1, 256, 13, 13]          131,072                     0.56%                   [1, 1]                    22,151,168                True\n│    │    │    │    └─BatchNormAct2d: 5-55         [1, 256, 13, 13]          [1, 256, 13, 13]          512                         0.00%                   --                        0                         True\n│    │    │    │    └─Conv2d: 5-56                 [1, 256, 13, 13]          [1, 256, 7, 7]            589,824                     2.51%                   [3, 3]                    28,901,376                True\n│    │    │    │    └─BatchNormAct2d: 5-57         [1, 256, 7, 7]            [1, 256, 7, 7]            512                         0.00%                   --                        0                         True\n│    │    │    │    └─Conv2d: 5-58                 [1, 256, 7, 7]            [1, 1024, 7, 7]           262,144                     1.11%                   [1, 1]                    12,845,056                True\n│    │    │    │    └─Identity: 5-59               [1, 1024, 7, 7]           [1, 1024, 7, 7]           --                             --                   --                        --                        --\n│    │    │    └─PreActBottleneck: 4-9             [1, 1024, 7, 7]           [1, 1024, 7, 7]           --                             --                   --                        --                        True\n│    │    │    │    └─BatchNormAct2d: 5-60         [1, 1024, 7, 7]           [1, 1024, 7, 7]           2,048                       0.01%                   --                        0                         True\n│    │    │    │    └─Conv2d: 5-61                 [1, 1024, 7, 7]           [1, 256, 7, 7]            262,144                     1.11%                   [1, 1]                    12,845,056                True\n│    │    │    │    └─BatchNormAct2d: 5-62         [1, 256, 7, 7]            [1, 256, 7, 7]            512                         0.00%                   --                        0                         True\n│    │    │    │    └─Conv2d: 5-63                 [1, 256, 7, 7]            [1, 256, 7, 7]            589,824                     2.51%                   [3, 3]                    28,901,376                True\n│    │    │    │    └─BatchNormAct2d: 5-64         [1, 256, 7, 7]            [1, 256, 7, 7]            512                         0.00%                   --                        0                         True\n│    │    │    │    └─Conv2d: 5-65                 [1, 256, 7, 7]            [1, 1024, 7, 7]           262,144                     1.11%                   [1, 1]                    12,845,056                True\n│    │    │    │    └─Identity: 5-66               [1, 1024, 7, 7]           [1, 1024, 7, 7]           --                             --                   --                        --                        --\n│    │    │    └─PreActBottleneck: 4-10            [1, 1024, 7, 7]           [1, 1024, 7, 7]           --                             --                   --                        --                        True\n│    │    │    │    └─BatchNormAct2d: 5-67         [1, 1024, 7, 7]           [1, 1024, 7, 7]           2,048                       0.01%                   --                        0                         True\n│    │    │    │    └─Conv2d: 5-68                 [1, 1024, 7, 7]           [1, 256, 7, 7]            262,144                     1.11%                   [1, 1]                    12,845,056                True\n│    │    │    │    └─BatchNormAct2d: 5-69         [1, 256, 7, 7]            [1, 256, 7, 7]            512                         0.00%                   --                        0                         True\n│    │    │    │    └─Conv2d: 5-70                 [1, 256, 7, 7]            [1, 256, 7, 7]            589,824                     2.51%                   [3, 3]                    28,901,376                True\n│    │    │    │    └─BatchNormAct2d: 5-71         [1, 256, 7, 7]            [1, 256, 7, 7]            512                         0.00%                   --                        0                         True\n│    │    │    │    └─Conv2d: 5-72                 [1, 256, 7, 7]            [1, 1024, 7, 7]           262,144                     1.11%                   [1, 1]                    12,845,056                True\n│    │    │    │    └─Identity: 5-73               [1, 1024, 7, 7]           [1, 1024, 7, 7]           --                             --                   --                        --                        --\n│    │    │    └─PreActBottleneck: 4-11            [1, 1024, 7, 7]           [1, 1024, 7, 7]           --                             --                   --                        --                        True\n│    │    │    │    └─BatchNormAct2d: 5-74         [1, 1024, 7, 7]           [1, 1024, 7, 7]           2,048                       0.01%                   --                        0                         True\n│    │    │    │    └─Conv2d: 5-75                 [1, 1024, 7, 7]           [1, 256, 7, 7]            262,144                     1.11%                   [1, 1]                    12,845,056                True\n│    │    │    │    └─BatchNormAct2d: 5-76         [1, 256, 7, 7]            [1, 256, 7, 7]            512                         0.00%                   --                        0                         True\n│    │    │    │    └─Conv2d: 5-77                 [1, 256, 7, 7]            [1, 256, 7, 7]            589,824                     2.51%                   [3, 3]                    28,901,376                True\n│    │    │    │    └─BatchNormAct2d: 5-78         [1, 256, 7, 7]            [1, 256, 7, 7]            512                         0.00%                   --                        0                         True\n│    │    │    │    └─Conv2d: 5-79                 [1, 256, 7, 7]            [1, 1024, 7, 7]           262,144                     1.11%                   [1, 1]                    12,845,056                True\n│    │    │    │    └─Identity: 5-80               [1, 1024, 7, 7]           [1, 1024, 7, 7]           --                             --                   --                        --                        --\n│    │    │    └─PreActBottleneck: 4-12            [1, 1024, 7, 7]           [1, 1024, 7, 7]           --                             --                   --                        --                        True\n│    │    │    │    └─BatchNormAct2d: 5-81         [1, 1024, 7, 7]           [1, 1024, 7, 7]           2,048                       0.01%                   --                        0                         True\n│    │    │    │    └─Conv2d: 5-82                 [1, 1024, 7, 7]           [1, 256, 7, 7]            262,144                     1.11%                   [1, 1]                    12,845,056                True\n│    │    │    │    └─BatchNormAct2d: 5-83         [1, 256, 7, 7]            [1, 256, 7, 7]            512                         0.00%                   --                        0                         True\n│    │    │    │    └─Conv2d: 5-84                 [1, 256, 7, 7]            [1, 256, 7, 7]            589,824                     2.51%                   [3, 3]                    28,901,376                True\n│    │    │    │    └─BatchNormAct2d: 5-85         [1, 256, 7, 7]            [1, 256, 7, 7]            512                         0.00%                   --                        0                         True\n│    │    │    │    └─Conv2d: 5-86                 [1, 256, 7, 7]            [1, 1024, 7, 7]           262,144                     1.11%                   [1, 1]                    12,845,056                True\n│    │    │    │    └─Identity: 5-87               [1, 1024, 7, 7]           [1, 1024, 7, 7]           --                             --                   --                        --                        --\n│    │    │    └─PreActBottleneck: 4-13            [1, 1024, 7, 7]           [1, 1024, 7, 7]           --                             --                   --                        --                        True\n│    │    │    │    └─BatchNormAct2d: 5-88         [1, 1024, 7, 7]           [1, 1024, 7, 7]           2,048                       0.01%                   --                        0                         True\n│    │    │    │    └─Conv2d: 5-89                 [1, 1024, 7, 7]           [1, 256, 7, 7]            262,144                     1.11%                   [1, 1]                    12,845,056                True\n│    │    │    │    └─BatchNormAct2d: 5-90         [1, 256, 7, 7]            [1, 256, 7, 7]            512                         0.00%                   --                        0                         True\n│    │    │    │    └─Conv2d: 5-91                 [1, 256, 7, 7]            [1, 256, 7, 7]            589,824                     2.51%                   [3, 3]                    28,901,376                True\n│    │    │    │    └─BatchNormAct2d: 5-92         [1, 256, 7, 7]            [1, 256, 7, 7]            512                         0.00%                   --                        0                         True\n│    │    │    │    └─Conv2d: 5-93                 [1, 256, 7, 7]            [1, 1024, 7, 7]           262,144                     1.11%                   [1, 1]                    12,845,056                True\n│    │    │    │    └─Identity: 5-94               [1, 1024, 7, 7]           [1, 1024, 7, 7]           --                             --                   --                        --                        --\n│    └─ResNetStage: 2-6                            [1, 1024, 7, 7]           [1, 2048, 4, 4]           --                             --                   --                        --                        True\n│    │    └─Sequential: 3-4                        [1, 1024, 7, 7]           [1, 2048, 4, 4]           --                             --                   --                        --                        True\n│    │    │    └─PreActBottleneck: 4-14            [1, 1024, 7, 7]           [1, 2048, 4, 4]           --                             --                   --                        --                        True\n│    │    │    │    └─BatchNormAct2d: 5-95         [1, 1024, 7, 7]           [1, 1024, 7, 7]           2,048                       0.01%                   --                        0                         True\n│    │    │    │    └─DownsampleConv: 5-96         [1, 1024, 7, 7]           [1, 2048, 4, 4]           2,097,152                   8.92%                   --                        33,554,432                True\n│    │    │    │    └─Conv2d: 5-97                 [1, 1024, 7, 7]           [1, 512, 7, 7]            524,288                     2.23%                   [1, 1]                    25,690,112                True\n│    │    │    │    └─BatchNormAct2d: 5-98         [1, 512, 7, 7]            [1, 512, 7, 7]            1,024                       0.00%                   --                        0                         True\n│    │    │    │    └─Conv2d: 5-99                 [1, 512, 7, 7]            [1, 512, 4, 4]            2,359,296                  10.03%                   [3, 3]                    37,748,736                True\n│    │    │    │    └─BatchNormAct2d: 5-100        [1, 512, 4, 4]            [1, 512, 4, 4]            1,024                       0.00%                   --                        0                         True\n│    │    │    │    └─Conv2d: 5-101                [1, 512, 4, 4]            [1, 2048, 4, 4]           1,048,576                   4.46%                   [1, 1]                    16,777,216                True\n│    │    │    │    └─Identity: 5-102              [1, 2048, 4, 4]           [1, 2048, 4, 4]           --                             --                   --                        --                        --\n│    │    │    └─PreActBottleneck: 4-15            [1, 2048, 4, 4]           [1, 2048, 4, 4]           --                             --                   --                        --                        True\n│    │    │    │    └─BatchNormAct2d: 5-103        [1, 2048, 4, 4]           [1, 2048, 4, 4]           4,096                       0.02%                   --                        0                         True\n│    │    │    │    └─Conv2d: 5-104                [1, 2048, 4, 4]           [1, 512, 4, 4]            1,048,576                   4.46%                   [1, 1]                    16,777,216                True\n│    │    │    │    └─BatchNormAct2d: 5-105        [1, 512, 4, 4]            [1, 512, 4, 4]            1,024                       0.00%                   --                        0                         True\n│    │    │    │    └─Conv2d: 5-106                [1, 512, 4, 4]            [1, 512, 4, 4]            2,359,296                  10.03%                   [3, 3]                    37,748,736                True\n│    │    │    │    └─BatchNormAct2d: 5-107        [1, 512, 4, 4]            [1, 512, 4, 4]            1,024                       0.00%                   --                        0                         True\n│    │    │    │    └─Conv2d: 5-108                [1, 512, 4, 4]            [1, 2048, 4, 4]           1,048,576                   4.46%                   [1, 1]                    16,777,216                True\n│    │    │    │    └─Identity: 5-109              [1, 2048, 4, 4]           [1, 2048, 4, 4]           --                             --                   --                        --                        --\n│    │    │    └─PreActBottleneck: 4-16            [1, 2048, 4, 4]           [1, 2048, 4, 4]           --                             --                   --                        --                        True\n│    │    │    │    └─BatchNormAct2d: 5-110        [1, 2048, 4, 4]           [1, 2048, 4, 4]           4,096                       0.02%                   --                        0                         True\n│    │    │    │    └─Conv2d: 5-111                [1, 2048, 4, 4]           [1, 512, 4, 4]            1,048,576                   4.46%                   [1, 1]                    16,777,216                True\n│    │    │    │    └─BatchNormAct2d: 5-112        [1, 512, 4, 4]            [1, 512, 4, 4]            1,024                       0.00%                   --                        0                         True\n│    │    │    │    └─Conv2d: 5-113                [1, 512, 4, 4]            [1, 512, 4, 4]            2,359,296                  10.03%                   [3, 3]                    37,748,736                True\n│    │    │    │    └─BatchNormAct2d: 5-114        [1, 512, 4, 4]            [1, 512, 4, 4]            1,024                       0.00%                   --                        0                         True\n│    │    │    │    └─Conv2d: 5-115                [1, 512, 4, 4]            [1, 2048, 4, 4]           1,048,576                   4.46%                   [1, 1]                    16,777,216                True\n│    │    │    │    └─Identity: 5-116              [1, 2048, 4, 4]           [1, 2048, 4, 4]           --                             --                   --                        --                        --\n├─BatchNormAct2d: 1-3                              [1, 2048, 4, 4]           [1, 2048, 4, 4]           4,096                       0.02%                   --                        --                        True\n│    └─Identity: 2-7                               [1, 2048, 4, 4]           [1, 2048, 4, 4]           --                             --                   --                        --                        --\n│    └─ReLU: 2-8                                   [1, 2048, 4, 4]           [1, 2048, 4, 4]           --                             --                   --                        --                        --\n├─ClassifierHead: 1-4                              [1, 2048, 4, 4]           [1, 10]                   --                             --                   --                        --                        True\n│    └─SelectAdaptivePool2d: 2-9                   [1, 2048, 4, 4]           [1, 2048, 1, 1]           --                             --                   --                        --                        --\n│    │    └─AdaptiveAvgPool2d: 3-5                 [1, 2048, 4, 4]           [1, 2048, 1, 1]           --                             --                   --                        --                        --\n│    │    └─Identity: 3-6                          [1, 2048, 1, 1]           [1, 2048, 1, 1]           --                             --                   --                        --                        --\n│    └─Dropout: 2-10                               [1, 2048, 1, 1]           [1, 2048, 1, 1]           --                             --                   --                        --                        --\n│    └─Conv2d: 2-11                                [1, 2048, 1, 1]           [1, 10, 1, 1]             20,490                      0.09%                   [1, 1]                    20,490                    True\n│    └─Flatten: 2-12                               [1, 10, 1, 1]             [1, 10]                   --                             --                   --                        --                        --\n=================================================================================================================================================================================================================================\nTotal params: 23,520,842\nTrainable params: 23,520,842\nNon-trainable params: 0\nTotal mult-adds (Units.MEGABYTES): 995.42\n=================================================================================================================================================================================================================================\nInput size (MB): 0.12\nForward/backward pass size (MB): 19.47\nParams size (MB): 93.90\nEstimated Total Size (MB): 113.49\n=================================================================================================================================================================================================================================",
    "crumbs": [
      "Blog",
      "Timm - Loading Models"
    ]
  },
  {
    "objectID": "timm_models_torch.html#detection-models",
    "href": "timm_models_torch.html#detection-models",
    "title": "Timm - Loading Models",
    "section": "Detection models",
    "text": "Detection models\n\nfrom torchvision.models import detection \nimport timm\n\n\n############## TENSORBOARD ########################\nimport sys\nimport torch.nn.functional as F\nfrom torch.utils.tensorboard import SummaryWriter\n# default `log_dir` is \"runs\" - we'll be more specific here\nwriter = SummaryWriter()\n###################################################\n\n\n# Load ResNet model without the final classification layer\nmodel = timm.create_model('resnetv2_50', pretrained=True, num_classes=10)\n\n# weights = detection.FasterRCNN_ResNet50_FPN_V2_Weights.DEFAULT\n# model = detection.fasterrcnn_resnet50_fpn_v2(weights=weights,\n#                                     box_score_thresh=0.9).train()\n\n\nimport torch\n\n# Generate random input tensor\ninput_tensor = torch.randn(1, 3, 224, 224)",
    "crumbs": [
      "Blog",
      "Timm - Loading Models"
    ]
  },
  {
    "objectID": "timm_models_torch.html#onnx",
    "href": "timm_models_torch.html#onnx",
    "title": "Timm - Loading Models",
    "section": "ONNX",
    "text": "ONNX\n\ntorch.onnx.export(model, input_tensor, 'Data/resnet50_v2.onnx', input_names=[\"features\"], output_names=[\"logits\"])\n\n\n############## TENSORBOARD ########################\n# writer.add_graph(model, example_data.reshape(-1, 28*28).to(device))\n\nwriter.add_graph(model, input_tensor)\nwriter.flush()\nwriter.close()\n###################################################\n\n\nlist(model.children())[:]\n\n[Sequential(\n   (conv): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n   (pool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n ),\n Sequential(\n   (0): ResNetStage(\n     (blocks): Sequential(\n       (0): PreActBottleneck(\n         (downsample): DownsampleConv(\n           (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n           (norm): Identity()\n         )\n         (norm1): BatchNormAct2d(\n           64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n           (drop): Identity()\n           (act): ReLU(inplace=True)\n         )\n         (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n         (norm2): BatchNormAct2d(\n           64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n           (drop): Identity()\n           (act): ReLU(inplace=True)\n         )\n         (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n         (norm3): BatchNormAct2d(\n           64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n           (drop): Identity()\n           (act): ReLU(inplace=True)\n         )\n         (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n         (drop_path): Identity()\n       )\n       (1): PreActBottleneck(\n         (norm1): BatchNormAct2d(\n           256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n           (drop): Identity()\n           (act): ReLU(inplace=True)\n         )\n         (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n         (norm2): BatchNormAct2d(\n           64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n           (drop): Identity()\n           (act): ReLU(inplace=True)\n         )\n         (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n         (norm3): BatchNormAct2d(\n           64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n           (drop): Identity()\n           (act): ReLU(inplace=True)\n         )\n         (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n         (drop_path): Identity()\n       )\n       (2): PreActBottleneck(\n         (norm1): BatchNormAct2d(\n           256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n           (drop): Identity()\n           (act): ReLU(inplace=True)\n         )\n         (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n         (norm2): BatchNormAct2d(\n           64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n           (drop): Identity()\n           (act): ReLU(inplace=True)\n         )\n         (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n         (norm3): BatchNormAct2d(\n           64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n           (drop): Identity()\n           (act): ReLU(inplace=True)\n         )\n         (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n         (drop_path): Identity()\n       )\n     )\n   )\n   (1): ResNetStage(\n     (blocks): Sequential(\n       (0): PreActBottleneck(\n         (downsample): DownsampleConv(\n           (conv): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n           (norm): Identity()\n         )\n         (norm1): BatchNormAct2d(\n           256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n           (drop): Identity()\n           (act): ReLU(inplace=True)\n         )\n         (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n         (norm2): BatchNormAct2d(\n           128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n           (drop): Identity()\n           (act): ReLU(inplace=True)\n         )\n         (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n         (norm3): BatchNormAct2d(\n           128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n           (drop): Identity()\n           (act): ReLU(inplace=True)\n         )\n         (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n         (drop_path): Identity()\n       )\n       (1): PreActBottleneck(\n         (norm1): BatchNormAct2d(\n           512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n           (drop): Identity()\n           (act): ReLU(inplace=True)\n         )\n         (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n         (norm2): BatchNormAct2d(\n           128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n           (drop): Identity()\n           (act): ReLU(inplace=True)\n         )\n         (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n         (norm3): BatchNormAct2d(\n           128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n           (drop): Identity()\n           (act): ReLU(inplace=True)\n         )\n         (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n         (drop_path): Identity()\n       )\n       (2): PreActBottleneck(\n         (norm1): BatchNormAct2d(\n           512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n           (drop): Identity()\n           (act): ReLU(inplace=True)\n         )\n         (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n         (norm2): BatchNormAct2d(\n           128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n           (drop): Identity()\n           (act): ReLU(inplace=True)\n         )\n         (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n         (norm3): BatchNormAct2d(\n           128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n           (drop): Identity()\n           (act): ReLU(inplace=True)\n         )\n         (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n         (drop_path): Identity()\n       )\n       (3): PreActBottleneck(\n         (norm1): BatchNormAct2d(\n           512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n           (drop): Identity()\n           (act): ReLU(inplace=True)\n         )\n         (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n         (norm2): BatchNormAct2d(\n           128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n           (drop): Identity()\n           (act): ReLU(inplace=True)\n         )\n         (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n         (norm3): BatchNormAct2d(\n           128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n           (drop): Identity()\n           (act): ReLU(inplace=True)\n         )\n         (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n         (drop_path): Identity()\n       )\n     )\n   )\n   (2): ResNetStage(\n     (blocks): Sequential(\n       (0): PreActBottleneck(\n         (downsample): DownsampleConv(\n           (conv): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n           (norm): Identity()\n         )\n         (norm1): BatchNormAct2d(\n           512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n           (drop): Identity()\n           (act): ReLU(inplace=True)\n         )\n         (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n         (norm2): BatchNormAct2d(\n           256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n           (drop): Identity()\n           (act): ReLU(inplace=True)\n         )\n         (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n         (norm3): BatchNormAct2d(\n           256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n           (drop): Identity()\n           (act): ReLU(inplace=True)\n         )\n         (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n         (drop_path): Identity()\n       )\n       (1): PreActBottleneck(\n         (norm1): BatchNormAct2d(\n           1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n           (drop): Identity()\n           (act): ReLU(inplace=True)\n         )\n         (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n         (norm2): BatchNormAct2d(\n           256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n           (drop): Identity()\n           (act): ReLU(inplace=True)\n         )\n         (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n         (norm3): BatchNormAct2d(\n           256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n           (drop): Identity()\n           (act): ReLU(inplace=True)\n         )\n         (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n         (drop_path): Identity()\n       )\n       (2): PreActBottleneck(\n         (norm1): BatchNormAct2d(\n           1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n           (drop): Identity()\n           (act): ReLU(inplace=True)\n         )\n         (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n         (norm2): BatchNormAct2d(\n           256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n           (drop): Identity()\n           (act): ReLU(inplace=True)\n         )\n         (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n         (norm3): BatchNormAct2d(\n           256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n           (drop): Identity()\n           (act): ReLU(inplace=True)\n         )\n         (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n         (drop_path): Identity()\n       )\n       (3): PreActBottleneck(\n         (norm1): BatchNormAct2d(\n           1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n           (drop): Identity()\n           (act): ReLU(inplace=True)\n         )\n         (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n         (norm2): BatchNormAct2d(\n           256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n           (drop): Identity()\n           (act): ReLU(inplace=True)\n         )\n         (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n         (norm3): BatchNormAct2d(\n           256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n           (drop): Identity()\n           (act): ReLU(inplace=True)\n         )\n         (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n         (drop_path): Identity()\n       )\n       (4): PreActBottleneck(\n         (norm1): BatchNormAct2d(\n           1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n           (drop): Identity()\n           (act): ReLU(inplace=True)\n         )\n         (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n         (norm2): BatchNormAct2d(\n           256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n           (drop): Identity()\n           (act): ReLU(inplace=True)\n         )\n         (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n         (norm3): BatchNormAct2d(\n           256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n           (drop): Identity()\n           (act): ReLU(inplace=True)\n         )\n         (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n         (drop_path): Identity()\n       )\n       (5): PreActBottleneck(\n         (norm1): BatchNormAct2d(\n           1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n           (drop): Identity()\n           (act): ReLU(inplace=True)\n         )\n         (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n         (norm2): BatchNormAct2d(\n           256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n           (drop): Identity()\n           (act): ReLU(inplace=True)\n         )\n         (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n         (norm3): BatchNormAct2d(\n           256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n           (drop): Identity()\n           (act): ReLU(inplace=True)\n         )\n         (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n         (drop_path): Identity()\n       )\n     )\n   )\n   (3): ResNetStage(\n     (blocks): Sequential(\n       (0): PreActBottleneck(\n         (downsample): DownsampleConv(\n           (conv): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n           (norm): Identity()\n         )\n         (norm1): BatchNormAct2d(\n           1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n           (drop): Identity()\n           (act): ReLU(inplace=True)\n         )\n         (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n         (norm2): BatchNormAct2d(\n           512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n           (drop): Identity()\n           (act): ReLU(inplace=True)\n         )\n         (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n         (norm3): BatchNormAct2d(\n           512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n           (drop): Identity()\n           (act): ReLU(inplace=True)\n         )\n         (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n         (drop_path): Identity()\n       )\n       (1): PreActBottleneck(\n         (norm1): BatchNormAct2d(\n           2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n           (drop): Identity()\n           (act): ReLU(inplace=True)\n         )\n         (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n         (norm2): BatchNormAct2d(\n           512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n           (drop): Identity()\n           (act): ReLU(inplace=True)\n         )\n         (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n         (norm3): BatchNormAct2d(\n           512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n           (drop): Identity()\n           (act): ReLU(inplace=True)\n         )\n         (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n         (drop_path): Identity()\n       )\n       (2): PreActBottleneck(\n         (norm1): BatchNormAct2d(\n           2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n           (drop): Identity()\n           (act): ReLU(inplace=True)\n         )\n         (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n         (norm2): BatchNormAct2d(\n           512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n           (drop): Identity()\n           (act): ReLU(inplace=True)\n         )\n         (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n         (norm3): BatchNormAct2d(\n           512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n           (drop): Identity()\n           (act): ReLU(inplace=True)\n         )\n         (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n         (drop_path): Identity()\n       )\n     )\n   )\n ),\n BatchNormAct2d(\n   2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n   (drop): Identity()\n   (act): ReLU(inplace=True)\n ),\n ClassifierHead(\n   (global_pool): SelectAdaptivePool2d(pool_type=avg, flatten=Identity())\n   (drop): Dropout(p=0.0, inplace=False)\n   (fc): Conv2d(2048, 10, kernel_size=(1, 1), stride=(1, 1))\n   (flatten): Flatten(start_dim=1, end_dim=-1)\n )]",
    "crumbs": [
      "Blog",
      "Timm - Loading Models"
    ]
  },
  {
    "objectID": "save_and_load.html",
    "href": "save_and_load.html",
    "title": "Save and Load",
    "section": "",
    "text": "import torch\nimport torch.nn as nn",
    "crumbs": [
      "Blog",
      "Save and Load"
    ]
  },
  {
    "objectID": "save_and_load.html#save-arg-dict-with-python-pickle",
    "href": "save_and_load.html#save-arg-dict-with-python-pickle",
    "title": "Save and Load",
    "section": "Save arg dict with python pickle",
    "text": "Save arg dict with python pickle\ntorch.save(arg, PATH)\ntorch.load(PATH)\nmodel.load_state_dict(arg)",
    "crumbs": [
      "Blog",
      "Save and Load"
    ]
  },
  {
    "objectID": "save_and_load.html#save-model-with-python-pickle",
    "href": "save_and_load.html#save-model-with-python-pickle",
    "title": "Save and Load",
    "section": "Save model with python pickle",
    "text": "Save model with python pickle\ntorch.save(model, PATH)\n\nmodel = torch.load(PATH)\n\nmodel.eval()",
    "crumbs": [
      "Blog",
      "Save and Load"
    ]
  },
  {
    "objectID": "save_and_load.html#recommented-method",
    "href": "save_and_load.html#recommented-method",
    "title": "Save and Load",
    "section": "Recommented Method",
    "text": "Recommented Method\ntorch.save(model.state_dict(), PATH)\n\nmodel = Model(*args, **kwargs)\n\nmodel.load_state_dict(torch.load(PATH))\n\nmodel.eval()",
    "crumbs": [
      "Blog",
      "Save and Load"
    ]
  },
  {
    "objectID": "save_and_load.html#test-model",
    "href": "save_and_load.html#test-model",
    "title": "Save and Load",
    "section": "Test model",
    "text": "Test model\n\nclass Model(nn.Module):\n    def __init__(self, n_input_features):\n        super(Model, self).__init__()\n        self.linear = nn.Linear(n_input_features, 1)\n\n    def forward(self, x):\n        y_pred = torch.sigmoid(self.linear(x))\n        return y_pred\n\n\ndevice = torch.device(\"cuda\")\nmodel = Model(n_input_features = 6)\nmodel\n\nModel(\n  (linear): Linear(in_features=6, out_features=1, bias=True)\n)",
    "crumbs": [
      "Blog",
      "Save and Load"
    ]
  },
  {
    "objectID": "save_and_load.html#method-1",
    "href": "save_and_load.html#method-1",
    "title": "Save and Load",
    "section": "Method 1",
    "text": "Method 1\n\nFILE = \"model.pth1\"\ntorch.save(model, FILE)\n\n\nmodel = torch.load(FILE)\nmodel.eval()\n\nModel(\n  (linear): Linear(in_features=6, out_features=1, bias=True)\n)\n\n\n\nfor param in model.parameters():\n    print(param)\n\nParameter containing:\ntensor([[-0.3002, -0.2477, -0.2695, -0.1810, -0.0604,  0.1516]],\n       requires_grad=True)\nParameter containing:\ntensor([-0.3233], requires_grad=True)",
    "crumbs": [
      "Blog",
      "Save and Load"
    ]
  },
  {
    "objectID": "save_and_load.html#method-2",
    "href": "save_and_load.html#method-2",
    "title": "Save and Load",
    "section": "Method 2",
    "text": "Method 2\n\nFILE = \"model.pth2\"\ntorch.save(model.state_dict(), FILE)\n\n\nmodel = Model(n_input_features = 6)\n\n\nmodel.load_state_dict(torch.load(FILE))\n\nmodel.eval()\n\nModel(\n  (linear): Linear(in_features=6, out_features=1, bias=True)\n)\n\n\n\nfor param in model.parameters():\n    print(param)\n\nParameter containing:\ntensor([[-0.3002, -0.2477, -0.2695, -0.1810, -0.0604,  0.1516]],\n       requires_grad=True)\nParameter containing:\ntensor([-0.3233], requires_grad=True)\n\n\n\nmodel.state_dict()\n\nOrderedDict([('linear.weight',\n              tensor([[-0.3002, -0.2477, -0.2695, -0.1810, -0.0604,  0.1516]])),\n             ('linear.bias', tensor([-0.3233]))])\n\n\n\nlearning_rate = 0.01\noptimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\noptimizer.state_dict()\n\n{'state': {},\n 'param_groups': [{'lr': 0.01,\n   'momentum': 0,\n   'dampening': 0,\n   'weight_decay': 0,\n   'nesterov': False,\n   'maximize': False,\n   'foreach': None,\n   'differentiable': False,\n   'params': [0, 1]}]}\n\n\n\ncheckpoint = {\n    \"epoch\": 90,\n    \"model_state\": model.state_dict(),\n    \"optim_state\": optimizer.state_dict()\n}\ncheckpoint\n\n{'epoch': 90,\n 'model_state': OrderedDict([('linear.weight',\n               tensor([[-0.3002, -0.2477, -0.2695, -0.1810, -0.0604,  0.1516]])),\n              ('linear.bias', tensor([-0.3233]))]),\n 'optim_state': {'state': {},\n  'param_groups': [{'lr': 0.01,\n    'momentum': 0,\n    'dampening': 0,\n    'weight_decay': 0,\n    'nesterov': False,\n    'maximize': False,\n    'foreach': None,\n    'differentiable': False,\n    'params': [0, 1]}]}}\n\n\n\ntorch.save(checkpoint, \"checkpoint.pth\")\n\n\nloaded_checkpoint = torch.load(\"checkpoint.pth\")\n\n\nloaded_checkpoint['epoch']\n\n90\n\n\n\nmodel.load_state_dict(loaded_checkpoint['model_state'])\nmodel.state_dict()\n\nOrderedDict([('linear.weight',\n              tensor([[-0.3002, -0.2477, -0.2695, -0.1810, -0.0604,  0.1516]])),\n             ('linear.bias', tensor([-0.3233]))])\n\n\n\noptimizer.load_state_dict(loaded_checkpoint['optim_state'])\noptimizer.state_dict()\n\n{'state': {},\n 'param_groups': [{'lr': 0.01,\n   'momentum': 0,\n   'dampening': 0,\n   'weight_decay': 0,\n   'nesterov': False,\n   'maximize': False,\n   'foreach': None,\n   'differentiable': False,\n   'params': [0, 1]}]}",
    "crumbs": [
      "Blog",
      "Save and Load"
    ]
  },
  {
    "objectID": "tensorflow.html",
    "href": "tensorflow.html",
    "title": "TensorFlow",
    "section": "",
    "text": "# Load the TensorBoard notebook extension\n\n\n\n\nReusing TensorBoard on port 6006 (pid 431396), started 0:35:57 ago. (Use '!kill 431396' to kill it.)\n\n\n\n      \n      \n      \n    \n\n\n\nfrom tensorboard import notebook\nnotebook.list() # View open TensorBoard instances\n\nNo known TensorBoard instances running.\n\n\n\nnotebook.display(port=6006, height=1000)\n\n\n      \n      \n      \n    \n\n\n\nimport torch\nimport torch.nn as nn\nimport torchvision\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\n\n\n############## TENSORBOARD ########################\nimport sys\nfrom torch.utils.tensorboard import SummaryWriter\n# default `log_dir` is \"runs\" - we'll be more specific here\nwriter = SummaryWriter()\n###################################################\n\n\n# Device configuration\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Hyper-parameters \ninput_size = 784 # 28x28\nhidden_size = 500 \nnum_classes = 10\nnum_epochs = 1\nbatch_size = 64\nlearning_rate = 0.001\n\n# MNIST dataset \ntrain_dataset = torchvision.datasets.MNIST(root='./Data', \n                                           train=True, \n                                           transform=transforms.ToTensor(),  \n                                           download=True)\n\ntest_dataset = torchvision.datasets.MNIST(root='./Data', \n                                          train=False, \n                                          transform=transforms.ToTensor())\n\n# Data loader\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n                                           batch_size=batch_size, \n                                           shuffle=True)\n\ntest_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n                                          batch_size=batch_size, \n                                          shuffle=False)\n\nexamples = iter(test_loader)\nexample_data, example_targets = next(examples)\n\nfor i in range(6):\n    plt.subplot(2,3,i+1)\n    plt.imshow(example_data[i][0], cmap='gray')\n    plt.axis('off')\nplt.show()\n\n\n\n\n\n\n\n\n\n############## TENSORBOARD ########################\nimg_grid = torchvision.utils.make_grid(example_data)\nimg_grid\n\nwriter.add_image('mnist_images', img_grid)\nwriter.flush()\n#sys.exit()\n###################################################\n\n\n# Fully connected neural network with one hidden layer\nclass NeuralNet(nn.Module):\n    def __init__(self, input_size, hidden_size, num_classes):\n        super(NeuralNet, self).__init__()\n        self.input_size = input_size\n        self.l1 = nn.Linear(input_size, hidden_size) \n        self.relu = nn.ReLU()\n        self.l2 = nn.Linear(hidden_size, num_classes)  \n    \n    def forward(self, x):\n        out = self.l1(x)\n        out = self.relu(out)\n        out = self.l2(out)\n        # no activation and no softmax at the end\n        return out\n\n\nimport timm\n\n\n# Load ResNet model without the final classification layer\nmodel = timm.create_model('resnet18', pretrained=True, num_classes=10)\n# Modify the first convolution layer to accept single-channel images\nmodel.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n\nmodel = model.to(device)\n\n# model = NeuralNet(input_size, hidden_size, num_classes).to(device)\n\n# Loss and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\n\n############## TENSORBOARD ########################\n# writer.add_graph(model, example_data.reshape(-1, 28*28).to(device))\nwriter.add_graph(model, example_data.to(device))\nwriter.flush()\n#sys.exit()\n###################################################\n\n\n# Train the model\nrunning_loss = 0.0\nrunning_correct = 0\nn_total_steps = len(train_loader)\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):  \n        # origin shape: [100, 1, 28, 28]\n        # resized: [100, 784]\n        # images = images.reshape(-1, 28*28).to(device)\n        images = images.to(device)\n        labels = labels.to(device)\n        \n        # Forward pass\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        \n        # Backward and optimize\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item()\n\n        _, predicted = torch.max(outputs.data, 1)\n        running_correct += (predicted == labels).sum().item()\n        if (i+1) % 100 == 0:\n            print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n            ############## TENSORBOARD ########################\n            writer.add_scalar('training loss', running_loss / 100, epoch * n_total_steps + i)\n            running_accuracy = running_correct / 100 / predicted.size(0)\n            writer.add_scalar('accuracy', running_accuracy, epoch * n_total_steps + i)\n            running_correct = 0\n            running_loss = 0.0\n            writer.flush()\n            ###################################################\n\nEpoch [1/1], Step [100/938], Loss: 0.4240\nEpoch [1/1], Step [200/938], Loss: 0.2560\nEpoch [1/1], Step [300/938], Loss: 0.1293\nEpoch [1/1], Step [400/938], Loss: 0.1558\nEpoch [1/1], Step [500/938], Loss: 0.1048\nEpoch [1/1], Step [600/938], Loss: 0.0294\nEpoch [1/1], Step [700/938], Loss: 0.1048\nEpoch [1/1], Step [800/938], Loss: 0.1394\nEpoch [1/1], Step [900/938], Loss: 0.0257\n\n\n\n# Test the model\n# In test phase, we don't need to compute gradients (for memory efficiency)\nclass_labels = []\nclass_preds = []\nwith torch.no_grad():\n    n_correct = 0\n    n_samples = 0\n    for images, labels in test_loader:\n        # images = images.reshape(-1, 28*28).to(device)\n        images = images.to(device)\n        labels = labels.to(device)\n        outputs = model(images)\n        # max returns (value ,index)\n        values, predicted = torch.max(outputs.data, 1)\n        n_samples += labels.size(0)\n        n_correct += (predicted == labels).sum().item()\n\n        class_probs_batch = [F.softmax(output, dim=0) for output in outputs]\n\n        class_preds.append(class_probs_batch)\n        class_labels.append(labels)\n\n    # 10000, 10, and 10000, 1\n    # stack concatenates tensors along a new dimension\n    # cat concatenates tensors in the given dimension\n    class_preds = torch.cat([torch.stack(batch) for batch in class_preds])\n    class_labels = torch.cat(class_labels)\n\n    acc = 100.0 * n_correct / n_samples\n    print(f'Accuracy of the network on the 10000 test images: {acc} %')\n\n    ############## TENSORBOARD ########################\n    classes = range(10)\n    for i in classes:\n        labels_i = class_labels == i\n        preds_i = class_preds[:, i]\n        writer.add_pr_curve(str(i), labels_i, preds_i, global_step=0)\n        writer.flush()\n    ###################################################\n\nAccuracy of the network on the 10000 test images: 97.53 %\n\n\n\nwriter.close()\n\n\n\n\n Back to top",
    "crumbs": [
      "Blog",
      "TensorFlow"
    ]
  },
  {
    "objectID": "transforms.html",
    "href": "transforms.html",
    "title": "Image Classification using ImageNette + depth",
    "section": "",
    "text": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import transforms, datasets, utils\nimport torchvision.transforms.functional as TF\nfrom torch.utils.data import DataLoader\n\nfrom datetime import datetime\nfrom tqdm import tqdm \nimport matplotlib.pyplot as plt\nimport timm\nimport numpy as np\n\n\ntransform_default = transforms.Compose([\n    transforms.Resize((150, 150)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Normalize with ImageNet mean and std\n])\n\n\n# Download and load the Imagenette dataset\ntrain_dataset = datasets.Imagenette(root='Data',\n                                    split='train',\n                                    # download=True,\n                                    transform=transform_default,\n                                    )\n\n\n# Download and load the Imagenette dataset\ntest_dataset = datasets.Imagenette(root='Data',\n                                  split='val',\n                                  # download=True,\n                                  transform=transform_default,\n                                 )\n\n\n# Define a function to display images\ndef show_image(dataset):\n    # Access an image and its label from the dataset\n    image, label = dataset\n    \n    # Convert the image tensor to a NumPy array\n    image_np = image[:3].numpy().transpose((1, 2, 0))\n\n    # Display the image using Matplotlib\n    plt.imshow(image_np.clip(0,1))\n    plt.axis('off')\n    plt.title(f' {train_dataset.classes[label][0]}')\n    plt.show()\n    \n   \n# Define a function to display images\ndef show_images(images, labels, **kwargs):\n    nrows = int(np.ceil(np.sqrt(len(images))))\n    ncols = int(np.ceil(len(images)/nrows))\n    \n    fig, axes = plt.subplots(nrows, ncols, figsize=(12, 12),  **kwargs)\n    # Adjust the spacing between subplots\n    plt.subplots_adjust(wspace=0.3, hspace=0.3)\n    for ax, image, label in zip(axes.flat, images, labels):\n        # Convert image to numpy array and adjust pixel values\n        img_np = image[:3].numpy().transpose((1, 2, 0))\n        \n        # Display image\n        ax.imshow(img_np.clip(0,1))\n        ax.axis('off')\n        ax.set_title(f' {train_dataset.classes[label][0]}')\n    for ax in axes.flat[len(images):]:\n        ax.axis('off')\n        \n    plt.show()\n\n\nimage, label = train_dataset[2]\n\n\ntype(image)\n\ntorch.Tensor\n\n\n\nshow_image(train_dataset[2])",
    "crumbs": [
      "Blog",
      "Image Classification using ImageNette + depth"
    ]
  },
  {
    "objectID": "transforms.html#images",
    "href": "transforms.html#images",
    "title": "Image Classification using ImageNette + depth",
    "section": "",
    "text": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import transforms, datasets, utils\nimport torchvision.transforms.functional as TF\nfrom torch.utils.data import DataLoader\n\nfrom datetime import datetime\nfrom tqdm import tqdm \nimport matplotlib.pyplot as plt\nimport timm\nimport numpy as np\n\n\ntransform_default = transforms.Compose([\n    transforms.Resize((150, 150)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Normalize with ImageNet mean and std\n])\n\n\n# Download and load the Imagenette dataset\ntrain_dataset = datasets.Imagenette(root='Data',\n                                    split='train',\n                                    # download=True,\n                                    transform=transform_default,\n                                    )\n\n\n# Download and load the Imagenette dataset\ntest_dataset = datasets.Imagenette(root='Data',\n                                  split='val',\n                                  # download=True,\n                                  transform=transform_default,\n                                 )\n\n\n# Define a function to display images\ndef show_image(dataset):\n    # Access an image and its label from the dataset\n    image, label = dataset\n    \n    # Convert the image tensor to a NumPy array\n    image_np = image[:3].numpy().transpose((1, 2, 0))\n\n    # Display the image using Matplotlib\n    plt.imshow(image_np.clip(0,1))\n    plt.axis('off')\n    plt.title(f' {train_dataset.classes[label][0]}')\n    plt.show()\n    \n   \n# Define a function to display images\ndef show_images(images, labels, **kwargs):\n    nrows = int(np.ceil(np.sqrt(len(images))))\n    ncols = int(np.ceil(len(images)/nrows))\n    \n    fig, axes = plt.subplots(nrows, ncols, figsize=(12, 12),  **kwargs)\n    # Adjust the spacing between subplots\n    plt.subplots_adjust(wspace=0.3, hspace=0.3)\n    for ax, image, label in zip(axes.flat, images, labels):\n        # Convert image to numpy array and adjust pixel values\n        img_np = image[:3].numpy().transpose((1, 2, 0))\n        \n        # Display image\n        ax.imshow(img_np.clip(0,1))\n        ax.axis('off')\n        ax.set_title(f' {train_dataset.classes[label][0]}')\n    for ax in axes.flat[len(images):]:\n        ax.axis('off')\n        \n    plt.show()\n\n\nimage, label = train_dataset[2]\n\n\ntype(image)\n\ntorch.Tensor\n\n\n\nshow_image(train_dataset[2])",
    "crumbs": [
      "Blog",
      "Image Classification using ImageNette + depth"
    ]
  },
  {
    "objectID": "transforms.html#test-transform",
    "href": "transforms.html#test-transform",
    "title": "Image Classification using ImageNette + depth",
    "section": "Test Transform",
    "text": "Test Transform\n\nimport torch\nimport torchvision.transforms.functional as TF\n\nclass AddGrayscaleChannel(object):\n    def __init__(self):\n        super().__init__()\n    \n    def __call__(self, img):\n        # Convert the image to grayscale\n        gray_img = TF.rgb_to_grayscale(img)\n        \n        # Concatenate the grayscale image with the original image along the fourth dimension\n        img_with_gray_channel = torch.cat((img, gray_img), dim=0)\n        \n        return img_with_gray_channel\n\n    def __repr__(self):\n        return self.__class__.__name__ + '()'",
    "crumbs": [
      "Blog",
      "Image Classification using ImageNette + depth"
    ]
  },
  {
    "objectID": "transforms.html#fft-transform",
    "href": "transforms.html#fft-transform",
    "title": "Image Classification using ImageNette + depth",
    "section": "FFT Transform",
    "text": "FFT Transform\n\nclass ComputeFFT(object):\n    def __init__(self):\n        super().__init__()\n    \n    def __call__(self, image):\n        # Convert the color image to grayscale\n        grayscale_image = TF.rgb_to_grayscale(image).squeeze()\n        \n        # Convert the grayscale image to tensor and apply FFT\n        fft_result = torch.fft.fft2(grayscale_image)\n        \n        # Compute magnitude spectrum\n        magnitude_spectrum = torch.log(torch.abs(fft_result) + 1)\n        \n        # Compute phase spectrum\n        phase_spectrum = torch.angle(fft_result)\n        \n        combined_image = torch.cat((image, magnitude_spectrum.unsqueeze(0), phase_spectrum.unsqueeze(0)), dim=0)\n\n        return combined_image\n\n    def __repr__(self):\n        return self.__class__.__name__ + '()'\n\n\ntransform = ComputeFFT()\nnew_image = transform(image)\ntransposed_image = torch.transpose(new_image[:3], 0, 2).transpose(0, 1)\n\nplt.imshow(transposed_image, cmap='gray')\nplt.title('image')\nplt.colorbar()\nplt.show()\n\nplt.imshow(new_image[3], cmap='gray')\nplt.title('Magnitude Spectrum')\nplt.colorbar()\nplt.show()\n\n# Visualize phase spectrum\nplt.imshow(new_image[4], cmap='gray')\nplt.title('Phase Spectrum')\nplt.colorbar()\nplt.show()\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#model_type = \"DPT_Large\"     # MiDaS v3 - Large     (highest accuracy, slowest inference speed)\n#model_type = \"DPT_Hybrid\"   # MiDaS v3 - Hybrid    (medium accuracy, medium inference speed)\nmodel_type = \"MiDaS_small\"  # MiDaS v2.1 - Small   (lowest accuracy, highest inference speed)\n\nmidas = torch.hub.load(\"intel-isl/MiDaS\", model_type)\n\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n\nmidas_transforms = torch.hub.load(\"intel-isl/MiDaS\", \"transforms\")\n\nif model_type == \"DPT_Large\" or model_type == \"DPT_Hybrid\":\n    transform = midas_transforms.dpt_transform\nelse:\n    transform = midas_transforms.small_transform\n\nUsing cache found in /home/ben/.cache/torch/hub/intel-isl_MiDaS_master\nUsing cache found in /home/ben/.cache/torch/hub/rwightman_gen-efficientnet-pytorch_master\nUsing cache found in /home/ben/.cache/torch/hub/intel-isl_MiDaS_master\n\n\nLoading weights:  None\n\n\n\nclass ComputeDepth(object):\n    def __init__(self, model, transform, device = \"cuda\"):\n        super().__init__()\n        self.device = device\n        self.model = model\n        self.model.to(self.device)\n        self.model.eval()\n        self.transform = transform\n    \n    def __call__(self, image):\n        input_image = image.to('cpu').squeeze().numpy().transpose(1, 2, 0)\n        # input_image = np.array(image)\n        input_batch = transform(input_image).to(self.device)\n        \n        with torch.no_grad():\n            prediction = self.model(input_batch)\n            \n            prediction = torch.nn.functional.interpolate(\n                prediction.unsqueeze(1),\n                size=input_image.shape[:2],\n                mode=\"bicubic\",\n                align_corners=False,\n            ).squeeze(0)\n            prediction = prediction.to('cpu')\n\n        prediction_mean = torch.mean(prediction)\n        prediction_std = torch.std(prediction)\n        \n        # Calculate the scaling factors for normalization\n        scale_factor = 0.225 / prediction_std\n        bias = 0.45 - prediction_mean * scale_factor\n        \n        # Normalize the tensor to the desired mean and standard deviation\n        prediction = prediction * scale_factor + bias\n        \n        combined_image = torch.cat((image, prediction), dim=0)\n\n        return combined_image\n\n    def __repr__(self):\n        return self.__class__.__name__ + '()'\n\n\nimage, label = train_dataset[2]\n\n\ntype(image)\n\ntorch.Tensor\n\n\n\ntransform_depth = ComputeDepth(midas, transform = transform, device = \"cpu\")\nnew_image = transform_depth(image)\ntransposed_image = torch.transpose(new_image[:3], 0, 2).transpose(0, 1)\n\n\nplt.imshow(transposed_image, cmap='gray')\nplt.title('image')\nplt.colorbar()\nplt.show()\n\nplt.imshow(new_image[3], cmap='gray')\nplt.title('Magnitude Spectrum')\nplt.colorbar()\nplt.show()\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).",
    "crumbs": [
      "Blog",
      "Image Classification using ImageNette + depth"
    ]
  },
  {
    "objectID": "model_creation.html",
    "href": "model_creation.html",
    "title": "Pytorch Model Creation",
    "section": "",
    "text": "import torch\nimport numpy as np\ntorch.cuda.is_available()\n\nTrue",
    "crumbs": [
      "Blog",
      "Pytorch Model Creation"
    ]
  },
  {
    "objectID": "model_creation.html#autograd",
    "href": "model_creation.html#autograd",
    "title": "Pytorch Model Creation",
    "section": "Autograd",
    "text": "Autograd\n\nx = torch.tensor([1,2,3], dtype=torch.float, requires_grad = True)\nx\n\ntensor([1., 2., 3.], requires_grad=True)",
    "crumbs": [
      "Blog",
      "Pytorch Model Creation"
    ]
  },
  {
    "objectID": "model_creation.html#containers",
    "href": "model_creation.html#containers",
    "title": "Pytorch Model Creation",
    "section": "Containers",
    "text": "Containers\n\nModule\n\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\n\n\n@torch.no_grad()\ndef init_weights(m):\n    print(m)\n    if type(m) == nn.Linear:\n        m.weight.fill_(1.0)\n        print(m.weight)\n\n\nnet = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\nnew = net.apply(init_weights)\n\nLinear(in_features=2, out_features=2, bias=True)\nParameter containing:\ntensor([[1., 1.],\n        [1., 1.]], requires_grad=True)\nLinear(in_features=2, out_features=2, bias=True)\nParameter containing:\ntensor([[1., 1.],\n        [1., 1.]], requires_grad=True)\nSequential(\n  (0): Linear(in_features=2, out_features=2, bias=True)\n  (1): Linear(in_features=2, out_features=2, bias=True)\n)\n\n\n\nmodel = Model()\nmodel\n\nModel(\n  (conv1): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n  (conv2): Conv2d(20, 20, kernel_size=(5, 5), stride=(1, 1))\n)\n\n\n\nmodel.__dict__\n\n{'training': True,\n '_parameters': OrderedDict(),\n '_buffers': OrderedDict(),\n '_non_persistent_buffers_set': set(),\n '_backward_pre_hooks': OrderedDict(),\n '_backward_hooks': OrderedDict(),\n '_is_full_backward_hook': None,\n '_forward_hooks': OrderedDict(),\n '_forward_hooks_with_kwargs': OrderedDict(),\n '_forward_hooks_always_called': OrderedDict(),\n '_forward_pre_hooks': OrderedDict(),\n '_forward_pre_hooks_with_kwargs': OrderedDict(),\n '_state_dict_hooks': OrderedDict(),\n '_state_dict_pre_hooks': OrderedDict(),\n '_load_state_dict_pre_hooks': OrderedDict(),\n '_load_state_dict_post_hooks': OrderedDict(),\n '_modules': OrderedDict([('conv1',\n               Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))),\n              ('conv2', Conv2d(20, 20, kernel_size=(5, 5), stride=(1, 1)))])}\n\n\n\nfor para in model.parameters():\n    print(para.shape)\n\ntorch.Size([20, 1, 5, 5])\ntorch.Size([20])\ntorch.Size([20, 20, 5, 5])\ntorch.Size([20])\n\n\n\n\nSequential\n\nmodel = nn.Sequential(\n          nn.Conv2d(1,20,5),\n          nn.ReLU(),\n          nn.Conv2d(20,64,5),\n          nn.ReLU()\n        )\n\n\nmodel\n\nSequential(\n  (0): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n  (1): ReLU()\n  (2): Conv2d(20, 64, kernel_size=(5, 5), stride=(1, 1))\n  (3): ReLU()\n)\n\n\n\n\nModuleList\n\nclass MyModule(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linears = nn.ModuleList([nn.Linear(10, 10) for i in range(10)])\n\n    def forward(self, x):\n        # ModuleList can act as an iterable, or be indexed using ints\n        for i, l in enumerate(self.linears):\n            x = self.linears[i // 2](x) + l(x)\n        return x\n\n\nmodel = MyModule()\nmodel\n\nMyModule(\n  (linears): ModuleList(\n    (0-9): 10 x Linear(in_features=10, out_features=10, bias=True)\n  )\n)\n\n\n\nclass MyModule(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.choices = nn.ModuleDict({\n                'conv': nn.Conv2d(10, 10, 3),\n                'pool': nn.MaxPool2d(3)\n        })\n        self.activations = nn.ModuleDict([\n                ['lrelu', nn.LeakyReLU()],\n                ['prelu', nn.PReLU()]\n        ])\n\n    def forward(self, x, choice, act):\n        x = self.choices[choice](x)\n        x = self.activations[act](x)\n        return x\n\n\nmodel = MyModule()\nmodel\n\nMyModule(\n  (choices): ModuleDict(\n    (conv): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n    (pool): MaxPool2d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n  )\n  (activations): ModuleDict(\n    (lrelu): LeakyReLU(negative_slope=0.01)\n    (prelu): PReLU(num_parameters=1)\n  )\n)\n\n\n\n\nParameterList\n\nclass MyModule(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.params = nn.ParameterList([nn.Parameter(torch.randn(10, 10)) for i in range(10)])\n\n    def forward(self, x):\n        # ParameterList can act as an iterable, or be indexed using ints\n        for i, p in enumerate(self.params):\n            x = self.params[i // 2].mm(x) + p.mm(x)\n        return x\n\n\nmodel = MyModule()\nmodel\n\nMyModule(\n  (params): ParameterList(\n      (0): Parameter containing: [torch.float32 of size 10x10]\n      (1): Parameter containing: [torch.float32 of size 10x10]\n      (2): Parameter containing: [torch.float32 of size 10x10]\n      (3): Parameter containing: [torch.float32 of size 10x10]\n      (4): Parameter containing: [torch.float32 of size 10x10]\n      (5): Parameter containing: [torch.float32 of size 10x10]\n      (6): Parameter containing: [torch.float32 of size 10x10]\n      (7): Parameter containing: [torch.float32 of size 10x10]\n      (8): Parameter containing: [torch.float32 of size 10x10]\n      (9): Parameter containing: [torch.float32 of size 10x10]\n  )\n)\n\n\n\n\nParameterDict\n\nclass MyModule(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.params = nn.ParameterDict({\n                'left': nn.Parameter(torch.randn(5, 10)),\n                'right': nn.Parameter(torch.randn(5, 10))\n        })\n\n    def forward(self, x, choice):\n        x = self.params[choice].mm(x)\n        return x\n\n\nmodel = MyModule()\nmodel\n\nMyModule(\n  (params): ParameterDict(\n      (left): Parameter containing: [torch.FloatTensor of size 5x10]\n      (right): Parameter containing: [torch.FloatTensor of size 5x10]\n  )\n)",
    "crumbs": [
      "Blog",
      "Pytorch Model Creation"
    ]
  },
  {
    "objectID": "model_creation.html#convolution-layers",
    "href": "model_creation.html#convolution-layers",
    "title": "Pytorch Model Creation",
    "section": "Convolution Layers",
    "text": "Convolution Layers\n\nnn.Conv1d\n\ninput1 = torch.torch.tensor([[[[ 1.,  2.,  3., 4., 5.],\n                                  [ 6.,  7.,  8., 9., 10.],\n                                  [11., 12., 13., 14., 15.],\n                                  [16., 17., 18., 19., 20.]]]])\n\n\nnew = input1.reshape(4,5)\nnew\n\ntensor([[ 1.,  2.,  3.,  4.,  5.],\n        [ 6.,  7.,  8.,  9., 10.],\n        [11., 12., 13., 14., 15.],\n        [16., 17., 18., 19., 20.]])\n\n\n\nm = nn.Conv1d(4, 2, 3, stride=2)\ntype(m)\n\ntorch.nn.modules.conv.Conv1d\n\n\n\nfor para in m.parameters():\n    print(para.shape)\n\ntorch.Size([2, 4, 3])\ntorch.Size([2])\n\n\n\n# input = torch.randn(20, 16, 50)\noutput = m(new)\n\n\noutput.shape\n\ntorch.Size([2, 2])\n\n\n\noutput\n\ntensor([[-1.3600, -1.1102],\n        [ 6.1355,  6.0188]], grad_fn=&lt;SqueezeBackward1&gt;)\n\n\n\n\nnn.Conv2d\n\nnew = input1.reshape(4, 5,1)\nnew\n\ntensor([[[ 1.],\n         [ 2.],\n         [ 3.],\n         [ 4.],\n         [ 5.]],\n\n        [[ 6.],\n         [ 7.],\n         [ 8.],\n         [ 9.],\n         [10.]],\n\n        [[11.],\n         [12.],\n         [13.],\n         [14.],\n         [15.]],\n\n        [[16.],\n         [17.],\n         [18.],\n         [19.],\n         [20.]]])\n\n\n\n# With square kernels and equal stride\nm = nn.Conv2d(4, 2, 3, stride=2)\n# non-square kernels and unequal stride and with padding\nm = nn.Conv2d(4, 2, (3, 5), stride=(2, 1), padding=(4, 2))\n# non-square kernels and unequal stride and with padding and dilation\nm = nn.Conv2d(4, 2, (3, 5), stride=(2, 1), padding=(4, 2), dilation=(3, 1))\n\noutput = m(new)\noutput.shape\n\ntorch.Size([2, 4, 1])\n\n\n\noutput\n\ntensor([[[-1.3839],\n         [-1.8465],\n         [-0.8563],\n         [-0.0644]],\n\n        [[ 1.2715],\n         [ 1.5135],\n         [-2.8098],\n         [-3.2210]]], grad_fn=&lt;SqueezeBackward1&gt;)\n\n\n\n\nnn.Conv3d\n\n# With square kernels and equal stride\nm = nn.Conv3d(16, 33, 3, stride=2)\n# non-square kernels and unequal} stride and with padding\nm = nn.Conv3d(16, 33, (3, 5, 2), stride=(2, 1, 1), padding=(4, 2, 0))\ninput = torch.randn(20, 16, 10, 50, 100)\noutput = m(input)\n\n\noutput.shape\n\ntorch.Size([20, 33, 8, 50, 99])\n\n\n\n\nnn.ConvTranspose2d\n\nThis module can be seen as the gradient of Conv2d with respect to its input. It is also known as a fractionally-strided convolution or a deconvolution (although it is not an actual deconvolution operation as it does not compute a true inverse of convolution).\n\n\n# With square kernels and equal stride\nm = nn.ConvTranspose2d(16, 33, 3, stride=2)\n# non-square kernels and unequal stride and with padding\nm = nn.ConvTranspose2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2))\ninput = torch.randn(20, 16, 50, 100)\noutput = m(input)\noutput.shape\n\ntorch.Size([20, 33, 93, 100])\n\n\n\n# exact output size can be also specified as an argument\ninput = torch.randn(1, 16, 12, 12)\ndownsample = nn.Conv2d(16, 16, 3, stride=2, padding=1)\nupsample = nn.ConvTranspose2d(16, 16, 3, stride=2, padding=1)\nh = downsample(input)\nh.size()\n\ntorch.Size([1, 16, 6, 6])\n\n\n\noutput = upsample(h, output_size=input.size())\noutput.size()\n\ntorch.Size([1, 16, 12, 12])",
    "crumbs": [
      "Blog",
      "Pytorch Model Creation"
    ]
  },
  {
    "objectID": "model_creation.html#pooling-layers",
    "href": "model_creation.html#pooling-layers",
    "title": "Pytorch Model Creation",
    "section": "Pooling Layers",
    "text": "Pooling Layers\n\nnn.MaxPool2d\n\nApplies a 2D max pooling over an input signal composed of several input planes.\n\n\nnew = input1.reshape(1, 4,5)\nnew\n\ntensor([[[ 1.,  2.,  3.,  4.,  5.],\n         [ 6.,  7.,  8.,  9., 10.],\n         [11., 12., 13., 14., 15.],\n         [16., 17., 18., 19., 20.]]])\n\n\n\n# pool of square window of size=3, stride=2\nm = nn.MaxPool2d(3, stride=2)\n# pool of non-square window\nm = nn.MaxPool2d((2, 2), stride=(2, 1))\ninput = torch.randn(20, 16, 50, 32)\noutput = m(new)\noutput\n\ntensor([[[ 7.,  8.,  9., 10.],\n         [17., 18., 19., 20.]]])\n\n\n\n\nnn.MaxUnpool2d\n\nComputes a partial inverse of MaxPool2d.\n\n\ninput = torch.tensor([[[[ 1.,  2.,  3.,  4.],\n                            [ 5.,  6.,  7.,  8.],\n                            [ 9., 10., 11., 12.],\n                            [13., 14., 15., 16.]]]])\n\n\npool = nn.MaxPool2d(2, stride=2, return_indices=True)\nunpool = nn.MaxUnpool2d(2, stride=2)\n\noutput, indices = pool(input)\nunpool(output, indices)\n\ntensor([[[[ 0.,  0.,  0.,  0.],\n          [ 0.,  6.,  0.,  8.],\n          [ 0.,  0.,  0.,  0.],\n          [ 0., 14.,  0., 16.]]]])\n\n\n\ninput1\n\ntensor([[[[ 1.,  2.,  3.,  4.,  5.],\n          [ 6.,  7.,  8.,  9., 10.],\n          [11., 12., 13., 14., 15.],\n          [16., 17., 18., 19., 20.]]]])\n\n\n\noutput, indices = pool(input1)\n# This call will not work without specifying output_size\nunpool(output, indices, output_size=input1.size())\n\ntensor([[[[ 0.,  0.,  0.,  0.,  0.],\n          [ 0.,  7.,  0.,  9.,  0.],\n          [ 0.,  0.,  0.,  0.,  0.],\n          [ 0., 17.,  0., 19.,  0.]]]])\n\n\n\n\nnn.AvgPool2d\n\nApplies a 2D average pooling over an input signal composed of several input planes.\n\n\ninput1\n\ntensor([[[[ 1.,  2.,  3.,  4.,  5.],\n          [ 6.,  7.,  8.,  9., 10.],\n          [11., 12., 13., 14., 15.],\n          [16., 17., 18., 19., 20.]]]])\n\n\n\n# pool of square window of size=3, stride=2\nm = nn.AvgPool2d(3, stride=2)\n# pool of non-square window\nm = nn.AvgPool2d((2, 2), stride=(1, 1))\n# input = torch.randn(20, 16, 50, 32)\noutput = m(input1)\noutput\n\ntensor([[[[ 4.,  5.,  6.,  7.],\n          [ 9., 10., 11., 12.],\n          [14., 15., 16., 17.]]]])\n\n\n\n\nnn.FractionalMaxPool2d\n\nApplies a 2D fractional max pooling over an input signal composed of several input planes.\n\n\n# pool of square window of size=3, and target output size 13x12\nm = nn.FractionalMaxPool2d(3, output_size=(13, 12))\n# pool of square window and target output size being half of input image size\nm = nn.FractionalMaxPool2d(2, output_ratio=(0.7, 0.7))\noutput = m(input1)\noutput\n\ntensor([[[[ 7.,  8., 10.],\n          [17., 18., 20.]]]])\n\n\n\n\nnn.LPPool2d\n\nApplies a 2D power-average pooling over an input signal composed of several input planes.\n\n\n# power-2 pool of square window of size=3, stride=2\nm = nn.LPPool2d(2, 3, stride=2)\n# pool of non-square window of power 1.2\nm = nn.LPPool2d(1.2, (3, 2), stride=(2, 1))\n\noutput = m(input)\noutput\n\ntensor([[[[25.4396, 29.7206, 34.0561]]]])\n\n\n\n\nnn.AdaptiveMaxPool2d\n\nApplies a 2D adaptive max pooling over an input signal composed of several input planes.\n\n\n# target output size of 5x7\nm = nn.AdaptiveMaxPool2d((5, 7))\noutput = m(input1)\noutput\n\ntensor([[[[ 1.,  2.,  3.,  3.,  4.,  5.,  5.],\n          [ 6.,  7.,  8.,  8.,  9., 10., 10.],\n          [11., 12., 13., 13., 14., 15., 15.],\n          [16., 17., 18., 18., 19., 20., 20.],\n          [16., 17., 18., 18., 19., 20., 20.]]]])\n\n\n\n# target output size of 7x7 (square)\nm = nn.AdaptiveMaxPool2d(7)\noutput = m(input1)\noutput\n\ntensor([[[[ 1.,  2.,  3.,  3.,  4.,  5.,  5.],\n          [ 6.,  7.,  8.,  8.,  9., 10., 10.],\n          [ 6.,  7.,  8.,  8.,  9., 10., 10.],\n          [11., 12., 13., 13., 14., 15., 15.],\n          [11., 12., 13., 13., 14., 15., 15.],\n          [16., 17., 18., 18., 19., 20., 20.],\n          [16., 17., 18., 18., 19., 20., 20.]]]])\n\n\n\n# target output size of 10x7\nm = nn.AdaptiveMaxPool2d((None, 7))\noutput = m(input1)\noutput\n\ntensor([[[[ 1.,  2.,  3.,  3.,  4.,  5.,  5.],\n          [ 6.,  7.,  8.,  8.,  9., 10., 10.],\n          [11., 12., 13., 13., 14., 15., 15.],\n          [16., 17., 18., 18., 19., 20., 20.]]]])\n\n\n\n\nnn.AdaptiveAvgPool2d\n\nApplies a 2D adaptive average pooling over an input signal composed of several input planes.\n\n\n# target output size of 5x7\nm = nn.AdaptiveAvgPool2d((5, 7))\nm(input1)\n\ntensor([[[[ 1.0000,  1.5000,  2.5000,  3.0000,  3.5000,  4.5000,  5.0000],\n          [ 3.5000,  4.0000,  5.0000,  5.5000,  6.0000,  7.0000,  7.5000],\n          [ 8.5000,  9.0000, 10.0000, 10.5000, 11.0000, 12.0000, 12.5000],\n          [13.5000, 14.0000, 15.0000, 15.5000, 16.0000, 17.0000, 17.5000],\n          [16.0000, 16.5000, 17.5000, 18.0000, 18.5000, 19.5000, 20.0000]]]])\n\n\n\n# target output size of 7x7 (square)\nm = nn.AdaptiveAvgPool2d(7)\nm(input1)\n\ntensor([[[[ 1.0000,  1.5000,  2.5000,  3.0000,  3.5000,  4.5000,  5.0000],\n          [ 3.5000,  4.0000,  5.0000,  5.5000,  6.0000,  7.0000,  7.5000],\n          [ 6.0000,  6.5000,  7.5000,  8.0000,  8.5000,  9.5000, 10.0000],\n          [ 8.5000,  9.0000, 10.0000, 10.5000, 11.0000, 12.0000, 12.5000],\n          [11.0000, 11.5000, 12.5000, 13.0000, 13.5000, 14.5000, 15.0000],\n          [13.5000, 14.0000, 15.0000, 15.5000, 16.0000, 17.0000, 17.5000],\n          [16.0000, 16.5000, 17.5000, 18.0000, 18.5000, 19.5000, 20.0000]]]])\n\n\n\n# target output size of 10x7\nm = nn.AdaptiveAvgPool2d((None, 7))\nm(input1)\n\ntensor([[[[ 1.0000,  1.5000,  2.5000,  3.0000,  3.5000,  4.5000,  5.0000],\n          [ 6.0000,  6.5000,  7.5000,  8.0000,  8.5000,  9.5000, 10.0000],\n          [11.0000, 11.5000, 12.5000, 13.0000, 13.5000, 14.5000, 15.0000],\n          [16.0000, 16.5000, 17.5000, 18.0000, 18.5000, 19.5000, 20.0000]]]])",
    "crumbs": [
      "Blog",
      "Pytorch Model Creation"
    ]
  },
  {
    "objectID": "model_creation.html#padding-layers",
    "href": "model_creation.html#padding-layers",
    "title": "Pytorch Model Creation",
    "section": "Padding Layers",
    "text": "Padding Layers\n\nnn.ReflectionPad2d\n\nPads the input tensor using the reflection of the input boundary.\n\n\nm = nn.ReflectionPad2d(2)\ninput = torch.arange(9, dtype=torch.float).reshape(1, 1, 3, 3)\ninput\n\ntensor([[[[0., 1., 2.],\n          [3., 4., 5.],\n          [6., 7., 8.]]]])\n\n\n\nm(input)\n\ntensor([[[[8., 7., 6., 7., 8., 7., 6.],\n          [5., 4., 3., 4., 5., 4., 3.],\n          [2., 1., 0., 1., 2., 1., 0.],\n          [5., 4., 3., 4., 5., 4., 3.],\n          [8., 7., 6., 7., 8., 7., 6.],\n          [5., 4., 3., 4., 5., 4., 3.],\n          [2., 1., 0., 1., 2., 1., 0.]]]])\n\n\n\n# using different paddings for different sides\nm = nn.ReflectionPad2d((1, 1, 2, 0))\nm(input)\n\ntensor([[[[7., 6., 7., 8., 7.],\n          [4., 3., 4., 5., 4.],\n          [1., 0., 1., 2., 1.],\n          [4., 3., 4., 5., 4.],\n          [7., 6., 7., 8., 7.]]]])\n\n\n\n\nnn.ReplicationPad2d\n\nPads the input tensor using replication of the input boundary.\n\n\nm = nn.ReplicationPad2d(2)\n\nm(input)\n\ntensor([[[[0., 0., 0., 1., 2., 2., 2.],\n          [0., 0., 0., 1., 2., 2., 2.],\n          [0., 0., 0., 1., 2., 2., 2.],\n          [3., 3., 3., 4., 5., 5., 5.],\n          [6., 6., 6., 7., 8., 8., 8.],\n          [6., 6., 6., 7., 8., 8., 8.],\n          [6., 6., 6., 7., 8., 8., 8.]]]])\n\n\n\n# using different paddings for different sides\nm = nn.ReplicationPad2d((1, 1, 2, 0))\nm(input)\n\ntensor([[[[0., 0., 1., 2., 2.],\n          [0., 0., 1., 2., 2.],\n          [0., 0., 1., 2., 2.],\n          [3., 3., 4., 5., 5.],\n          [6., 6., 7., 8., 8.]]]])\n\n\n\n\nnn.ZeroPad2d\n\nPads the input tensor boundaries with zero.\n\n\nm = nn.ZeroPad2d(2)\nm(input)\n\ntensor([[[[0., 0., 0., 0., 0., 0., 0.],\n          [0., 0., 0., 0., 0., 0., 0.],\n          [0., 0., 0., 1., 2., 0., 0.],\n          [0., 0., 3., 4., 5., 0., 0.],\n          [0., 0., 6., 7., 8., 0., 0.],\n          [0., 0., 0., 0., 0., 0., 0.],\n          [0., 0., 0., 0., 0., 0., 0.]]]])\n\n\n\n# using different paddings for different sides\nm = nn.ZeroPad2d((1, 1, 2, 0))\nm(input)\n\ntensor([[[[0., 0., 0., 0., 0.],\n          [0., 0., 0., 0., 0.],\n          [0., 0., 1., 2., 0.],\n          [0., 3., 4., 5., 0.],\n          [0., 6., 7., 8., 0.]]]])\n\n\n\n\nnn.ConstantPad2d\n\nPads the input tensor boundaries with a constant value.\n\n\nm = nn.ConstantPad2d(2, 11)\n\nm(input)\n\ntensor([[[[11., 11., 11., 11., 11., 11., 11.],\n          [11., 11., 11., 11., 11., 11., 11.],\n          [11., 11.,  0.,  1.,  2., 11., 11.],\n          [11., 11.,  3.,  4.,  5., 11., 11.],\n          [11., 11.,  6.,  7.,  8., 11., 11.],\n          [11., 11., 11., 11., 11., 11., 11.],\n          [11., 11., 11., 11., 11., 11., 11.]]]])\n\n\n\n# using different paddings for different sides\nm = nn.ConstantPad2d((3, 0, 2, 1), 11)\nm(input)\n\ntensor([[[[11., 11., 11., 11., 11., 11.],\n          [11., 11., 11., 11., 11., 11.],\n          [11., 11., 11.,  0.,  1.,  2.],\n          [11., 11., 11.,  3.,  4.,  5.],\n          [11., 11., 11.,  6.,  7.,  8.],\n          [11., 11., 11., 11., 11., 11.]]]])",
    "crumbs": [
      "Blog",
      "Pytorch Model Creation"
    ]
  },
  {
    "objectID": "model_creation.html#non-linear-activations-weighted-sum-nonlinearity",
    "href": "model_creation.html#non-linear-activations-weighted-sum-nonlinearity",
    "title": "Pytorch Model Creation",
    "section": "Non-linear Activations (weighted sum, nonlinearity)",
    "text": "Non-linear Activations (weighted sum, nonlinearity)\n\ninput = torch.linspace(-5,5,100)\n\n\nimport matplotlib.pyplot as plt \ndef plot_show(input, output):\n    plt.plot(input, input, color='green', linestyle='dashed',\n         linewidth=1, label = 'input')\n    plt.plot(input, output, color='red',\n             linewidth=1, label = 'output')\n    plt.legend()\n    plt.show()\n\n\nnn.LogSigmoid()\n\nm = nn.LogSigmoid()\noutput = m(input)\n\n\nplot_show(input, output)\n\n\n\n\n\n\n\n\n\n\nnn.ReLU()\n\nm = nn.ReLU()\noutput = m(input)\nplot_show(input, output)\n\n\n\n\n\n\n\n\n\n\nnn.LeakyReLU(0.5)\n\nm = nn.LeakyReLU(0.5)\noutput = m(input)\nplot_show(input, output)\n\n\n\n\n\n\n\n\n\nm = nn.SELU()\noutput = m(input)\nplot_show(input, output)\n\n\n\n\n\n\n\n\n\n\nnn.Sigmoid()\n\nm = nn.Sigmoid()\noutput = m(input)\nplot_show(input, output)\n\n\n\n\n\n\n\n\n\n\nnn.Softplus()\n\nm = nn.Softplus()\noutput = m(input)\nplot_show(input, output)\n\n\n\n\n\n\n\n\n\n\nnn.Tanh()\n\nm = nn.Tanh()\noutput = m(input)\nplot_show(input, output)\n\n\n\n\n\n\n\n\n\n\nnn.Threshold\n\nm = nn.Threshold(0, -5)\noutput = m(input)\nplot_show(input, output)\n\n\n\n\n\n\n\n\n\n\nnn.SELU\n\n\nNon-linear Activations (other)\n\ninput = torch.linspace(-1,1,10)\ninput = input.reshape(2,5)\ninput\n\ntensor([[-1.0000, -0.7778, -0.5556, -0.3333, -0.1111],\n        [ 0.1111,  0.3333,  0.5556,  0.7778,  1.0000]])\n\n\n\nm = nn.Softmin(dim=1)\n\n\noutput = m(input)\n\n\noutput\n\ntensor([[0.2970, 0.2379, 0.1905, 0.1525, 0.1221],\n        [0.2970, 0.2379, 0.1905, 0.1525, 0.1221]])\n\n\n\noutput.sum()\n\ntensor(2.0000)\n\n\n\nplot_show(input.flatten(), output.flatten())\n\n\n\n\n\n\n\n\n\n\nNormalization Layers\n\n\nnn.BatchNorm2d\n\ninput = torch.arange(27, dtype=torch.float).reshape(1,3, 3, 3)\ninput\n\ntensor([[[[ 0.,  1.,  2.],\n          [ 3.,  4.,  5.],\n          [ 6.,  7.,  8.]],\n\n         [[ 9., 10., 11.],\n          [12., 13., 14.],\n          [15., 16., 17.]],\n\n         [[18., 19., 20.],\n          [21., 22., 23.],\n          [24., 25., 26.]]]])\n\n\n\n# With Learnable Parameters\nm = nn.BatchNorm2d(3)\n# Without Learnable Parameters\nm = nn.BatchNorm2d(3, affine=False)\n\noutput = m(input)\noutput\n\ntensor([[[[-1.5492e+00, -1.1619e+00, -7.7460e-01],\n          [-3.8730e-01,  0.0000e+00,  3.8730e-01],\n          [ 7.7460e-01,  1.1619e+00,  1.5492e+00]],\n\n         [[-1.5492e+00, -1.1619e+00, -7.7460e-01],\n          [-3.8730e-01,  1.7881e-07,  3.8730e-01],\n          [ 7.7460e-01,  1.1619e+00,  1.5492e+00]],\n\n         [[-1.5492e+00, -1.1619e+00, -7.7460e-01],\n          [-3.8730e-01, -3.5763e-07,  3.8730e-01],\n          [ 7.7460e-01,  1.1619e+00,  1.5492e+00]]]])\n\n\n\n\nnn.GroupNorm\n\n# Separate 6 channels into 3 groups\nm = nn.GroupNorm(1, 3)\noutput = m(input)\noutput\n\ntensor([[[[-1.6690e+00, -1.5407e+00, -1.4123e+00],\n          [-1.2839e+00, -1.1555e+00, -1.0271e+00],\n          [-8.9872e-01, -7.7033e-01, -6.4194e-01]],\n\n         [[-5.1355e-01, -3.8516e-01, -2.5678e-01],\n          [-1.2839e-01, -2.9802e-08,  1.2839e-01],\n          [ 2.5678e-01,  3.8516e-01,  5.1355e-01]],\n\n         [[ 6.4194e-01,  7.7033e-01,  8.9872e-01],\n          [ 1.0271e+00,  1.1555e+00,  1.2839e+00],\n          [ 1.4123e+00,  1.5407e+00,  1.6690e+00]]]],\n       grad_fn=&lt;NativeGroupNormBackward0&gt;)\n\n\n\ninput_t = torch.randn(20, 6, 10, 10)\n# Separate 6 channels into 6 groups (equivalent with InstanceNorm)\nm = nn.GroupNorm(6, 6)\noutput = m(input_t)\n\n\n# Put all 6 channels into a single group (equivalent with LayerNorm)\nm = nn.GroupNorm(1, 6)\n# Activating the module\noutput = m(input_t)\n\n\n\nnn.LayerNorm\n\n# NLP Example\nbatch, sentence_length, embedding_dim = 20, 5, 10\nembedding = torch.randn(batch, sentence_length, embedding_dim)\nlayer_norm = nn.LayerNorm(embedding_dim)\n# Activate module\noutput = layer_norm(embedding)\n\n\nembedding[7,:,:].std()\n\ntensor(1.0733)\n\n\n\noutput[7,:,:].std()\n\ntensor(1.0101, grad_fn=&lt;StdBackward0&gt;)",
    "crumbs": [
      "Blog",
      "Pytorch Model Creation"
    ]
  },
  {
    "objectID": "model_creation.html#recurrent-layers",
    "href": "model_creation.html#recurrent-layers",
    "title": "Pytorch Model Creation",
    "section": "Recurrent Layers",
    "text": "Recurrent Layers\n\nRNNBase - Base class for RNN modules (RNN, LSTM, GRU).",
    "crumbs": [
      "Blog",
      "Pytorch Model Creation"
    ]
  },
  {
    "objectID": "model_creation.html#nn.rnn",
    "href": "model_creation.html#nn.rnn",
    "title": "Pytorch Model Creation",
    "section": "nn.RNN",
    "text": "nn.RNN\n\nrnn = nn.RNN(10, 20, 2)\ninput = torch.randn(5, 3, 10)\nh0 = torch.randn(2, 3, 20)\noutput, hn = rnn(input, h0)\n\n\noutput.shape\n\ntorch.Size([5, 3, 20])\n\n\n\nnn.LSTM\n\nlstm = nn.LSTM(10, 20, 2)\ninput = torch.randn(5, 3, 10)\nh0 = torch.randn(2, 3, 20)\nc0 = torch.randn(2, 3, 20)\noutput, (hn, cn) = lstm(input, (h0, c0))\noutput.shape\n\ntorch.Size([5, 3, 20])\n\n\n\n\nnn.GRU\n\ngru = nn.GRU(10, 20, 2)\ninput = torch.randn(5, 3, 10)\nh0 = torch.randn(2, 3, 20)\noutput, hn = gru(input, h0)\noutput.shape\n\ntorch.Size([5, 3, 20])\n\n\n\n\nnn.RNNCell\n\nrnn = nn.RNNCell(10, 20)\ninput = torch.randn(6, 3, 10)\nhx = torch.randn(3, 20)\noutput = []\nfor i in range(6):\n    hx = rnn(input[i], hx)\n    output.append(hx)",
    "crumbs": [
      "Blog",
      "Pytorch Model Creation"
    ]
  },
  {
    "objectID": "model_creation.html#transformer-layers",
    "href": "model_creation.html#transformer-layers",
    "title": "Pytorch Model Creation",
    "section": "Transformer Layers",
    "text": "Transformer Layers\n\nnn.Transformer\n\ntransformer_model = nn.Transformer(nhead=16, num_encoder_layers=12)\nsrc = torch.rand((10, 32, 512))\ntgt = torch.rand((20, 32, 512))\nout = transformer_model(src, tgt)\n\n\n\nnn.TransformerEncoderLayer\n\nencoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)\ntransformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=6)\nsrc = torch.rand(10, 32, 512)\nout = transformer_encoder(src)\n\n\n\nnn.TransformerDecoderLayer\n\ndecoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8)\ntransformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=6)\nmemory = torch.rand(10, 32, 512)\ntgt = torch.rand(20, 32, 512)\nout = transformer_decoder(tgt, memory)\n\n\n\nnn.TransformerEncoderLayer\n\nencoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8, batch_first=True)\nsrc = torch.rand(32, 10, 512)\nout = encoder_layer(src)\n\n\n\nnn.TransformerDecoderLayer\n\ndecoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8, batch_first=True)\nmemory = torch.rand(32, 10, 512)\ntgt = torch.rand(32, 20, 512)\nout = decoder_layer(tgt, memory)",
    "crumbs": [
      "Blog",
      "Pytorch Model Creation"
    ]
  },
  {
    "objectID": "model_creation.html#linear-layers",
    "href": "model_creation.html#linear-layers",
    "title": "Pytorch Model Creation",
    "section": "Linear Layers",
    "text": "Linear Layers\n\nnn.Identity\n\nA placeholder identity operator that is argument-insensitive.\n\n\nm = nn.Identity(54, unused_argument1=0.1, unused_argument2=False)\ninput = torch.randn(128, 20)\noutput = m(input)\nprint(output.size())\n\ntorch.Size([128, 20])\n\n\n\n\nnn.Linear\n\nm = nn.Linear(20, 30)\ninput = torch.randn(128, 20)\noutput = m(input)\nprint(output.size())\n\ntorch.Size([128, 30])\n\n\n\nfor para in m.parameters():\n    print(para.shape)\n\ntorch.Size([30, 20])\ntorch.Size([30])\n\n\n\n\nDropout Layers\n\nm = nn.Dropout(p=0.2)\ninput = torch.randn(20, 16)\noutput = m(input)\noutput.shape\n\ntorch.Size([20, 16])\n\n\n\nm = nn.Dropout2d(p=0.2)\ninput = torch.randn(20, 16, 32, 32)\noutput = m(input)\noutput.shape\n\ntorch.Size([20, 16, 32, 32])\n\n\n\nm = nn.AlphaDropout(p=0.2)\ninput = torch.randn(20, 16)\noutput = m(input)\noutput.shape\n\ntorch.Size([20, 16])\n\n\n\nm = nn.FeatureAlphaDropout(p=0.2)\ninput = torch.randn(20, 16, 4, 32, 32)\noutput = m(input)\noutput.shape\n\ntorch.Size([20, 16, 4, 32, 32])\n\n\n\n\nLoss Functions\n\n\nnn.L1Loss\n\ninput = torch.linspace(1,10,10, requires_grad=True)\ntarget = torch.linspace(1.1, 10.1, 10, requires_grad=True)\ninput, target\n\n(tensor([ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.], requires_grad=True),\n tensor([ 1.1000,  2.1000,  3.1000,  4.1000,  5.1000,  6.1000,  7.1000,  8.1000,\n          9.1000, 10.1000], requires_grad=True))\n\n\n\nloss = nn.L1Loss()\noutput = loss(input, target)\noutput\n\ntensor(0.1000, grad_fn=&lt;MeanBackward0&gt;)\n\n\n\n\nnn.MSELoss\n\nloss = nn.MSELoss()\noutput = loss(input, target)\noutput\n\ntensor(0.0100, grad_fn=&lt;MseLossBackward0&gt;)\n\n\n\n\nnn.CrossEntropyLoss\n\n# Example of target with class indices\nloss = nn.CrossEntropyLoss()\noutput = loss(input, target)\noutput\n\ntensor(195.1833, grad_fn=&lt;DivBackward1&gt;)\n\n\n\nloss = nn.GaussianNLLLoss()\n\nvar = torch.ones(10, requires_grad=True)  # heteroscedastic\noutput = loss(input, target, var)\noutput\n\ntensor(0.0050, grad_fn=&lt;MeanBackward0&gt;)",
    "crumbs": [
      "Blog",
      "Pytorch Model Creation"
    ]
  },
  {
    "objectID": "model_creation.html#vision-layers",
    "href": "model_creation.html#vision-layers",
    "title": "Pytorch Model Creation",
    "section": "Vision Layers",
    "text": "Vision Layers\nRearrange elements in a tensor according to an upscaling factor.\n\npixel_shuffle = nn.PixelShuffle(3)\ninput = torch.randn(1, 9, 4, 4)\noutput = pixel_shuffle(input)\nprint(output.size())\n\ntorch.Size([1, 1, 12, 12])\n\n\n\npixel_unshuffle = nn.PixelUnshuffle(3)\ninput = torch.randn(1, 1, 12, 12)\noutput = pixel_unshuffle(input)\nprint(output.size())\n\ntorch.Size([1, 9, 4, 4])",
    "crumbs": [
      "Blog",
      "Pytorch Model Creation"
    ]
  },
  {
    "objectID": "model_diagrams.html",
    "href": "model_diagrams.html",
    "title": "Model Diagrams",
    "section": "",
    "text": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.transforms as transforms \nfrom torchvision.transforms import ToPILImage\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n\n#Option 1 (create nn modules)\nclass NeuralNet2(nn.Module):\n    def __init__(self, input_size, hidden_size, num_classes):\n        super(NeuralNet2, self).__init__()\n        self.linear1 = nn.Linear(input_size, hidden_size)\n        self.relu = nn.ReLU()\n        self.linear2 = nn.Linear(hidden_size, num_classes)\n        self.sigmoid = nn.Sigmoid()\n        \n\n    def forward(self, x):\n        out = self.linear1(x)\n        out = self.relu(out)\n        out = self.linear2(out)\n        out = self.sigmoid(out)\n\n        return out\n\n\n# Image Classifier Neural Network\nclass ImageClassifier(nn.Module): \n    def __init__(self):\n        super().__init__()\n        self.model = nn.Sequential(\n            nn.Conv2d(3, 32, (3,3)), \n            nn.ReLU(),\n            nn.Conv2d(32, 64, (3,3)), \n            nn.ReLU(),\n            nn.Conv2d(64, 64, (3,3)), \n            nn.ReLU(),\n            nn.Flatten(), \n            nn.Linear(64*(28-2)*(28-2), 10)  \n        )\n\n    def forward(self, x): \n        return self.model(x)\n\n\nmodel = ImageClassifier()\nmodel\n\nImageClassifier(\n  (model): Sequential(\n    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1))\n    (1): ReLU()\n    (2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n    (3): ReLU()\n    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n    (5): ReLU()\n    (6): Flatten(start_dim=1, end_dim=-1)\n    (7): Linear(in_features=43264, out_features=10, bias=True)\n  )\n)\n\n\n\nfor item in model.state_dict():\n    print(f'{item}        size: {model.state_dict()[item].shape}')\n\nmodel.0.weight        size: torch.Size([32, 3, 3, 3])\nmodel.0.bias        size: torch.Size([32])\nmodel.2.weight        size: torch.Size([64, 32, 3, 3])\nmodel.2.bias        size: torch.Size([64])\nmodel.4.weight        size: torch.Size([64, 64, 3, 3])\nmodel.4.bias        size: torch.Size([64])\nmodel.7.weight        size: torch.Size([10, 43264])\nmodel.7.bias        size: torch.Size([10])\n\n\n\nfor param in model.parameters():\n    print(param.shape)\n\ntorch.Size([32, 3, 3, 3])\ntorch.Size([32])\ntorch.Size([64, 32, 3, 3])\ntorch.Size([64])\ntorch.Size([64, 64, 3, 3])\ntorch.Size([64])\ntorch.Size([10, 43264])\ntorch.Size([10])\n\n\n\nfor child in model.children():\n    print(child)\n\nSequential(\n  (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1))\n  (1): ReLU()\n  (2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n  (3): ReLU()\n  (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n  (5): ReLU()\n  (6): Flatten(start_dim=1, end_dim=-1)\n  (7): Linear(in_features=43264, out_features=10, bias=True)\n)\n\n\n\na = 'sadf(asdf(123(123'\na.split('(', 1)\n\n['sadf', 'asdf(123(123']\n\n\n\n# Iterate through each layer in the model\nfor name, layer in model.named_modules():\n    layer_name = str(layer).split('(', 1)[0]\n    print(f'[name:{name}] layer:{layer_name}')\n\n[name:] layer:ImageClassifier\n[name:model] layer:Sequential\n[name:model.0] layer:Conv2d\n[name:model.1] layer:ReLU\n[name:model.2] layer:Conv2d\n[name:model.3] layer:ReLU\n[name:model.4] layer:Conv2d\n[name:model.5] layer:ReLU\n[name:model.6] layer:Flatten\n[name:model.7] layer:Linear\n\n\n\ndef create_model_diagram(model):\n    from nbdevAuto.functions import graph\n    from graphviz import Digraph\n    dot = graph()\n    dot.attr(rankdir='TB')\n\n    for name, layer in model.named_modules():\n        layer_name = str(layer).split('(', 1)[0]\n        node = f'{name}\\n{layer_name}'\n        dot.node(node, node)\n\n    for index, (name, layer) in enumerate(model.named_modules()):\n        layer_name = str(layer).split('(', 1)[0]\n        node = f'{name}\\n{layer_name}'\n        if index == 0:\n            previous = node\n            continue    \n        \n        dot.edge(previous, node)\n        previous = node\n\n    # Render the graph\n    return dot\n\n\ncreate_model_diagram(model)\n\n\n\n\n\n\n\n\n\nfrom torchviz import make_dot\n\n\n# Define a dummy input tensor\ndummy_input = torch.randn(1, 3, 32, 32)\n\n# Perform a forward pass\noutput = model(dummy_input)\n\n# Visualize the model's forward structure\n# This will create a graph representing the forward pass\ngraph = make_dot(output, params=dict(model.named_parameters()))\ngraph\n\n\n\n\n\n\n\n\n\ngraph.render(\"forward_structure\", format=\"png\", cleanup=True)\n\n'forward_structure.png'",
    "crumbs": [
      "Blog",
      "Model Diagrams"
    ]
  },
  {
    "objectID": "model_diagrams.html#neutral-network",
    "href": "model_diagrams.html#neutral-network",
    "title": "Model Diagrams",
    "section": "",
    "text": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.transforms as transforms \nfrom torchvision.transforms import ToPILImage\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n\n#Option 1 (create nn modules)\nclass NeuralNet2(nn.Module):\n    def __init__(self, input_size, hidden_size, num_classes):\n        super(NeuralNet2, self).__init__()\n        self.linear1 = nn.Linear(input_size, hidden_size)\n        self.relu = nn.ReLU()\n        self.linear2 = nn.Linear(hidden_size, num_classes)\n        self.sigmoid = nn.Sigmoid()\n        \n\n    def forward(self, x):\n        out = self.linear1(x)\n        out = self.relu(out)\n        out = self.linear2(out)\n        out = self.sigmoid(out)\n\n        return out\n\n\n# Image Classifier Neural Network\nclass ImageClassifier(nn.Module): \n    def __init__(self):\n        super().__init__()\n        self.model = nn.Sequential(\n            nn.Conv2d(3, 32, (3,3)), \n            nn.ReLU(),\n            nn.Conv2d(32, 64, (3,3)), \n            nn.ReLU(),\n            nn.Conv2d(64, 64, (3,3)), \n            nn.ReLU(),\n            nn.Flatten(), \n            nn.Linear(64*(28-2)*(28-2), 10)  \n        )\n\n    def forward(self, x): \n        return self.model(x)\n\n\nmodel = ImageClassifier()\nmodel\n\nImageClassifier(\n  (model): Sequential(\n    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1))\n    (1): ReLU()\n    (2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n    (3): ReLU()\n    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n    (5): ReLU()\n    (6): Flatten(start_dim=1, end_dim=-1)\n    (7): Linear(in_features=43264, out_features=10, bias=True)\n  )\n)\n\n\n\nfor item in model.state_dict():\n    print(f'{item}        size: {model.state_dict()[item].shape}')\n\nmodel.0.weight        size: torch.Size([32, 3, 3, 3])\nmodel.0.bias        size: torch.Size([32])\nmodel.2.weight        size: torch.Size([64, 32, 3, 3])\nmodel.2.bias        size: torch.Size([64])\nmodel.4.weight        size: torch.Size([64, 64, 3, 3])\nmodel.4.bias        size: torch.Size([64])\nmodel.7.weight        size: torch.Size([10, 43264])\nmodel.7.bias        size: torch.Size([10])\n\n\n\nfor param in model.parameters():\n    print(param.shape)\n\ntorch.Size([32, 3, 3, 3])\ntorch.Size([32])\ntorch.Size([64, 32, 3, 3])\ntorch.Size([64])\ntorch.Size([64, 64, 3, 3])\ntorch.Size([64])\ntorch.Size([10, 43264])\ntorch.Size([10])\n\n\n\nfor child in model.children():\n    print(child)\n\nSequential(\n  (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1))\n  (1): ReLU()\n  (2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n  (3): ReLU()\n  (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n  (5): ReLU()\n  (6): Flatten(start_dim=1, end_dim=-1)\n  (7): Linear(in_features=43264, out_features=10, bias=True)\n)\n\n\n\na = 'sadf(asdf(123(123'\na.split('(', 1)\n\n['sadf', 'asdf(123(123']\n\n\n\n# Iterate through each layer in the model\nfor name, layer in model.named_modules():\n    layer_name = str(layer).split('(', 1)[0]\n    print(f'[name:{name}] layer:{layer_name}')\n\n[name:] layer:ImageClassifier\n[name:model] layer:Sequential\n[name:model.0] layer:Conv2d\n[name:model.1] layer:ReLU\n[name:model.2] layer:Conv2d\n[name:model.3] layer:ReLU\n[name:model.4] layer:Conv2d\n[name:model.5] layer:ReLU\n[name:model.6] layer:Flatten\n[name:model.7] layer:Linear\n\n\n\ndef create_model_diagram(model):\n    from nbdevAuto.functions import graph\n    from graphviz import Digraph\n    dot = graph()\n    dot.attr(rankdir='TB')\n\n    for name, layer in model.named_modules():\n        layer_name = str(layer).split('(', 1)[0]\n        node = f'{name}\\n{layer_name}'\n        dot.node(node, node)\n\n    for index, (name, layer) in enumerate(model.named_modules()):\n        layer_name = str(layer).split('(', 1)[0]\n        node = f'{name}\\n{layer_name}'\n        if index == 0:\n            previous = node\n            continue    \n        \n        dot.edge(previous, node)\n        previous = node\n\n    # Render the graph\n    return dot\n\n\ncreate_model_diagram(model)\n\n\n\n\n\n\n\n\n\nfrom torchviz import make_dot\n\n\n# Define a dummy input tensor\ndummy_input = torch.randn(1, 3, 32, 32)\n\n# Perform a forward pass\noutput = model(dummy_input)\n\n# Visualize the model's forward structure\n# This will create a graph representing the forward pass\ngraph = make_dot(output, params=dict(model.named_parameters()))\ngraph\n\n\n\n\n\n\n\n\n\ngraph.render(\"forward_structure\", format=\"png\", cleanup=True)\n\n'forward_structure.png'",
    "crumbs": [
      "Blog",
      "Model Diagrams"
    ]
  },
  {
    "objectID": "transfer_learning_resnet18.html",
    "href": "transfer_learning_resnet18.html",
    "title": "Transfer Learning",
    "section": "",
    "text": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nimport torchvision\nfrom torchvision import datasets, models, transforms\nfrom torchvision.transforms import ToPILImage\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport copy\nimport time\n\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n\ntorch.cuda.empty_cache()\n\n\nmean = np.array([0.485, 0.456, 0.406])\nstd = np.array([0.229, 0.224, 0.225])\n\n\ndata_transforms = {\n    'train': transforms.Compose([\n        transforms.RandomResizedCrop(224),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize(mean, std)\n    ]),\n    'val': transforms.Compose([\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize(mean, std)\n    ]),\n}\n\n\ndata_dir = 'Data/hymenoptera_data'\nimage_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n                                          data_transforms[x])\n                  for x in ['train', 'val']}\ndataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,\n                                             shuffle=True, num_workers=0)\n              for x in ['train', 'val']}\n\n\ndataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\nclass_names = image_datasets['train'].classes\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(class_names)\n\n['ants', 'bees']\n\n\n\ndef imshow(inp, title):\n    \"\"\"Imshow for Tensor.\"\"\"\n    inp = inp.numpy().transpose((1, 2, 0))\n    inp = std * inp + mean\n    inp = np.clip(inp, 0, 1)\n    plt.imshow(inp)\n    plt.title(title)\n    plt.show()\n\n\n# Get a batch of training data\ninputs, classes = next(iter(dataloaders['train']))\n\n# Make a grid from batch\nout = torchvision.utils.make_grid(inputs)\n\nimshow(out, title=[class_names[x] for x in classes])\n\n\n\n\n\n\n\n\n\ndef train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n    since = time.time()\n\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_acc = 0.0\n\n    for epoch in range(num_epochs):\n        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n        print('-' * 10)\n\n        # Each epoch has a training and validation phase\n        for phase in ['train', 'val']:\n            if phase == 'train':\n                model.train()  # Set model to training mode\n            else:\n                model.eval()   # Set model to evaluate mode\n\n            running_loss = 0.0\n            running_corrects = 0\n\n            # Iterate over data.\n            for inputs, labels in dataloaders[phase]:\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n\n                # forward\n                # track history if only in train\n                with torch.set_grad_enabled(phase == 'train'):\n                    outputs = model(inputs)\n                    _, preds = torch.max(outputs, 1)\n                    loss = criterion(outputs, labels)\n\n                    # backward + optimize only if in training phase\n                    if phase == 'train':\n                        optimizer.zero_grad()\n                        loss.backward()\n                        optimizer.step()\n\n                # statistics\n                running_loss += loss.item() * inputs.size(0)\n                running_corrects += torch.sum(preds == labels.data)\n\n            if phase == 'train':\n                scheduler.step()\n\n            epoch_loss = running_loss / dataset_sizes[phase]\n            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n\n            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n                phase, epoch_loss, epoch_acc))\n\n            # deep copy the model\n            if phase == 'val' and epoch_acc &gt; best_acc:\n                best_acc = epoch_acc\n                best_model_wts = copy.deepcopy(model.state_dict())\n\n        print()\n\n    time_elapsed = time.time() - since\n    print('Training complete in {:.0f}m {:.0f}s'.format(\n        time_elapsed // 60, time_elapsed % 60))\n    print('Best val Acc: {:4f}'.format(best_acc))\n\n    # load best model weights\n    model.load_state_dict(best_model_wts)\n    return model\n\n\nimport timm\n\n\nprint(timm.list_models('resnet*')[:10])\n\n['resnet10t', 'resnet14t', 'resnet18', 'resnet18d', 'resnet26', 'resnet26d', 'resnet26t', 'resnet32ts', 'resnet33ts', 'resnet34']\n\n\n\n# Load ResNet-18 model\nmodel = timm.create_model('resnet18', pretrained=True)\nmodel\n\nResNet(\n  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (act1): ReLU(inplace=True)\n  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (layer1): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act1): ReLU(inplace=True)\n      (aa): Identity()\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act2): ReLU(inplace=True)\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act1): ReLU(inplace=True)\n      (aa): Identity()\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act2): ReLU(inplace=True)\n    )\n  )\n  (layer2): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act1): ReLU(inplace=True)\n      (aa): Identity()\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act2): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act1): ReLU(inplace=True)\n      (aa): Identity()\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act2): ReLU(inplace=True)\n    )\n  )\n  (layer3): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act1): ReLU(inplace=True)\n      (aa): Identity()\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act2): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act1): ReLU(inplace=True)\n      (aa): Identity()\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act2): ReLU(inplace=True)\n    )\n  )\n  (layer4): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act1): ReLU(inplace=True)\n      (aa): Identity()\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act2): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act1): ReLU(inplace=True)\n      (aa): Identity()\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act2): ReLU(inplace=True)\n    )\n  )\n  (global_pool): SelectAdaptivePool2d(pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))\n  (fc): Linear(in_features=512, out_features=1000, bias=True)\n)\n\n\n\n#### Finetuning the convnet ####\n# Load a pretrained model and reset final fully connected layer.\n\nmodel = models.resnet18(pretrained=True)\nmodel\n\n/home/ben/mambaforge/envs/pfast/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/home/ben/mambaforge/envs/pfast/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n\n\nResNet(\n  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace=True)\n  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (layer1): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer2): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer3): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer4): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n  (fc): Linear(in_features=512, out_features=1000, bias=True)\n)\n\n\n\nnum_ftrs = model.fc.in_features\nnum_ftrs\n\n512\n\n\n\n# Here the size of each output sample is set to 2.\n# Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).\nmodel.fc = nn.Linear(model.fc.in_features, 2)\nmodel\n\nResNet(\n  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace=True)\n  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (layer1): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer2): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer3): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer4): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n  (fc): Linear(in_features=512, out_features=2, bias=True)\n)\n\n\n\nmodel = model.to(device)\n\n\ncriterion = nn.CrossEntropyLoss()\n\n# Observe that all parameters are being optimized\noptimizer = optim.SGD(model.parameters(), lr=0.001)\n\n# StepLR Decays the learning rate of each parameter group by gamma every step_size epochs\n# Decay LR by a factor of 0.1 every 7 epochs\n# Learning rate scheduling should be applied after optimizer’s update\n# e.g., you should write your code this way:\n# for epoch in range(100):\n#     train(...)\n#     validate(...)\n#     scheduler.step()\n\nstep_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n\nmodel = train_model(model, criterion, optimizer, step_lr_scheduler, num_epochs=25)\n\nEpoch 0/24\n----------\ntrain Loss: 1.3138 Acc: 0.6270\nval Loss: 2.4487 Acc: 0.4837\n\nEpoch 1/24\n----------\ntrain Loss: 1.1552 Acc: 0.6721\nval Loss: 2.3901 Acc: 0.5033\n\nEpoch 2/24\n----------\ntrain Loss: 0.9656 Acc: 0.7254\nval Loss: 2.0542 Acc: 0.5229\n\nEpoch 3/24\n----------\ntrain Loss: 1.0340 Acc: 0.6762\nval Loss: 1.8399 Acc: 0.5425\n\nEpoch 4/24\n----------\ntrain Loss: 0.8638 Acc: 0.7049\nval Loss: 1.8519 Acc: 0.5686\n\nEpoch 5/24\n----------\ntrain Loss: 0.9689 Acc: 0.6598\nval Loss: 1.9669 Acc: 0.5817\n\nEpoch 6/24\n----------\ntrain Loss: 0.8450 Acc: 0.7008\nval Loss: 1.7546 Acc: 0.6209\n\nEpoch 7/24\n----------\ntrain Loss: 0.9041 Acc: 0.6598\nval Loss: 1.6658 Acc: 0.5948\n\nEpoch 8/24\n----------\ntrain Loss: 0.8533 Acc: 0.6926\nval Loss: 1.6936 Acc: 0.6209\n\nEpoch 9/24\n----------\ntrain Loss: 0.8974 Acc: 0.6393\nval Loss: 1.5477 Acc: 0.6013\n\nEpoch 10/24\n----------\ntrain Loss: 0.9109 Acc: 0.6516\nval Loss: 1.7137 Acc: 0.6078\n\nEpoch 11/24\n----------\ntrain Loss: 0.8369 Acc: 0.6475\nval Loss: 1.7870 Acc: 0.6078\n\nEpoch 12/24\n----------\ntrain Loss: 0.8221 Acc: 0.6844\nval Loss: 1.6008 Acc: 0.6078\n\nEpoch 13/24\n----------\ntrain Loss: 0.7777 Acc: 0.6926\nval Loss: 1.4073 Acc: 0.6340\n\nEpoch 14/24\n----------\ntrain Loss: 0.8776 Acc: 0.6352\nval Loss: 1.6360 Acc: 0.5948\n\nEpoch 15/24\n----------\ntrain Loss: 0.8583 Acc: 0.6639\nval Loss: 1.5304 Acc: 0.6405\n\nEpoch 16/24\n----------\ntrain Loss: 0.7772 Acc: 0.6926\nval Loss: 1.6465 Acc: 0.6275\n\nEpoch 17/24\n----------\ntrain Loss: 0.8548 Acc: 0.6762\nval Loss: 1.7349 Acc: 0.6340\n\nEpoch 18/24\n----------\ntrain Loss: 0.8174 Acc: 0.7008\nval Loss: 1.6733 Acc: 0.6209\n\nEpoch 19/24\n----------\ntrain Loss: 0.7678 Acc: 0.7172\nval Loss: 1.5187 Acc: 0.6209\n\nEpoch 20/24\n----------\ntrain Loss: 0.7592 Acc: 0.7295\nval Loss: 1.6524 Acc: 0.6209\n\nEpoch 21/24\n----------\ntrain Loss: 0.7918 Acc: 0.6926\nval Loss: 1.6008 Acc: 0.6013\n\nEpoch 22/24\n----------\ntrain Loss: 0.8519 Acc: 0.6721\nval Loss: 1.6299 Acc: 0.6013\n\nEpoch 23/24\n----------\ntrain Loss: 0.8987 Acc: 0.6680\nval Loss: 1.7279 Acc: 0.6013\n\nEpoch 24/24\n----------\ntrain Loss: 0.8057 Acc: 0.6680\nval Loss: 1.6766 Acc: 0.6536\n\nTraining complete in 1m 22s\nBest val Acc: 0.653595\n\n\n\n#### ConvNet as fixed feature extractor ####\n# Here, we need to freeze all the network except the final layer.\n# We need to set requires_grad == False to freeze the parameters so that the gradients are not computed in backward()\nmodel_conv = models.resnet18(pretrained=True)\nfor param in model_conv.parameters():\n    param.requires_grad = False\n\n# Parameters of newly constructed modules have requires_grad=True by default\nnum_ftrs = model_conv.fc.in_features\nmodel_conv.fc = nn.Linear(num_ftrs, 2)\n\nmodel_conv = model_conv.to(device)\n\ncriterion = nn.CrossEntropyLoss()\n\n# Observe that only parameters of final layer are being optimized as\n# opposed to before.\noptimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=0.001, momentum=0.9)\n\n# Decay LR by a factor of 0.1 every 7 epochs\nexp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1)\n\nmodel_conv = train_model(model_conv, criterion, optimizer_conv,\n                         exp_lr_scheduler, num_epochs=25)\n\nEpoch 0/24\n----------\ntrain Loss: 0.6815 Acc: 0.6393\nval Loss: 0.2546 Acc: 0.8889\n\nEpoch 1/24\n----------\ntrain Loss: 0.5108 Acc: 0.7541\nval Loss: 0.1813 Acc: 0.9477\n\nEpoch 2/24\n----------\ntrain Loss: 0.4495 Acc: 0.8156\nval Loss: 0.4027 Acc: 0.8235\n\nEpoch 3/24\n----------\ntrain Loss: 0.4571 Acc: 0.7951\nval Loss: 0.2098 Acc: 0.9085\n\nEpoch 4/24\n----------\ntrain Loss: 0.4004 Acc: 0.8320\nval Loss: 0.2219 Acc: 0.9085\n\nEpoch 5/24\n----------\ntrain Loss: 0.4751 Acc: 0.8115\nval Loss: 0.1881 Acc: 0.9477\n\nEpoch 6/24\n----------\ntrain Loss: 0.4426 Acc: 0.8279\nval Loss: 0.2063 Acc: 0.9216\n\nEpoch 7/24\n----------\ntrain Loss: 0.4091 Acc: 0.8238\nval Loss: 0.2144 Acc: 0.9216\n\nEpoch 8/24\n----------\ntrain Loss: 0.3693 Acc: 0.8320\nval Loss: 0.1896 Acc: 0.9216\n\nEpoch 9/24\n----------\ntrain Loss: 0.3681 Acc: 0.8279\nval Loss: 0.1911 Acc: 0.9281\n\nEpoch 10/24\n----------\ntrain Loss: 0.2974 Acc: 0.8689\nval Loss: 0.1962 Acc: 0.9216\n\nEpoch 11/24\n----------\ntrain Loss: 0.3128 Acc: 0.8525\nval Loss: 0.1886 Acc: 0.9346\n\nEpoch 12/24\n----------\ntrain Loss: 0.3634 Acc: 0.8361\nval Loss: 0.1868 Acc: 0.9412\n\nEpoch 13/24\n----------\ntrain Loss: 0.3299 Acc: 0.8484\nval Loss: 0.1979 Acc: 0.9216\n\nEpoch 14/24\n----------\ntrain Loss: 0.3036 Acc: 0.8893\nval Loss: 0.2028 Acc: 0.9216\n\nEpoch 15/24\n----------\ntrain Loss: 0.3533 Acc: 0.8361\nval Loss: 0.1694 Acc: 0.9477\n\nEpoch 16/24\n----------\ntrain Loss: 0.3248 Acc: 0.8525\nval Loss: 0.1838 Acc: 0.9281\n\nEpoch 17/24\n----------\ntrain Loss: 0.3293 Acc: 0.8648\nval Loss: 0.1941 Acc: 0.9216\n\nEpoch 18/24\n----------\ntrain Loss: 0.2718 Acc: 0.8484\nval Loss: 0.1880 Acc: 0.9346\n\nEpoch 19/24\n----------\ntrain Loss: 0.3811 Acc: 0.8074\nval Loss: 0.2232 Acc: 0.9150\n\nEpoch 20/24\n----------\ntrain Loss: 0.3523 Acc: 0.8402\nval Loss: 0.1787 Acc: 0.9346\n\nEpoch 21/24\n----------\ntrain Loss: 0.2430 Acc: 0.8893\nval Loss: 0.2104 Acc: 0.9281\n\nEpoch 22/24\n----------\ntrain Loss: 0.2858 Acc: 0.8730\nval Loss: 0.1836 Acc: 0.9346\n\nEpoch 23/24\n----------\ntrain Loss: 0.3786 Acc: 0.8443\nval Loss: 0.1786 Acc: 0.9412\n\nEpoch 24/24\n----------\ntrain Loss: 0.2935 Acc: 0.8770\nval Loss: 0.1835 Acc: 0.9346\n\nTraining complete in 1m 5s\nBest val Acc: 0.947712\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Blog",
      "Transfer Learning"
    ]
  },
  {
    "objectID": "simple-mnist-nn-from-scratch-numpy.html",
    "href": "simple-mnist-nn-from-scratch-numpy.html",
    "title": "Simple MNIST NN from scratch",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nfrom torchvision import datasets\nfrom tqdm.notebook import tqdm\n\n\n# Import dependencies\nimport torch \nfrom PIL import Image\nfrom torch import nn, save, load\nfrom torch.optim import Adam\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets\nfrom torchvision.transforms import ToTensor\n\n# Get data \ntrain = datasets.MNIST(root=\"data\", download=True, train=True)\n\n\nm_data = train.data.reshape(60000, 28*28)\n\n\nnp_data = np.array(m_data)\ntarget = np.array(train.targets)\n\n\ndata = np.insert(np_data, 0, target, axis=1)\n\n\nnp_data.shape, target.shape, data.shape\n\n((60000, 784), (60000,), (60000, 785))\n\n\n\nm, n = data.shape\nnp.random.shuffle(data) # shuffle before splitting into dev and training sets\n\n\ndata_dev = data[0:1000].T\nY_dev = data_dev[0]\nX_dev = data_dev[1:n]\nX_dev = X_dev / 255.\n\ndata_train = data[1000:m].T\nY_train = data_train[0]\nX_train = data_train[1:n]\nX_train = X_train / 255.\n_,m_train = X_train.shape\n\n\nY_train\n\narray([0, 7, 6, ..., 6, 5, 3], dtype=uint8)\n\n\nOur NN will have a simple two-layer architecture. Input layer \\(a^{[0]}\\) will have 784 units corresponding to the 784 pixels in each 28x28 input image. A hidden layer \\(a^{[1]}\\) will have 10 units with ReLU activation, and finally our output layer \\(a^{[2]}\\) will have 10 units corresponding to the ten digit classes with softmax activation.\nForward propagation\n\\[Z^{[1]} = W^{[1]} X + b^{[1]}\\] \\[A^{[1]} = g_{\\text{ReLU}}(Z^{[1]}))\\] \\[Z^{[2]} = W^{[2]} A^{[1]} + b^{[2]}\\] \\[A^{[2]} = g_{\\text{softmax}}(Z^{[2]})\\]\nBackward propagation\n\\[dZ^{[2]} = A^{[2]} - Y\\] \\[dW^{[2]} = \\frac{1}{m} dZ^{[2]} A^{[1]T}\\] \\[dB^{[2]} = \\frac{1}{m} \\Sigma {dZ^{[2]}}\\] \\[dZ^{[1]} = W^{[2]T} dZ^{[2]} .* g^{[1]\\prime} (z^{[1]})\\] \\[dW^{[1]} = \\frac{1}{m} dZ^{[1]} A^{[0]T}\\] \\[dB^{[1]} = \\frac{1}{m} \\Sigma {dZ^{[1]}}\\]\nParameter updates\n\\[W^{[2]} := W^{[2]} - \\alpha dW^{[2]}\\] \\[b^{[2]} := b^{[2]} - \\alpha db^{[2]}\\] \\[W^{[1]} := W^{[1]} - \\alpha dW^{[1]}\\] \\[b^{[1]} := b^{[1]} - \\alpha db^{[1]}\\]\nVars and shapes\nForward prop\n\n\\(A^{[0]} = X\\): 784 x m\n\\(Z^{[1]} \\sim A^{[1]}\\): 10 x m\n\\(W^{[1]}\\): 10 x 784 (as \\(W^{[1]} A^{[0]} \\sim Z^{[1]}\\))\n\\(B^{[1]}\\): 10 x 1\n\\(Z^{[2]} \\sim A^{[2]}\\): 10 x m\n\\(W^{[1]}\\): 10 x 10 (as \\(W^{[2]} A^{[1]} \\sim Z^{[2]}\\))\n\\(B^{[2]}\\): 10 x 1\n\nBackprop\n\n\\(dZ^{[2]}\\): 10 x m (\\(~A^{[2]}\\))\n\\(dW^{[2]}\\): 10 x 10\n\\(dB^{[2]}\\): 10 x 1\n\\(dZ^{[1]}\\): 10 x m (\\(~A^{[1]}\\))\n\\(dW^{[1]}\\): 10 x 10\n\\(dB^{[1]}\\): 10 x 1\n\n\ndef init_params():\n    W1 = np.random.rand(10, 784) - 0.5\n    b1 = np.random.rand(10, 1) - 0.5\n    W2 = np.random.rand(10, 10) - 0.5\n    b2 = np.random.rand(10, 1) - 0.5\n    return W1, b1, W2, b2\n\ndef ReLU(Z):\n    return np.maximum(Z, 0)\n\ndef softmax(Z):\n    A = np.exp(Z) / sum(np.exp(Z))\n    return A\n    \ndef forward_prop(W1, b1, W2, b2, X):\n    Z1 = W1.dot(X) + b1\n    A1 = ReLU(Z1)\n    Z2 = W2.dot(A1) + b2\n    A2 = softmax(Z2)\n    return Z1, A1, Z2, A2\n\ndef ReLU_deriv(Z):\n    return Z &gt; 0\n\ndef one_hot(Y):\n    one_hot_Y = np.zeros((Y.size, Y.max() + 1))\n    one_hot_Y[np.arange(Y.size), Y] = 1\n    one_hot_Y = one_hot_Y.T\n    return one_hot_Y\n\ndef backward_prop(Z1, A1, Z2, A2, W1, W2, X, Y):\n    one_hot_Y = one_hot(Y)\n    dZ2 = A2 - one_hot_Y\n    dW2 = 1 / m * dZ2.dot(A1.T)\n    db2 = 1 / m * np.sum(dZ2)\n    dZ1 = W2.T.dot(dZ2) * ReLU_deriv(Z1)\n    dW1 = 1 / m * dZ1.dot(X.T)\n    db1 = 1 / m * np.sum(dZ1)\n    return dW1, db1, dW2, db2\n\ndef update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha):\n    W1 = W1 - alpha * dW1\n    b1 = b1 - alpha * db1    \n    W2 = W2 - alpha * dW2  \n    b2 = b2 - alpha * db2    \n    return W1, b1, W2, b2\n\n\ndef get_predictions(A2):\n    return np.argmax(A2, 0)\n\ndef get_accuracy(predictions, Y):\n    print(predictions, Y)\n    return np.sum(predictions == Y) / Y.size\n\ndef gradient_descent(X, Y, alpha, iterations):\n    W1, b1, W2, b2 = init_params()\n    for i in tqdm(range(iterations)):\n        Z1, A1, Z2, A2 = forward_prop(W1, b1, W2, b2, X)\n        dW1, db1, dW2, db2 = backward_prop(Z1, A1, Z2, A2, W1, W2, X, Y)\n        W1, b1, W2, b2 = update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha)\n        if i % 50 == 0:\n            print(\"Iteration: \", i)\n            predictions = get_predictions(A2)\n            print(get_accuracy(predictions, Y))\n    return W1, b1, W2, b2\n\n\nW1, b1, W2, b2 = gradient_descent(X_train, Y_train, 0.20, 500)\n\n\n\n\nIteration:  0\n[0 9 0 ... 0 0 5] [0 7 6 ... 6 5 3]\n0.1644406779661017\nIteration:  50\n[0 7 6 ... 6 0 8] [0 7 6 ... 6 5 3]\n0.6483898305084745\nIteration:  100\n[0 7 6 ... 6 5 7] [0 7 6 ... 6 5 3]\n0.7637118644067796\nIteration:  150\n[0 7 6 ... 6 5 7] [0 7 6 ... 6 5 3]\n0.810322033898305\nIteration:  200\n[0 7 6 ... 6 5 7] [0 7 6 ... 6 5 3]\n0.8343559322033899\nIteration:  250\n[0 7 6 ... 6 5 7] [0 7 6 ... 6 5 3]\n0.8489322033898306\nIteration:  300\n[0 7 6 ... 6 5 7] [0 7 6 ... 6 5 3]\n0.8593050847457627\nIteration:  350\n[0 7 6 ... 6 5 7] [0 7 6 ... 6 5 3]\n0.8668135593220339\nIteration:  400\n[0 7 6 ... 6 5 7] [0 7 6 ... 6 5 3]\n0.8729491525423729\nIteration:  450\n[0 7 6 ... 6 5 7] [0 7 6 ... 6 5 3]\n0.8777457627118644\n\n\n~85% accuracy on training set.\n\ndef make_predictions(X, W1, b1, W2, b2):\n    _, _, _, A2 = forward_prop(W1, b1, W2, b2, X)\n    predictions = get_predictions(A2)\n    return predictions\n\ndef test_prediction(index, W1, b1, W2, b2):\n    current_image = X_train[:, index, None]\n    prediction = make_predictions(X_train[:, index, None], W1, b1, W2, b2)\n    label = Y_train[index]\n    print(\"Prediction: \", prediction)\n    print(\"Label: \", label)\n    \n    current_image = current_image.reshape((28, 28)) * 255\n    plt.gray()\n    plt.imshow(current_image, interpolation='nearest')\n    plt.show()\n\nLet’s look at a couple of examples:\n\ntest_prediction(0, W1, b1, W2, b2)\ntest_prediction(1, W1, b1, W2, b2)\ntest_prediction(2, W1, b1, W2, b2)\ntest_prediction(3, W1, b1, W2, b2)\n\nPrediction:  [0]\nLabel:  0\nPrediction:  [7]\nLabel:  7\nPrediction:  [6]\nLabel:  6\nPrediction:  [7]\nLabel:  7\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFinally, let’s find the accuracy on the dev set:\n\ndev_predictions = make_predictions(X_dev, W1, b1, W2, b2)\nget_accuracy(dev_predictions, Y_dev)\n\n[1 0 1 9 9 0 3 4 5 3 8 0 3 9 1 4 6 7 0 3 7 5 2 1 0 0 2 5 7 0 9 2 5 0 2 8 8\n 5 2 3 2 3 6 5 6 5 6 5 1 7 1 7 5 7 4 9 7 7 1 3 2 5 9 9 6 4 1 6 7 7 1 5 1 6\n 2 5 0 9 7 2 5 3 4 0 7 7 5 0 7 9 6 4 5 3 1 9 0 2 1 2 0 2 5 5 4 1 8 9 1 5 2\n 5 3 4 2 0 2 5 0 0 4 0 3 0 7 0 0 8 4 1 7 6 3 5 0 4 6 6 1 3 8 9 1 5 6 6 1 7\n 6 2 5 8 9 7 6 9 0 7 6 4 7 1 2 9 9 4 2 0 7 1 4 5 8 0 7 9 2 0 4 6 6 4 2 7 7\n 0 4 7 0 9 4 6 7 6 2 4 5 1 8 1 3 2 0 4 4 7 1 2 8 2 2 3 3 7 7 9 5 1 6 1 0 7\n 4 0 6 0 6 1 9 8 7 0 5 4 7 7 5 0 2 6 6 1 7 8 2 9 7 3 5 1 6 2 8 1 0 4 7 6 8\n 6 6 0 7 7 7 0 0 9 6 0 8 6 6 8 6 6 8 8 0 8 7 1 8 4 5 8 2 2 5 0 1 9 0 7 3 0\n 3 9 2 0 6 9 8 9 5 7 7 5 2 7 7 9 3 3 6 8 3 8 5 3 9 1 8 0 4 5 9 9 6 2 1 4 4\n 0 0 9 9 1 6 3 1 7 3 7 4 6 2 9 1 0 3 2 1 1 4 8 1 2 8 4 8 7 6 4 1 3 7 7 0 3\n 0 6 2 6 6 5 2 9 2 7 8 8 0 0 0 7 0 1 2 2 4 0 4 1 5 4 8 0 8 8 8 8 4 5 4 1 9\n 6 5 6 4 3 8 1 3 6 9 6 7 9 9 6 3 5 1 5 1 4 1 1 9 4 0 6 4 3 8 7 9 5 1 7 0 7\n 7 2 3 5 7 8 8 4 7 6 8 1 4 7 8 9 4 3 6 0 4 5 7 4 4 5 8 4 6 5 8 3 1 2 5 5 5\n 6 1 9 7 6 0 3 7 4 1 8 2 4 8 8 1 4 9 7 3 9 2 1 1 8 2 6 7 2 6 6 1 0 0 1 9 3\n 9 8 3 9 6 7 6 8 4 6 3 9 4 8 4 1 9 8 2 5 4 4 3 5 7 1 3 7 1 6 4 6 1 7 5 7 5\n 9 9 6 3 0 1 6 1 8 0 8 8 0 6 2 8 7 3 1 8 6 9 6 7 1 0 3 3 1 3 7 4 9 0 4 9 0\n 3 3 1 4 7 4 0 7 4 0 1 5 2 2 7 2 7 7 5 4 3 2 8 3 5 7 9 3 5 2 0 8 1 9 4 1 4\n 9 9 1 3 1 3 9 7 0 1 3 4 0 5 2 9 8 3 8 9 0 5 1 7 1 5 5 9 6 9 1 1 8 1 1 6 2\n 0 7 1 5 2 2 1 8 2 7 7 1 4 6 0 2 2 1 4 0 6 2 1 2 0 3 6 7 6 3 6 7 9 0 7 7 9\n 7 5 2 2 8 6 7 8 0 3 0 1 4 5 3 6 1 3 9 1 6 1 8 3 7 3 1 5 3 5 0 1 4 0 9 0 0\n 8 5 9 8 0 2 4 9 1 2 1 8 3 6 4 2 4 1 1 2 3 2 7 5 4 2 5 0 6 3 0 4 4 7 9 3 8\n 0 3 6 1 0 1 8 3 6 5 7 0 5 3 3 4 7 7 2 7 1 3 4 7 0 1 2 3 8 4 3 8 0 0 7 6 8\n 6 1 6 2 3 2 8 2 3 1 0 2 8 3 3 0 3 1 2 0 1 1 6 4 0 1 7 9 4 2 8 6 7 9 1 7 7\n 3 1 8 8 8 9 4 0 7 0 3 1 6 1 1 5 4 8 4 7 5 9 0 3 1 0 1 7 6 1 0 6 4 1 8 2 4\n 2 5 5 6 7 8 4 5 7 6 7 2 1 8 1 5 0 3 8 2 1 0 4 1 9 9 5 6 9 6 1 9 2 3 9 9 7\n 6 9 9 7 7 1 0 9 3 3 2 1 8 3 0 8 9 5 3 1 9 1 5 1 3 6 0 7 4 6 1 2 7 9 5 1 2\n 0 3 5 1 7 4 7 3 8 5 6 4 6 1 4 1 6 5 5 5 1 0 0 2 6 2 8 4 0 6 7 7 0 7 6 1 3\n 0] [1 0 1 9 8 0 3 4 5 3 8 0 3 9 1 4 6 7 0 3 2 3 8 1 5 0 2 5 7 0 9 2 6 0 2 6 8\n 5 2 3 2 3 6 5 6 5 6 5 1 7 1 7 8 7 4 9 3 7 1 3 2 3 8 9 6 9 1 6 7 7 1 5 1 6\n 2 5 0 4 7 2 5 3 4 7 7 7 5 0 7 9 6 4 5 3 1 7 0 6 1 2 0 2 5 5 4 1 8 9 1 5 2\n 5 5 9 2 0 7 8 0 0 4 0 3 0 7 5 0 8 4 1 7 6 3 5 2 4 6 6 1 8 8 4 1 5 6 6 1 7\n 6 2 3 8 9 7 6 9 0 7 6 4 7 1 2 9 9 4 2 0 7 1 4 5 8 0 7 9 2 0 4 4 6 4 2 7 7\n 0 4 7 0 7 4 6 7 6 7 4 1 1 8 1 3 2 0 4 4 7 1 2 8 2 2 3 3 7 7 9 5 1 6 1 0 7\n 4 0 6 0 6 1 9 8 9 0 5 4 1 9 5 0 2 6 2 1 7 8 2 9 7 3 5 1 6 2 8 1 0 8 7 6 8\n 6 6 0 7 7 7 8 0 9 6 0 8 6 6 8 6 6 8 2 0 8 7 1 8 4 5 8 2 2 8 2 1 9 0 7 5 7\n 0 9 2 0 6 7 8 9 3 7 7 8 2 7 9 9 3 8 4 4 3 8 5 3 9 2 8 0 6 5 9 9 6 2 1 4 4\n 0 0 9 9 1 6 3 1 7 3 7 4 6 8 9 7 0 3 2 1 1 2 6 1 2 8 4 8 7 6 6 1 3 7 7 0 3\n 0 6 2 6 6 5 2 9 2 7 8 8 0 0 0 7 0 1 2 2 4 0 6 1 5 4 8 0 8 2 8 8 4 3 0 1 9\n 6 5 4 4 3 8 1 3 6 9 6 7 9 9 6 5 8 1 5 1 4 1 1 9 4 2 5 4 3 8 7 9 3 1 7 0 9\n 7 2 3 5 2 8 8 4 9 6 8 1 4 7 8 9 4 3 6 0 4 5 7 4 4 5 8 4 6 5 3 3 1 2 5 5 5\n 6 1 9 7 6 0 3 7 4 1 8 7 4 8 8 1 7 9 3 3 4 2 8 5 8 7 6 7 1 6 6 1 0 2 1 9 3\n 9 8 3 9 5 7 6 4 4 6 3 9 5 8 4 1 9 8 8 3 4 4 3 5 7 1 3 7 1 6 4 6 1 2 5 7 5\n 5 9 6 3 0 1 6 1 8 0 8 8 0 6 2 5 7 3 8 8 6 9 4 7 1 0 3 1 1 3 7 4 9 0 4 9 0\n 3 3 1 4 7 4 0 7 4 0 1 5 2 2 7 2 7 5 3 4 9 2 8 5 5 7 4 3 5 1 0 8 1 9 4 1 4\n 9 4 1 3 1 3 9 7 0 1 3 4 0 7 8 9 8 3 5 9 0 5 1 7 1 8 5 9 6 9 1 1 8 1 1 6 2\n 0 7 1 5 2 2 1 8 2 2 5 1 4 6 0 2 2 1 4 0 6 2 1 2 0 3 6 7 6 3 6 7 9 0 9 7 9\n 7 5 2 2 8 6 7 8 0 3 0 1 4 5 3 6 1 3 4 1 6 4 8 3 7 3 2 5 3 3 0 1 4 0 9 0 0\n 8 3 9 8 0 2 9 9 1 2 1 8 3 6 4 7 4 1 1 2 3 2 7 5 4 2 5 0 6 3 0 4 4 7 9 3 2\n 0 3 6 1 0 8 8 8 6 5 7 0 5 8 8 4 3 7 0 7 1 7 4 7 0 4 7 3 8 4 3 8 0 0 7 2 2\n 6 1 6 2 3 2 8 5 9 1 9 2 8 3 3 0 3 1 2 0 1 1 6 4 0 1 7 9 4 2 8 6 7 9 8 7 9\n 3 1 8 3 8 9 4 0 7 0 3 1 6 1 1 5 5 8 4 7 5 9 0 3 1 0 1 7 6 1 0 6 4 1 8 2 4\n 2 8 8 6 7 8 4 5 7 6 7 6 1 8 1 5 0 3 8 2 1 0 9 1 5 9 5 6 9 6 1 9 2 3 7 9 2\n 6 9 9 7 7 1 0 9 8 3 2 1 5 3 0 8 7 3 3 1 9 1 4 1 3 6 0 7 4 6 1 2 7 9 5 1 3\n 0 3 5 1 7 4 7 3 8 5 6 4 6 1 4 1 6 5 5 5 1 2 0 2 6 2 8 4 0 6 7 7 0 7 6 1 3\n 7]\n\n\n0.856\n\n\nStill 84% accuracy, so our model generalized from the training data pretty well.\n\n\n\n Back to top",
    "crumbs": [
      "Blog",
      "Simple MNIST NN from scratch"
    ]
  },
  {
    "objectID": "resnet18_with_scratch.html",
    "href": "resnet18_with_scratch.html",
    "title": "Resnet 18 from Scratch",
    "section": "",
    "text": "import torch\n\n\ntorch.cuda.is_available()\n\nTrue\n\n\n\nfrom fastbook import search_images_ddg\nfrom fastdownload import download_url\nfrom fastai.vision.all import *\nfrom nbdevAuto import functions\nimport os\nimport shutil\n\n\nimport torch.nn as nn\nimport torch\nfrom torch import Tensor\nfrom typing import Type\n\n\nclass BasicBlock(nn.Module):\n    def __init__(\n        self, \n        in_channels: int,\n        out_channels: int,\n        stride: int = 1,\n        expansion: int = 1,\n        downsample: nn.Module = None\n    ) -&gt; None:\n        super(BasicBlock, self).__init__()\n        # Multiplicative factor for the subsequent conv2d layer's output channels.\n        # It is 1 for ResNet18 and ResNet34.\n        self.expansion = expansion\n        self.downsample = downsample\n        self.conv1 = nn.Conv2d(\n            in_channels, \n            out_channels, \n            kernel_size=3, \n            stride=stride, \n            padding=1,\n            bias=False\n        )\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(\n            out_channels, \n            out_channels*self.expansion, \n            kernel_size=3, \n            padding=1,\n            bias=False\n        )\n        self.bn2 = nn.BatchNorm2d(out_channels*self.expansion)\n    def forward(self, x: Tensor) -&gt; Tensor:\n        identity = x\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n        out = self.conv2(out)\n        out = self.bn2(out)\n        if self.downsample is not None:\n            identity = self.downsample(x)\n        out += identity\n        out = self.relu(out)\n        return  out\n\n\nclass ResNet(nn.Module):\n    def __init__(\n        self, \n        img_channels: int,\n        num_layers: int,\n        block: Type[BasicBlock],\n        num_classes: int  = 1000\n    ) -&gt; None:\n        super(ResNet, self).__init__()\n        if num_layers == 18:\n            # The following `layers` list defines the number of `BasicBlock` \n            # to use to build the network and how many basic blocks to stack\n            # together.\n            layers = [2, 2, 2, 2]\n            self.expansion = 1\n        \n        self.in_channels = 64\n        # All ResNets (18 to 152) contain a Conv2d =&gt; BN =&gt; ReLU for the first\n        # three layers. Here, kernel size is 7.\n        self.conv1 = nn.Conv2d(\n            in_channels=img_channels,\n            out_channels=self.in_channels,\n            kernel_size=7, \n            stride=2,\n            padding=3,\n            bias=False\n        )\n        self.bn1 = nn.BatchNorm2d(self.in_channels)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(512*self.expansion, num_classes)\n    def _make_layer(\n        self, \n        block: Type[BasicBlock],\n        out_channels: int,\n        blocks: int,\n        stride: int = 1\n    ) -&gt; nn.Sequential:\n        downsample = None\n        if stride != 1:\n            \"\"\"\n            This should pass from `layer2` to `layer4` or \n            when building ResNets50 and above. Section 3.3 of the paper\n            Deep Residual Learning for Image Recognition\n            (https://arxiv.org/pdf/1512.03385v1.pdf).\n            \"\"\"\n            downsample = nn.Sequential(\n                nn.Conv2d(\n                    self.in_channels, \n                    out_channels*self.expansion,\n                    kernel_size=1,\n                    stride=stride,\n                    bias=False \n                ),\n                nn.BatchNorm2d(out_channels * self.expansion),\n            )\n        layers = []\n        layers.append(\n            block(\n                self.in_channels, out_channels, stride, self.expansion, downsample\n            )\n        )\n        self.in_channels = out_channels * self.expansion\n        for i in range(1, blocks):\n            layers.append(block(\n                self.in_channels,\n                out_channels,\n                expansion=self.expansion\n            ))\n        return nn.Sequential(*layers)\n    def forward(self, x: Tensor) -&gt; Tensor:\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        # The spatial dimension of the final layer's feature \n        # map should be (7, 7) for all ResNets.\n        #print('Dimensions of the last convolutional feature map: ', x.shape)\n        x = self.avgpool(x)\n        x = torch.flatten(x, 1)\n        x = self.fc(x)\n        return x\n\n\ntensor = torch.rand([1, 3, 224, 224])\nmodel = ResNet(img_channels=3, num_layers=18, block=BasicBlock, num_classes=1000)\nprint(model)\n\nResNet(\n  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace=True)\n  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (layer1): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer2): Sequential(\n    (0): BasicBlock(\n      (downsample): Sequential(\n        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer3): Sequential(\n    (0): BasicBlock(\n      (downsample): Sequential(\n        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer4): Sequential(\n    (0): BasicBlock(\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n  (fc): Linear(in_features=512, out_features=1000, bias=True)\n)\n\n\n\n# Total parameters and trainable parameters.\ntotal_params = sum(p.numel() for p in model.parameters())\nprint(f\"{total_params:,} total parameters.\")\ntotal_trainable_params = sum(\n    p.numel() for p in model.parameters() if p.requires_grad)\nprint(f\"{total_trainable_params:,} training parameters.\")\noutput = model(tensor)\n\n11,689,512 total parameters.\n11,689,512 training parameters.\n\n\n\nimport matplotlib.pyplot as plt\nimport os\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets\nfrom torchvision.transforms import ToTensor\nplt.style.use('ggplot')\ndef get_data(batch_size=64):\n    # CIFAR10 training dataset.\n    dataset_train = datasets.CIFAR10(\n        root='data',\n        train=True,\n        download=True,\n        transform=ToTensor(),\n    )\n    # CIFAR10 validation dataset.\n    dataset_valid = datasets.CIFAR10(\n        root='data',\n        train=False,\n        download=True,\n        transform=ToTensor(),\n    )\n    # Create data loaders.\n    train_loader = DataLoader(\n        dataset_train, \n        batch_size=batch_size,\n        shuffle=True\n    )\n    valid_loader = DataLoader(\n        dataset_valid, \n        batch_size=batch_size,\n        shuffle=False\n    )\n    return train_loader, valid_loader\n\n\ndef save_plots(train_acc, valid_acc, train_loss, valid_loss, name=None):\n    \"\"\"\n    Function to save the loss and accuracy plots to disk.\n    \"\"\"\n    # Accuracy plots.\n    plt.figure(figsize=(10, 7))\n    plt.plot(\n        train_acc, color='tab:blue', linestyle='-', \n        label='train accuracy'\n    )\n    plt.plot(\n        valid_acc, color='tab:red', linestyle='-', \n        label='validataion accuracy'\n    )\n    plt.xlabel('Epochs')\n    plt.ylabel('Accuracy')\n    plt.legend()\n    plt.show()\n\n    \n    # Loss plots.\n    plt.figure(figsize=(10, 7))\n    plt.plot(\n        train_loss, color='tab:blue', linestyle='-', \n        label='train loss'\n    )\n    plt.plot(\n        valid_loss, color='tab:red', linestyle='-', \n        label='validataion loss'\n    )\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.show()\n\n\nimport torch\nfrom tqdm import tqdm\n# Training function.\ndef train(model, trainloader, optimizer, criterion, device):\n    model.train()\n    print('Training')\n    train_running_loss = 0.0\n    train_running_correct = 0\n    counter = 0\n    for i, data in tqdm(enumerate(trainloader), total=len(trainloader)):\n        counter += 1\n        image, labels = data\n        image = image.to(device)\n        labels = labels.to(device)\n        optimizer.zero_grad()\n        # Forward pass.\n        outputs = model(image)\n        # Calculate the loss.\n        loss = criterion(outputs, labels)\n        train_running_loss += loss.item()\n        # Calculate the accuracy.\n        _, preds = torch.max(outputs.data, 1)\n        train_running_correct += (preds == labels).sum().item()\n        # Backpropagation\n        loss.backward()\n        # Update the weights.\n        optimizer.step()\n    \n    # Loss and accuracy for the complete epoch.\n    epoch_loss = train_running_loss / counter\n    # epoch_acc = 100. * (train_running_correct / len(trainloader.dataset))\n    epoch_acc = 100. * (train_running_correct / len(trainloader.dataset))\n    return epoch_loss, epoch_acc\n\n\n# Validation function.\ndef validate(model, testloader, criterion, device):\n    model.eval()\n    print('Validation')\n    valid_running_loss = 0.0\n    valid_running_correct = 0\n    counter = 0\n    with torch.no_grad():\n        for i, data in tqdm(enumerate(testloader), total=len(testloader)):\n            counter += 1\n            \n            image, labels = data\n            image = image.to(device)\n            labels = labels.to(device)\n            # Forward pass.\n            outputs = model(image)\n            # Calculate the loss.\n            loss = criterion(outputs, labels)\n            valid_running_loss += loss.item()\n            # Calculate the accuracy.\n            _, preds = torch.max(outputs.data, 1)\n            valid_running_correct += (preds == labels).sum().item()\n        \n    # Loss and accuracy for the complete epoch.\n    epoch_loss = valid_running_loss / counter\n    epoch_acc = 100. * (valid_running_correct / len(testloader.dataset))\n    return epoch_loss, epoch_acc\n\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport argparse\nimport numpy as np\nimport random\n\n# Set seed.\nseed = 42\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = True\nnp.random.seed(seed)\nrandom.seed(seed)\n\n\n# Learning and training parameters.\nepochs = 20\nbatch_size = 64\nlearning_rate = 0.01\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\ntrain_loader, valid_loader = get_data(batch_size=batch_size)\n# Define model based on the argument parser string.\n\nprint('[INFO]: Training ResNet18 built from scratch...')\nmodel = ResNet(img_channels=3, num_layers=18, block=BasicBlock, num_classes=10).to(device)\nplot_name = 'resnet_scratch'\n\n# print(model)\n# Total parameters and trainable parameters.\ntotal_params = sum(p.numel() for p in model.parameters())\nprint(f\"{total_params:,} total parameters.\")\ntotal_trainable_params = sum(\n    p.numel() for p in model.parameters() if p.requires_grad)\nprint(f\"{total_trainable_params:,} training parameters.\")\n# Optimizer.\noptimizer = optim.SGD(model.parameters(), lr=learning_rate)\n# Loss function.\ncriterion = nn.CrossEntropyLoss()\n\nFiles already downloaded and verified\nFiles already downloaded and verified\n[INFO]: Training ResNet18 built from scratch...\n11,181,642 total parameters.\n11,181,642 training parameters.\n\n\n\n# Lists to keep track of losses and accuracies.\ntrain_loss, valid_loss = [], []\ntrain_acc, valid_acc = [], []\n# Start the training.\nfor epoch in range(epochs):\n    print(f\"[INFO]: Epoch {epoch+1} of {epochs}\")\n    train_epoch_loss, train_epoch_acc = train(\n        model, \n        train_loader, \n        optimizer, \n        criterion,\n        device\n    )\n    valid_epoch_loss, valid_epoch_acc = validate(\n        model, \n        valid_loader, \n        criterion,\n        device\n    )\n    train_loss.append(train_epoch_loss)\n    valid_loss.append(valid_epoch_loss)\n    train_acc.append(train_epoch_acc)\n    valid_acc.append(valid_epoch_acc)\n    print(f\"Training loss: {train_epoch_loss:.3f}, training acc: {train_epoch_acc:.3f}\")\n    print(f\"Validation loss: {valid_epoch_loss:.3f}, validation acc: {valid_epoch_acc:.3f}\")\n    print('-'*50)\n    \n# Save the loss and accuracy plots.\nsave_plots(\n    train_acc, \n    valid_acc, \n    train_loss, \n    valid_loss, \n    name=plot_name\n)\nprint('TRAINING COMPLETE')\n\n[INFO]: Epoch 1 of 20\nTraining\nValidation\nTraining loss: 1.421, training acc: 48.966\nValidation loss: 1.590, validation acc: 45.520\n--------------------------------------------------\n[INFO]: Epoch 2 of 20\nTraining\nValidation\nTraining loss: 1.021, training acc: 63.716\nValidation loss: 1.145, validation acc: 60.090\n--------------------------------------------------\n[INFO]: Epoch 3 of 20\nTraining\nValidation\nTraining loss: 0.834, training acc: 70.488\nValidation loss: 1.317, validation acc: 57.260\n--------------------------------------------------\n[INFO]: Epoch 4 of 20\nTraining\nValidation\nTraining loss: 0.701, training acc: 75.158\nValidation loss: 2.220, validation acc: 46.050\n--------------------------------------------------\n[INFO]: Epoch 5 of 20\nTraining\nValidation\nTraining loss: 0.595, training acc: 78.946\nValidation loss: 2.410, validation acc: 41.760\n--------------------------------------------------\n[INFO]: Epoch 6 of 20\nTraining\nValidation\nTraining loss: 0.495, training acc: 82.466\nValidation loss: 1.990, validation acc: 47.290\n--------------------------------------------------\n[INFO]: Epoch 7 of 20\nTraining\nValidation\nTraining loss: 0.410, training acc: 85.436\nValidation loss: 2.319, validation acc: 45.660\n--------------------------------------------------\n[INFO]: Epoch 8 of 20\nTraining\nValidation\nTraining loss: 0.341, training acc: 87.844\nValidation loss: 1.169, validation acc: 66.930\n--------------------------------------------------\n[INFO]: Epoch 9 of 20\nTraining\nValidation\nTraining loss: 0.272, training acc: 90.396\nValidation loss: 2.214, validation acc: 50.400\n--------------------------------------------------\n[INFO]: Epoch 10 of 20\nTraining\nValidation\nTraining loss: 0.223, training acc: 92.148\nValidation loss: 1.630, validation acc: 61.040\n--------------------------------------------------\n[INFO]: Epoch 11 of 20\nTraining\nValidation\nTraining loss: 0.189, training acc: 93.398\nValidation loss: 1.593, validation acc: 63.560\n--------------------------------------------------\n[INFO]: Epoch 12 of 20\nTraining\nValidation\nTraining loss: 0.157, training acc: 94.440\nValidation loss: 1.967, validation acc: 58.260\n--------------------------------------------------\n[INFO]: Epoch 13 of 20\nTraining\nValidation\nTraining loss: 0.143, training acc: 94.946\nValidation loss: 1.820, validation acc: 61.150\n--------------------------------------------------\n[INFO]: Epoch 14 of 20\nTraining\nValidation\nTraining loss: 0.112, training acc: 96.124\nValidation loss: 1.780, validation acc: 62.380\n--------------------------------------------------\n[INFO]: Epoch 15 of 20\nTraining\nValidation\nTraining loss: 0.087, training acc: 97.016\nValidation loss: 2.463, validation acc: 55.470\n--------------------------------------------------\n[INFO]: Epoch 16 of 20\nTraining\nValidation\nTraining loss: 0.088, training acc: 96.860\nValidation loss: 1.407, validation acc: 70.530\n--------------------------------------------------\n[INFO]: Epoch 17 of 20\nTraining\nValidation\nTraining loss: 0.068, training acc: 97.678\nValidation loss: 1.432, validation acc: 69.650\n--------------------------------------------------\n[INFO]: Epoch 18 of 20\nTraining\nValidation\nTraining loss: 0.062, training acc: 97.840\nValidation loss: 2.301, validation acc: 59.010\n--------------------------------------------------\n[INFO]: Epoch 19 of 20\nTraining\nValidation\nTraining loss: 0.058, training acc: 98.000\nValidation loss: 1.378, validation acc: 72.670\n--------------------------------------------------\n[INFO]: Epoch 20 of 20\nTraining\nValidation\nTraining loss: 0.044, training acc: 98.504\nValidation loss: 1.761, validation acc: 68.020\n--------------------------------------------------\nTRAINING COMPLETE\n\n\n100%|██████████████████████████████████████████████████████████████████████| 782/782 [00:17&lt;00:00, 44.98it/s]\n100%|██████████████████████████████████████████████████████████████████████| 157/157 [00:01&lt;00:00, 83.11it/s]\n100%|██████████████████████████████████████████████████████████████████████| 782/782 [00:15&lt;00:00, 49.09it/s]\n100%|██████████████████████████████████████████████████████████████████████| 157/157 [00:01&lt;00:00, 80.99it/s]\n100%|██████████████████████████████████████████████████████████████████████| 782/782 [00:14&lt;00:00, 54.44it/s]\n100%|██████████████████████████████████████████████████████████████████████| 157/157 [00:01&lt;00:00, 78.70it/s]\n100%|██████████████████████████████████████████████████████████████████████| 782/782 [00:17&lt;00:00, 44.86it/s]\n100%|██████████████████████████████████████████████████████████████████████| 157/157 [00:02&lt;00:00, 73.68it/s]\n100%|██████████████████████████████████████████████████████████████████████| 782/782 [00:17&lt;00:00, 44.30it/s]\n100%|██████████████████████████████████████████████████████████████████████| 157/157 [00:02&lt;00:00, 73.34it/s]\n100%|██████████████████████████████████████████████████████████████████████| 782/782 [00:15&lt;00:00, 49.72it/s]\n100%|██████████████████████████████████████████████████████████████████████| 157/157 [00:02&lt;00:00, 73.03it/s]\n100%|██████████████████████████████████████████████████████████████████████| 782/782 [00:18&lt;00:00, 42.86it/s]\n100%|██████████████████████████████████████████████████████████████████████| 157/157 [00:02&lt;00:00, 77.01it/s]\n100%|██████████████████████████████████████████████████████████████████████| 782/782 [00:15&lt;00:00, 49.12it/s]\n100%|██████████████████████████████████████████████████████████████████████| 157/157 [00:02&lt;00:00, 76.58it/s]\n100%|██████████████████████████████████████████████████████████████████████| 782/782 [00:14&lt;00:00, 52.23it/s]\n100%|██████████████████████████████████████████████████████████████████████| 157/157 [00:02&lt;00:00, 78.26it/s]\n100%|██████████████████████████████████████████████████████████████████████| 782/782 [00:14&lt;00:00, 53.12it/s]\n100%|██████████████████████████████████████████████████████████████████████| 157/157 [00:02&lt;00:00, 76.25it/s]\n100%|██████████████████████████████████████████████████████████████████████| 782/782 [00:14&lt;00:00, 52.97it/s]\n100%|██████████████████████████████████████████████████████████████████████| 157/157 [00:01&lt;00:00, 79.03it/s]\n100%|██████████████████████████████████████████████████████████████████████| 782/782 [00:19&lt;00:00, 39.58it/s]\n100%|██████████████████████████████████████████████████████████████████████| 157/157 [00:02&lt;00:00, 75.93it/s]\n100%|██████████████████████████████████████████████████████████████████████| 782/782 [00:17&lt;00:00, 43.74it/s]\n100%|██████████████████████████████████████████████████████████████████████| 157/157 [00:02&lt;00:00, 72.56it/s]\n100%|██████████████████████████████████████████████████████████████████████| 782/782 [00:15&lt;00:00, 50.41it/s]\n100%|██████████████████████████████████████████████████████████████████████| 157/157 [00:02&lt;00:00, 77.98it/s]\n100%|██████████████████████████████████████████████████████████████████████| 782/782 [00:15&lt;00:00, 51.22it/s]\n100%|██████████████████████████████████████████████████████████████████████| 157/157 [00:01&lt;00:00, 78.64it/s]\n100%|██████████████████████████████████████████████████████████████████████| 782/782 [00:15&lt;00:00, 51.76it/s]\n100%|██████████████████████████████████████████████████████████████████████| 157/157 [00:02&lt;00:00, 72.27it/s]\n100%|██████████████████████████████████████████████████████████████████████| 782/782 [00:16&lt;00:00, 46.56it/s]\n100%|██████████████████████████████████████████████████████████████████████| 157/157 [00:02&lt;00:00, 70.33it/s]\n100%|██████████████████████████████████████████████████████████████████████| 782/782 [00:17&lt;00:00, 45.68it/s]\n100%|██████████████████████████████████████████████████████████████████████| 157/157 [00:02&lt;00:00, 69.50it/s]\n100%|██████████████████████████████████████████████████████████████████████| 782/782 [00:17&lt;00:00, 44.99it/s]\n100%|██████████████████████████████████████████████████████████████████████| 157/157 [00:02&lt;00:00, 67.29it/s]\n100%|██████████████████████████████████████████████████████████████████████| 782/782 [00:15&lt;00:00, 49.71it/s]\n100%|██████████████████████████████████████████████████████████████████████| 157/157 [00:02&lt;00:00, 73.08it/s]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Blog",
      "Resnet 18 from Scratch"
    ]
  },
  {
    "objectID": "evaluation_metrics.html",
    "href": "evaluation_metrics.html",
    "title": "Evaluation Metrics",
    "section": "",
    "text": "Accuracy : is the percentage of correctly predicted examples out of all predictions, formally known as\n\n\\(Accuracy = \\frac{TP + TN}{TP + FP + TN + FN}\\)",
    "crumbs": [
      "Blog",
      "Evaluation Metrics"
    ]
  },
  {
    "objectID": "evaluation_metrics.html#accuracy",
    "href": "evaluation_metrics.html#accuracy",
    "title": "Evaluation Metrics",
    "section": "",
    "text": "Accuracy : is the percentage of correctly predicted examples out of all predictions, formally known as\n\n\\(Accuracy = \\frac{TP + TN}{TP + FP + TN + FN}\\)",
    "crumbs": [
      "Blog",
      "Evaluation Metrics"
    ]
  },
  {
    "objectID": "evaluation_metrics.html#precision",
    "href": "evaluation_metrics.html#precision",
    "title": "Evaluation Metrics",
    "section": "Precision",
    "text": "Precision\n\nPrecision is the the probability of the predicted bounding boxes matching actual ground truth boxes, also referred to as the positive predictive value.\n\n\\(Precision = \\frac{TP}{TP + FP} = \\frac{true \\ object \\ detection}{all \\ detected \\ boxes}\\)",
    "crumbs": [
      "Blog",
      "Evaluation Metrics"
    ]
  },
  {
    "objectID": "evaluation_metrics.html#recall",
    "href": "evaluation_metrics.html#recall",
    "title": "Evaluation Metrics",
    "section": "Recall",
    "text": "Recall\n\nRecall is the true positive rate, also referred to as sensitivity, measures the probability of ground truth objects being correctly detected.\n\n\\(Recall = \\frac{TP}{TP + FN} = \\frac{true \\ object \\ detection}{all \\ ground \\ truth \\ boxes}\\)",
    "crumbs": [
      "Blog",
      "Evaluation Metrics"
    ]
  },
  {
    "objectID": "evaluation_metrics.html#ap---average-precision",
    "href": "evaluation_metrics.html#ap---average-precision",
    "title": "Evaluation Metrics",
    "section": "AP - Average Precision",
    "text": "AP - Average Precision\n\nit is a single number metric that encapsulates both precision and recall and summarizes the Precision-Recall curve by averaging precision across recall values from 0 to 1\n\n\\(AP = \\dfrac{1}{11} \\sum_{r \\in \\{0,0.1,0.2,...,1\\}} p_{interp}(r)\\)\nwhere\n\\(p_{interp}(r) = \\max p(\\hat{r})\\)",
    "crumbs": [
      "Blog",
      "Evaluation Metrics"
    ]
  },
  {
    "objectID": "evaluation_metrics.html#map-mean-average-precision",
    "href": "evaluation_metrics.html#map-mean-average-precision",
    "title": "Evaluation Metrics",
    "section": "mAP (Mean Average Precision)",
    "text": "mAP (Mean Average Precision)\n\ncount the accumulated TP and the accumulated FP and compute the precision/recall at each line. Average Precision is computed as the average precision at 11 equally spaced recall levels.",
    "crumbs": [
      "Blog",
      "Evaluation Metrics"
    ]
  },
  {
    "objectID": "datasets_and_dataloaders.html",
    "href": "datasets_and_dataloaders.html",
    "title": "Dataset and Dataloaders",
    "section": "",
    "text": "import torch\nimport torchvision\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\n\nclass WineDataset(Dataset):\n    def __init__(self):\n        #data loading\n        xy = np.loadtxt('Data/wine.csv', delimiter=\",\", dtype=np.float32, skiprows = 1)\n        self.xy = xy\n        self.x = torch.from_numpy(xy[:,1:])\n        self.y = torch.from_numpy(xy[:,[0]])\n        self.n_samples = xy.shape[0]\n\n    def __getitem__(self, index):\n        return self.x[index], self.y[index]\n\n    def __len__(self):\n        return self.n_samples\ndataset = WineDataset()\nfirst_data = dataset[0]\nfirst_data\n\n(tensor([1.4230e+01, 1.7100e+00, 2.4300e+00, 1.5600e+01, 1.2700e+02, 2.8000e+00,\n         3.0600e+00, 2.8000e-01, 2.2900e+00, 5.6400e+00, 1.0400e+00, 3.9200e+00,\n         1.0650e+03]),\n tensor([1.]))\nfeatures, labels = dataset[0]\nfeatures, labels\n\n(tensor([1.4230e+01, 1.7100e+00, 2.4300e+00, 1.5600e+01, 1.2700e+02, 2.8000e+00,\n         3.0600e+00, 2.8000e-01, 2.2900e+00, 5.6400e+00, 1.0400e+00, 3.9200e+00,\n         1.0650e+03]),\n tensor([1.]))\ndataloader = DataLoader(dataset=dataset, batch_size = 4, shuffle = True, num_workers = 2)\ndataiter = iter(dataloader)\nnext(dataiter)\n\n[tensor([[1.2250e+01, 1.7300e+00, 2.1200e+00, 1.9000e+01, 8.0000e+01, 1.6500e+00,\n          2.0300e+00, 3.7000e-01, 1.6300e+00, 3.4000e+00, 1.0000e+00, 3.1700e+00,\n          5.1000e+02],\n         [1.2420e+01, 1.6100e+00, 2.1900e+00, 2.2500e+01, 1.0800e+02, 2.0000e+00,\n          2.0900e+00, 3.4000e-01, 1.6100e+00, 2.0600e+00, 1.0600e+00, 2.9600e+00,\n          3.4500e+02],\n         [1.2370e+01, 1.1300e+00, 2.1600e+00, 1.9000e+01, 8.7000e+01, 3.5000e+00,\n          3.1000e+00, 1.9000e-01, 1.8700e+00, 4.4500e+00, 1.2200e+00, 2.8700e+00,\n          4.2000e+02],\n         [1.3830e+01, 1.6500e+00, 2.6000e+00, 1.7200e+01, 9.4000e+01, 2.4500e+00,\n          2.9900e+00, 2.2000e-01, 2.2900e+00, 5.6000e+00, 1.2400e+00, 3.3700e+00,\n          1.2650e+03]]),\n tensor([[2.],\n         [2.],\n         [2.],\n         [1.]])]\ndataiter = iter(dataloader)\ndata = next(dataiter)\nfeatures, labels = data\nfeatures, labels\n\n(tensor([[1.3720e+01, 1.4300e+00, 2.5000e+00, 1.6700e+01, 1.0800e+02, 3.4000e+00,\n          3.6700e+00, 1.9000e-01, 2.0400e+00, 6.8000e+00, 8.9000e-01, 2.8700e+00,\n          1.2850e+03],\n         [1.1840e+01, 2.8900e+00, 2.2300e+00, 1.8000e+01, 1.1200e+02, 1.7200e+00,\n          1.3200e+00, 4.3000e-01, 9.5000e-01, 2.6500e+00, 9.6000e-01, 2.5200e+00,\n          5.0000e+02],\n         [1.3860e+01, 1.5100e+00, 2.6700e+00, 2.5000e+01, 8.6000e+01, 2.9500e+00,\n          2.8600e+00, 2.1000e-01, 1.8700e+00, 3.3800e+00, 1.3600e+00, 3.1600e+00,\n          4.1000e+02],\n         [1.4120e+01, 1.4800e+00, 2.3200e+00, 1.6800e+01, 9.5000e+01, 2.2000e+00,\n          2.4300e+00, 2.6000e-01, 1.5700e+00, 5.0000e+00, 1.1700e+00, 2.8200e+00,\n          1.2800e+03]]),\n tensor([[1.],\n         [2.],\n         [2.],\n         [1.]]))\nnum_epochs = 2\ntotal_samples = len(dataset)\nn_iterations = int(np.ceil(total_samples/4))\n\ntotal_samples, n_iterations\n\n(178, 45)\nfor epoch in range(num_epochs):\n    for i, (inputs, labels) in enumerate(dataloader):\n        if (i + 1) % 5 == 0:\n            print(f'epoch {epoch + 1}/{num_epochs}, step {i+1}/{n_iterations}, inputs:{inputs[0][:5]} labels:{labels[0]}')\n\nepoch 1/2, step 5/45, inputs:tensor([13.3600,  2.5600,  2.3500, 20.0000, 89.0000]) labels:tensor([3.])\nepoch 1/2, step 10/45, inputs:tensor([ 13.2900,   1.9700,   2.6800,  16.8000, 102.0000]) labels:tensor([1.])\nepoch 1/2, step 15/45, inputs:tensor([14.1600,  2.5100,  2.4800, 20.0000, 91.0000]) labels:tensor([3.])\nepoch 1/2, step 20/45, inputs:tensor([ 13.9400,   1.7300,   2.2700,  17.4000, 108.0000]) labels:tensor([1.])\nepoch 1/2, step 25/45, inputs:tensor([12.6000,  1.3400,  1.9000, 18.5000, 88.0000]) labels:tensor([2.])\nepoch 1/2, step 30/45, inputs:tensor([ 13.2400,   2.5900,   2.8700,  21.0000, 118.0000]) labels:tensor([1.])\nepoch 1/2, step 35/45, inputs:tensor([11.0300,  1.5100,  2.2000, 21.5000, 85.0000]) labels:tensor([2.])\nepoch 1/2, step 40/45, inputs:tensor([ 13.4800,   1.8100,   2.4100,  20.5000, 100.0000]) labels:tensor([1.])\nepoch 1/2, step 45/45, inputs:tensor([ 12.6400,   1.3600,   2.0200,  16.8000, 100.0000]) labels:tensor([2.])\nepoch 2/2, step 5/45, inputs:tensor([14.7500,  1.7300,  2.3900, 11.4000, 91.0000]) labels:tensor([1.])\nepoch 2/2, step 10/45, inputs:tensor([12.3700,  1.6300,  2.3000, 24.5000, 88.0000]) labels:tensor([2.])\nepoch 2/2, step 15/45, inputs:tensor([ 13.8300,   1.5700,   2.6200,  20.0000, 115.0000]) labels:tensor([1.])\nepoch 2/2, step 20/45, inputs:tensor([12.6900,  1.5300,  2.2600, 20.7000, 80.0000]) labels:tensor([2.])\nepoch 2/2, step 25/45, inputs:tensor([11.4100,  0.7400,  2.5000, 21.0000, 88.0000]) labels:tensor([2.])\nepoch 2/2, step 30/45, inputs:tensor([12.2500,  1.7300,  2.1200, 19.0000, 80.0000]) labels:tensor([2.])\nepoch 2/2, step 35/45, inputs:tensor([ 11.5600,   2.0500,   3.2300,  28.5000, 119.0000]) labels:tensor([2.])\nepoch 2/2, step 40/45, inputs:tensor([ 14.1000,   2.0200,   2.4000,  18.8000, 103.0000]) labels:tensor([1.])\nepoch 2/2, step 45/45, inputs:tensor([ 14.2200,   1.7000,   2.3000,  16.3000, 118.0000]) labels:tensor([1.])",
    "crumbs": [
      "Blog",
      "Dataset and Dataloaders"
    ]
  },
  {
    "objectID": "datasets_and_dataloaders.html#dataset-transform",
    "href": "datasets_and_dataloaders.html#dataset-transform",
    "title": "Dataset and Dataloaders",
    "section": "Dataset Transform",
    "text": "Dataset Transform\n\nTypes of Transform:\n\nOn Images:\n\nCenterCrop, Grayscale, Pad, RandomAffine RandomCrop, RandomHorizontalFlip, RandomRotation Resize, Scale\n\n\n\nOn Tensors:\n\nLinearTransformation, Normalize, RandomErasing\n\n\n\nConversion:\n\nToPILImage: from tensor or ndarray\n\n\nToTensor: from numpy.ndarray or PIL Image\n\n\nimport torch\nimport torchvision\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\n\nclass WineDataset(Dataset):\n    def __init__(self, transform = None):\n        #data loading\n        xy = np.loadtxt('Data/wine.csv', delimiter=\",\", dtype=np.float32, skiprows = 1)\n        self.xy = xy\n        self.x = xy[:,1:]\n        self.y = xy[:,[0]]\n        self.n_samples = xy.shape[0]\n        self.transform = transform\n\n    def __getitem__(self, index):\n        sample = self.x[index], self.y[index]\n\n        if self.transform:\n            sample = self.transform(sample)\n\n        return sample\n\n    def __len__(self):\n        return self.n_samples\n\n\nclass ToTensor():\n    def __call__(self, sample):\n        inputs, targets  = sample\n        return torch.from_numpy(inputs), torch.from_numpy(targets)\n\nclass MulTransform:\n    def __init__(self, factor):\n        self.factor = factor\n\n    def __call__(self, sample):\n        inputs, target = sample\n        inputs *= self.factor\n        return inputs, target\n\n\ncomposed = torchvision.transforms.Compose([ToTensor(), MulTransform(2)])\n\n\ndataset = WineDataset(transform = composed)\n\n\nfirst_data = dataset[0]\nfirst_data\n\n(tensor([2.8460e+01, 3.4200e+00, 4.8600e+00, 3.1200e+01, 2.5400e+02, 5.6000e+00,\n         6.1200e+00, 5.6000e-01, 4.5800e+00, 1.1280e+01, 2.0800e+00, 7.8400e+00,\n         2.1300e+03]),\n tensor([1.]))\n\n\n\nfeatures, labels = dataset[0]\nfeatures, labels\n\n(tensor([5.6920e+01, 6.8400e+00, 9.7200e+00, 6.2400e+01, 5.0800e+02, 1.1200e+01,\n         1.2240e+01, 1.1200e+00, 9.1600e+00, 2.2560e+01, 4.1600e+00, 1.5680e+01,\n         4.2600e+03]),\n tensor([1.]))\n\n\n\ndataloader = DataLoader(dataset=dataset, batch_size = 4, shuffle = True, num_workers = 2)\n\n\ndataiter = iter(dataloader)\n\n\nnext(dataiter)\n\n[tensor([[2.7460e+01, 3.0000e+00, 5.4000e+00, 4.5000e+01, 2.0200e+02, 6.0000e+00,\n          6.5000e+00, 5.8000e-01, 4.7600e+00, 1.1400e+01, 2.3800e+00, 5.4200e+00,\n          2.5700e+03],\n         [2.5440e+01, 3.6200e+00, 4.4000e+00, 3.7600e+01, 1.7200e+02, 4.4000e+00,\n          5.0600e+00, 5.2000e-01, 3.5400e+00, 7.8000e+00, 2.3200e+00, 6.2800e+00,\n          1.4280e+03],\n         [2.8200e+01, 4.0400e+00, 4.8000e+00, 3.7600e+01, 2.0600e+02, 5.5000e+00,\n          5.8400e+00, 6.4000e-01, 4.7600e+00, 1.2400e+01, 2.1400e+00, 5.5000e+00,\n          2.1200e+03],\n         [2.6100e+01, 3.5400e+00, 4.2000e+00, 3.4000e+01, 2.1400e+02, 6.0000e+00,\n          6.0000e+00, 5.6000e-01, 4.0600e+00, 1.0080e+01, 1.7600e+00, 6.7000e+00,\n          1.7700e+03]]),\n tensor([[1.],\n         [2.],\n         [1.],\n         [1.]])]\n\n\n\ndataiter = iter(dataloader)\n\n\ndata = next(dataiter)\nfeatures, labels = data\n\n\nfeatures, labels\n\n(tensor([[2.5200e+01, 2.6800e+00, 3.8000e+00, 3.7000e+01, 1.7600e+02, 2.9000e+00,\n          2.7200e+00, 5.8000e-01, 2.7000e+00, 4.9000e+00, 2.0800e+00, 5.5400e+00,\n          1.1240e+03],\n         [2.6460e+01, 6.6000e+00, 4.5600e+00, 3.7000e+01, 1.9600e+02, 3.6000e+00,\n          1.6600e+00, 1.2200e+00, 3.7400e+00, 2.1040e+01, 1.1200e+00, 3.0200e+00,\n          1.3500e+03],\n         [2.4000e+01, 3.0200e+00, 4.8400e+00, 4.4000e+01, 1.7200e+02, 2.9000e+00,\n          2.5000e+00, 1.0000e+00, 3.2600e+00, 7.2000e+00, 2.1000e+00, 5.3000e+00,\n          9.0000e+02],\n         [2.2820e+01, 1.4800e+00, 5.0000e+00, 4.2000e+01, 1.7600e+02, 4.9600e+00,\n          4.0200e+00, 8.4000e-01, 2.8800e+00, 6.1600e+00, 2.2000e+00, 4.6200e+00,\n          8.6800e+02]]),\n tensor([[2.],\n         [3.],\n         [2.],\n         [2.]]))\n\n\n\nnum_epochs = 2\ntotal_samples = len(dataset)\nn_iterations = int(np.ceil(total_samples/4))\n\ntotal_samples, n_iterations\n\n(178, 45)\n\n\n\nfor epoch in range(num_epochs):\n    for i, (inputs, labels) in enumerate(dataloader):\n        if (i + 1) % 5 == 0:\n            print(f'epoch {epoch + 1}/{num_epochs}, step {i+1}/{n_iterations}, inputs:{inputs[0][:5]} labels:{labels[0]}')\n\nepoch 1/2, step 5/45, inputs:tensor([ 24.0000,   6.8600,   4.0000,  38.0000, 174.0000]) labels:tensor([2.])\nepoch 1/2, step 10/45, inputs:tensor([ 26.9800,   3.3200,   4.4800,  48.0000, 174.0000]) labels:tensor([2.])\nepoch 1/2, step 15/45, inputs:tensor([ 25.4000,   7.1000,   4.7200,  43.0000, 212.0000]) labels:tensor([3.])\nepoch 1/2, step 20/45, inputs:tensor([ 22.9200,   7.4800,   3.6400,  39.0000, 214.0000]) labels:tensor([2.])\nepoch 1/2, step 25/45, inputs:tensor([ 23.2800,   4.1200,   4.9200,  43.2000, 168.0000]) labels:tensor([2.])\nepoch 1/2, step 30/45, inputs:tensor([ 27.4400,   2.8600,   5.0000,  33.4000, 216.0000]) labels:tensor([1.])\nepoch 1/2, step 35/45, inputs:tensor([ 23.3000,   3.3400,   5.2400,  52.0000, 176.0000]) labels:tensor([2.])\nepoch 1/2, step 40/45, inputs:tensor([ 26.3400,   5.1800,   4.7400,  40.0000, 240.0000]) labels:tensor([3.])\nepoch 1/2, step 45/45, inputs:tensor([ 29.5000,   3.4600,   4.7800,  22.8000, 182.0000]) labels:tensor([1.])\nepoch 2/2, step 5/45, inputs:tensor([ 27.1600,   5.1600,   5.3800,  49.0000, 210.0000]) labels:tensor([3.])\nepoch 2/2, step 10/45, inputs:tensor([ 26.1000,   3.5400,   4.2000,  34.0000, 214.0000]) labels:tensor([1.])\nepoch 2/2, step 15/45, inputs:tensor([ 24.1400,   4.3200,   4.3400,  42.0000, 170.0000]) labels:tensor([2.])\nepoch 2/2, step 20/45, inputs:tensor([ 24.7400,   3.2600,   4.6000,  49.0000, 176.0000]) labels:tensor([2.])\nepoch 2/2, step 25/45, inputs:tensor([ 24.5800,   2.8200,   3.9600,  32.0000, 170.0000]) labels:tensor([2.])\nepoch 2/2, step 30/45, inputs:tensor([ 24.7400,   2.2600,   4.3200,  38.0000, 174.0000]) labels:tensor([2.])\nepoch 2/2, step 35/45, inputs:tensor([ 26.5600,   3.2800,   5.6800,  31.0000, 220.0000]) labels:tensor([1.])\nepoch 2/2, step 40/45, inputs:tensor([ 24.7400,   1.8800,   2.7200,  21.2000, 176.0000]) labels:tensor([2.])\nepoch 2/2, step 45/45, inputs:tensor([ 26.3400,  10.3800,   4.6400,  44.0000, 186.0000]) labels:tensor([3.])\n\n\n\nfrom torchvision import datasets\n\n\ndatasets.__all__\n\n('LSUN',\n 'LSUNClass',\n 'ImageFolder',\n 'DatasetFolder',\n 'FakeData',\n 'CocoCaptions',\n 'CocoDetection',\n 'CIFAR10',\n 'CIFAR100',\n 'EMNIST',\n 'FashionMNIST',\n 'QMNIST',\n 'MNIST',\n 'KMNIST',\n 'StanfordCars',\n 'STL10',\n 'SUN397',\n 'SVHN',\n 'PhotoTour',\n 'SEMEION',\n 'Omniglot',\n 'SBU',\n 'Flickr8k',\n 'Flickr30k',\n 'Flowers102',\n 'VOCSegmentation',\n 'VOCDetection',\n 'Cityscapes',\n 'ImageNet',\n 'Caltech101',\n 'Caltech256',\n 'CelebA',\n 'WIDERFace',\n 'SBDataset',\n 'VisionDataset',\n 'USPS',\n 'Kinetics',\n 'HMDB51',\n 'UCF101',\n 'Places365',\n 'Kitti',\n 'INaturalist',\n 'LFWPeople',\n 'LFWPairs',\n 'KittiFlow',\n 'Sintel',\n 'FlyingChairs',\n 'FlyingThings3D',\n 'HD1K',\n 'Food101',\n 'DTD',\n 'FER2013',\n 'GTSRB',\n 'CLEVRClassification',\n 'OxfordIIITPet',\n 'PCAM',\n 'Country211',\n 'FGVCAircraft',\n 'EuroSAT',\n 'RenderedSST2',\n 'Kitti2012Stereo',\n 'Kitti2015Stereo',\n 'CarlaStereo',\n 'Middlebury2014Stereo',\n 'CREStereo',\n 'FallingThingsStereo',\n 'SceneFlowStereo',\n 'SintelStereo',\n 'InStereo2k',\n 'ETH3DStereo',\n 'wrap_dataset_for_transforms_v2',\n 'Imagenette')",
    "crumbs": [
      "Blog",
      "Dataset and Dataloaders"
    ]
  },
  {
    "objectID": "datasets_and_dataloaders.html#mnist",
    "href": "datasets_and_dataloaders.html#mnist",
    "title": "Dataset and Dataloaders",
    "section": "MNIST",
    "text": "MNIST\n\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\nimport torch\n\n\ntorch.cuda.is_available()\n\nTrue\n\n\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n\npath = './Data'\n# Define transforms for preprocessing\ntransform = transforms.Compose([\n    transforms.ToTensor(),          # Convert image to tensor\n    transforms.Normalize((0.5,), (0.5,))  # Normalize image pixel values to range [-1, 1]\n])\n\n# Define batch size for data loader\nbatch_size = 64\n\n# Create train and test datasets\ntrain_dataset = datasets.MNIST(root=path, train=True, download=True, transform=transform)\ntest_dataset = datasets.MNIST(root=path, train=False, download=True, transform=transform)\n\n# Create train and test data loaders\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n\nlen(train_dataset), len(test_dataset)\n\n(60000, 10000)\n\n\n\nimage, label = train_dataset[1]\nplt.imshow(transforms.ToPILImage()(image), cmap='gray')\nplt.axis('off')\nplt.show()\n\n\n\n\n\n\n\n\n\nexamples = iter(train_loader)\n\n\nimages, labels = next(examples)\nimages.shape, labels.shape\n\n(torch.Size([64, 1, 28, 28]), torch.Size([64]))\n\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Define a function to display images\ndef show_images(images, labels, **kwargs):\n    nrows = int(np.ceil(np.sqrt(len(images))))\n    ncols = int(np.ceil(len(images)/nrows))\n        \n    fig, axes = plt.subplots(nrows, ncols, figsize=(12, 12),  **kwargs)\n    # Adjust the spacing between subplots\n    plt.subplots_adjust(wspace=0.1, hspace=0.3)\n    for i, ax in enumerate(axes.flat):\n        # Convert image to numpy array and adjust pixel values\n        img_np = images[i].numpy().transpose((1, 2, 0))\n        img_np = (img_np + 1) / 2  # Adjust pixel values to range [0, 1]\n        \n        # Display image\n        ax.imshow(img_np, cmap='gray')\n        ax.axis('off')\n        ax.set_title(f'Label: {labels[i]}')\n    plt.show()\n\n\nshow_images(images, labels)",
    "crumbs": [
      "Blog",
      "Dataset and Dataloaders"
    ]
  },
  {
    "objectID": "datasets_and_dataloaders.html#cifar10",
    "href": "datasets_and_dataloaders.html#cifar10",
    "title": "Dataset and Dataloaders",
    "section": "Cifar10",
    "text": "Cifar10\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import transforms, datasets\nfrom torch.utils.data import DataLoader\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\ntorch.cuda.is_available()\n\nTrue\n\n\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n\npath = 'Data'\n\n# Define transforms for preprocessing\ntransform = transforms.Compose([\n    transforms.ToTensor(),                       # Convert image to tensor\n    transforms.Normalize((0.5, 0.5, 0.5),       # Normalize image pixel values to range [-1, 1]\n                         (0.5, 0.5, 0.5))\n])\n\n# Define batch size for data loader\n\n\n# Create train and test datasets\ntrain_dataset = datasets.CIFAR10(root=path, train=True, download=True, transform=transform)\ntest_dataset = datasets.CIFAR10(root=path, train=False, download=True, transform=transform)\n\n# Create train and test data loaders\nbatch_size = 64\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\nFiles already downloaded and verified\nFiles already downloaded and verified\n\n\n\nlen(train_dataset), len(test_dataset)\n\n(50000, 10000)\n\n\n\nimages, labels = train_dataset[1]\n\n\ntype(labels)\n\nint\n\n\n\nclasses = train_dataset.class_to_idx\n\n\nclasses = list(train_dataset.class_to_idx)\n\n\nlist(classes)\n\n['airplane',\n 'automobile',\n 'bird',\n 'cat',\n 'deer',\n 'dog',\n 'frog',\n 'horse',\n 'ship',\n 'truck']\n\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Define a function to display images\ndef show_images(images, labels, **kwargs):\n    nrows = int(np.ceil(np.sqrt(len(images))))\n    ncols = int(np.ceil(len(images)/nrows))\n        \n    fig, axes = plt.subplots(nrows, ncols, figsize=(12, 12),  **kwargs)\n    # Adjust the spacing between subplots\n    plt.subplots_adjust(wspace=0.3, hspace=0.3)\n    for i, ax in enumerate(axes.flat):\n        # Convert image to numpy array and adjust pixel values\n        img_np = images[i].numpy().transpose((1, 2, 0))\n        img_np = (img_np + 1) / 2  # Adjust pixel values to range [0, 1]\n        \n        # Display image\n        ax.imshow(img_np)\n        ax.axis('off')\n        ax.set_title(f' {classes[labels[i]]}')\n    plt.show()\n\n\n# Get a batch of images and labels from the data loader\nexamples = iter(train_loader)\nimages, labels = next(examples)\n\n\ntype(labels)\n\ntorch.Tensor\n\n\n\nimages.shape, labels.shape\n\n(torch.Size([64, 3, 32, 32]), torch.Size([64]))\n\n\n\n# Display the images\nshow_images(images, labels)",
    "crumbs": [
      "Blog",
      "Dataset and Dataloaders"
    ]
  },
  {
    "objectID": "datasets_and_dataloaders.html#imagenette",
    "href": "datasets_and_dataloaders.html#imagenette",
    "title": "Dataset and Dataloaders",
    "section": "Imagenette",
    "text": "Imagenette\n\nfrom torchvision.datasets import ImageFolder\nfrom tqdm import tqdm\n\n\n# Define transformation to convert images to tensors\ntransform = transforms.Compose([\n    transforms.Resize(256),     # Resize images to 256x256\n    transforms.CenterCrop(224), # Crop the center 224x224 region\n    transforms.ToTensor()       # Convert images to PyTorch tensors\n])\n\n# Load Imagenette dataset\nimagenette_dataset = ImageFolder(root='Data/Imagenette_depth/imagenette2', transform=transform)\nimagenette_dataset\n\nDataset ImageFolder\n    Number of datapoints: 13394\n    Root location: Data/Imagenette_depth/imagenette2\n    StandardTransform\nTransform: Compose(\n               Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n               CenterCrop(size=(224, 224))\n               ToTensor()\n           )\n\n\n\n# Calculate mean and standard deviation\nloader = torch.utils.data.DataLoader(imagenette_dataset, batch_size=128, shuffle=False)\nmean_list = []\nstd_list = []\n\nfor image, label in tqdm(loader):\n    mean = image.mean(dim=[0, 2, 3])  # Calculate mean across batch, height, and width\n    std = image.std(dim=[0, 2, 3])    # Calculate standard deviation across batch, height, and width\n    mean_list.append(mean)\n    std_list.append(std)\n\n100%|█████████████████████████████████████████████████████████████████████████████████| 105/105 [03:44&lt;00:00,  2.14s/it]\n\n\n\nmean_tensor = torch.stack(mean_list)\nstd_tensor = torch.stack(std_list)\n\n\nmean_tensor.mean(dim=[0]), std_tensor.mean(dim=[0])\n\n(tensor([0.4654, 0.4544, 0.4252]), tensor([0.2761, 0.2679, 0.2839]))\n\n\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import transforms, datasets\nfrom torch.utils.data import DataLoader\n\nfrom tqdm import tqdm \nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\n# Define the transformations to apply to the images\ntransform = transforms.Compose([\n    transforms.Resize((375, 500)),\n    transforms.ToTensor(),\n])\n\n# Download and load the Imagenette dataset\ntrain_dataset = datasets.Imagenette(root='Data',\n                                    split='train',\n                                    # download=True,\n                                    transform=transform,\n                                    )\n\n\n# Download and load the Imagenette dataset\ntest_dataset = datasets.Imagenette(root='Data',\n                                  split='val',\n                                  # download=True,\n                                  transform=transform,\n                                 )\n\nbatch_size = 64\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n\ntrain_dataset.__dict__.keys()\n\ndict_keys(['root', 'transform', 'target_transform', 'transforms', '_split', '_size', '_url', '_md5', '_size_root', '_image_root', 'wnids', 'wnid_to_idx', 'classes', 'class_to_idx', '_samples'])\n\n\n\ntrain_dataset.classes\n\n[('tench', 'Tinca tinca'),\n ('English springer', 'English springer spaniel'),\n ('cassette player',),\n ('chain saw', 'chainsaw'),\n ('church', 'church building'),\n ('French horn', 'horn'),\n ('garbage truck', 'dustcart'),\n ('gas pump', 'gasoline pump', 'petrol pump', 'island dispenser'),\n ('golf ball',),\n ('parachute', 'chute')]\n\n\n\nlen(train_dataset), len(test_dataset)\n\n(9469, 3925)\n\n\n\nimages, labels = train_dataset[10]\nimages.shape\n\ntorch.Size([3, 375, 500])\n\n\n\n# Define a function to display images\ndef show_image(dataset):\n    # Access an image and its label from the dataset\n    image, label = dataset\n    \n    # Convert the image tensor to a NumPy array\n    image_np = image.numpy().transpose((1, 2, 0))\n\n    # Display the image using Matplotlib\n    plt.imshow(image_np)\n    plt.axis('off')\n    plt.title(f' {train_dataset.classes[label][0]}')\n    plt.show()\n    \n   \n# Define a function to display images\ndef show_images(images, labels, **kwargs):\n    nrows = int(np.ceil(np.sqrt(len(images))))\n    ncols = int(np.ceil(len(images)/nrows))\n    \n    fig, axes = plt.subplots(nrows, ncols, figsize=(12, 12),  **kwargs)\n    # Adjust the spacing between subplots\n    plt.subplots_adjust(wspace=0.3, hspace=0.3)\n    for i, ax in enumerate(axes.flat):\n        # Convert image to numpy array and adjust pixel values\n        img_np = images[i].numpy().transpose((1, 2, 0))\n        \n        # Display image\n        ax.imshow(img_np)\n        ax.axis('off')\n        ax.set_title(f' {train_dataset.classes[labels[i]][0]}')\n    plt.show()\n\n\nshow_image(train_dataset[2])\n\n\n\n\n\n\n\n\n\n# Get a batch of images and labels from the data loader\nexamples = iter(train_loader)\nimages, labels = next(examples)\n\n\nshow_images(images, labels)",
    "crumbs": [
      "Blog",
      "Dataset and Dataloaders"
    ]
  },
  {
    "objectID": "datasets_and_dataloaders.html#country211",
    "href": "datasets_and_dataloaders.html#country211",
    "title": "Dataset and Dataloaders",
    "section": "Country211",
    "text": "Country211\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nfrom torchvision import transforms, datasets, utils\nfrom torch.utils.data import DataLoader, Dataset, random_split\n\nimport torch\nimport os\nfrom PIL import Image\n\nfrom tqdm import tqdm \nimport matplotlib.pyplot as plt\nimport timm\nimport numpy as np\nfrom datetime import datetime\n\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice\n\ndevice(type='cuda')\n\n\n\nfrom torchvision import transforms\n\n# Define transforms to apply to the images\ntransform_default = transforms.Compose([\n    transforms.Resize((200, 300)),  # Resize images to a fixed size\n    transforms.ToTensor(),  # Convert images to PyTorch tensors\n    # Add more transformations as needed (e.g., normalization)\n])\n\n# Path to the root directory of the dataset\nroot_dir = 'Data'\n\n# Create datasets\ntrain_dataset = datasets.Country211(root_dir,\n                                   split = 'train',\n                                    transform=transform_default,\n                                    download = False)\nval_dataset = datasets.Country211(root_dir,\n                                   split = 'valid',\n                                    transform=transform_default,\n                                    download = False)\ntest_dataset = datasets.Country211(root_dir,\n                                   split = 'test',\n                                    transform=transform_default,\n                                    download = False)\n\n\nimages, label = train_dataset[1000]\n\n\nimages.shape\n\ntorch.Size([3, 200, 300])\n\n\n\nlen(train_dataset.classes)\n\n211\n\n\n\nlen(train_dataset), len(val_dataset), len(test_dataset)\n\n(31650, 10550, 21100)\n\n\n\ntrain_dataset\n\nDataset Country211\n    Number of datapoints: 31650\n    Root location: Data\n    StandardTransform\nTransform: Compose(\n               Resize(size=(200, 300), interpolation=bilinear, max_size=None, antialias=True)\n               ToTensor()\n           )\n\n\n\ntrain_dataset.__dict__.keys()\n\ndict_keys(['_split', 'root', '_base_folder', 'transform', 'target_transform', 'transforms', 'loader', 'extensions', 'classes', 'class_to_idx', 'samples', 'targets', 'imgs'])\n\n\n\n# Define a function to display images\ndef show_image(dataset):\n    # Access an image and its label from the dataset\n    image, label = dataset\n    # Convert the image tensor to a NumPy array\n    image_np = image.numpy().transpose((1, 2, 0))\n\n    # Display the image using Matplotlib\n    plt.imshow(image_np.clip(0,1))\n    plt.axis('off')\n    plt.title(f' {train_dataset.classes[label]}')\n    plt.show()\n    \n   \n# Define a function to display images\ndef show_images(images, labels, **kwargs):\n    nrows = int(np.ceil(np.sqrt(len(images))))\n    ncols = int(np.ceil(len(images)/nrows))\n    \n    fig, axes = plt.subplots(nrows, ncols, figsize=(12, 12),  **kwargs)\n    # Adjust the spacing between subplots\n    plt.subplots_adjust(wspace=0.3, hspace=0.3)\n    for ax, image, label in zip(axes.flat, images, labels):\n        # Convert image to numpy array and adjust pixel values\n        img_np = image.numpy().transpose((1, 2, 0))\n        \n        # Display image\n        ax.imshow(img_np.clip(0,1))\n        ax.axis('off')\n        ax.set_title(f' {train_dataset.classes[label]}')\n    for ax in axes.flat[len(images):]:\n        ax.axis('off')\n        \n    plt.show()\n\n\nshow_image(train_dataset[6])\n\n\n\n\n\n\n\n\n\ndef loaders(batch_size):\n\n    train_loader = DataLoader(train_dataset,\n                              batch_size=batch_size,\n                              shuffle=True,\n                              num_workers=8)\n    val_loader = DataLoader(val_dataset,\n                             batch_size=batch_size,\n                             shuffle=True, \n                             num_workers=8)\n    test_loader = DataLoader(test_dataset,\n                              batch_size=batch_size,\n                              shuffle=True,\n                              num_workers=8)\n\n    # dataloaders = {'train': train_loader, 'val': test_loader}\n    # dataset_sizes = {'train': len(train_dataset), 'val': len(test_dataset) }\n    return train_loader, val_loader, test_loader\n\n\nbatch_size = 32\ntrain_loader, val_loader, test_loader = loaders(batch_size)\n\n\n# Get a batch of images and labels from the data loader\nexamples = iter(test_loader)\nimages, labels = next(examples)\n# ############## TENSORBOARD ########################\n# img_grid = utils.make_grid(images)\n\n# writer.add_image('Imagenette', img_grid)\n# writer.flush()\n# #sys.exit()\n# ###################################################\n\n\nshow_images(images, labels)\n\n\n\n\n\n\n\n\n\ndef find_mean_std(loader):\n    mean_list = []\n    std_list = []\n    for images, label in tqdm(loader):\n        mean, std = images.mean([0,2,3]), images.std([0,2,3])  \n        mean_list.append(mean)\n        std_list.append(std)\n    \n    mean_tensor = torch.stack(mean_list)\n    std_tensor = torch.stack(std_list)\n    return mean_tensor.mean(dim=[0]), std_tensor.mean(dim=[0])\n\n\ntrain_norm = find_mean_std(train_loader)\n\n100%|█████████████████████████████████████████████████████████████████████████████████| 990/990 [00:32&lt;00:00, 30.55it/s]\n\n\n(tensor([0.4571, 0.4504, 0.4209]), tensor([0.2706, 0.2646, 0.2857]))\n\n\n\nfind_mean_std(val_loader)\n\n100%|█████████████████████████████████████████████████████████████████████████████████| 330/330 [00:11&lt;00:00, 29.80it/s]\n\n\n(tensor([0.4587, 0.4514, 0.4219]), tensor([0.2706, 0.2647, 0.2852]))\n\n\n\nfind_mean_std(test_loader)\n\n100%|█████████████████████████████████████████████████████████████████████████████████| 660/660 [00:25&lt;00:00, 26.12it/s]\n\n\n(tensor([0.4578, 0.4512, 0.4218]), tensor([0.2702, 0.2642, 0.2856]))",
    "crumbs": [
      "Blog",
      "Dataset and Dataloaders"
    ]
  },
  {
    "objectID": "datasets_and_dataloaders.html#kitti",
    "href": "datasets_and_dataloaders.html#kitti",
    "title": "Dataset and Dataloaders",
    "section": "Kitti",
    "text": "Kitti\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import transforms, datasets\nfrom torchvision.transforms import ToPILImage, v2\nfrom torch.utils.data import DataLoader\n\nfrom tqdm import tqdm \nimport matplotlib.pyplot as plt\nimport numpy as np\nimport cv2\n\n\nfrom bokeh.plotting import figure, show\nfrom bokeh.io import output_notebook\n\n# Create a histogram plot\noutput_notebook()\n\n    \n    \n        \n        Loading BokehJS ...\n    \n\n\n\n\n\n\nclass Kitti_v2(datasets.Kitti):\n    def __init__(self, *args, **kwargs):\n        super(Kitti_v2, self).__init__(*args, **kwargs)\n\n\npath = './Data'\nbatch_size = 16\n\n# Define transforms\n# Define transforms for the dataset\ntransform2 = v2.Compose(\n    [\n        v2.ToImage(),\n        # v2.Resize(size = desired_size),  # Resize image\n        v2.RandomPhotometricDistort(p=0.2),\n        # v2.RandomZoomOut(fill={tv_tensors.Image: (123, 117, 104), \"others\": 0}),\n        # # v2.RandomIoUCrop(),\n        v2.RandomHorizontalFlip(p=0.4),\n        # # v2.SanitizeBoundingBoxes(),\n        v2.ToDtype(torch.float32, scale=True),\n    ]\n)\n\n\n# Load KITTI train dataset\ntrain_dataset = Kitti_v2(root=path, train='true', download=True, transform=transform2)\n\n# Load KITTI test dataset\ntest_dataset = Kitti_v2(root=path, train='false', download=True, transform=transform2)\n\n\nsample = train_dataset[1000]\nimg, target = sample\nprint(f\"{type(img) = }\\n{type(target) = }\")\n\ntype(img) = &lt;class 'torchvision.tv_tensors._image.Image'&gt;\ntype(target) = &lt;class 'list'&gt;\n\n\n\ntrain_dataset2 = datasets.wrap_dataset_for_transforms_v2(train_dataset, target_keys=(\"boxes\", \"labels\"))\n\ntest_dataset2 = datasets.wrap_dataset_for_transforms_v2(test_dataset, target_keys=(\"boxes\", \"labels\"))\n\n\nsample = train_dataset2[1000]\nimg, target = sample\nprint(f\"{type(img) = }\\n{type(target) = }\\n{target.keys() = }\")\nprint(f\"{type(target['boxes']) = }\\n{type(target['labels']) = }\")\n\ntype(img) = &lt;class 'torchvision.tv_tensors._image.Image'&gt;\ntype(target) = &lt;class 'dict'&gt;\ntarget.keys() = dict_keys(['boxes', 'labels'])\ntype(target['boxes']) = &lt;class 'torchvision.tv_tensors._bounding_boxes.BoundingBoxes'&gt;\ntype(target['labels']) = &lt;class 'torch.Tensor'&gt;\n\n\n\nbatch_size = 8\n\ntrain_loader = DataLoader(train_dataset2,\n                          batch_size=batch_size,\n                          shuffle=True,\n                          collate_fn=lambda batch: tuple(zip(*batch)),\n                          num_workers = 8)\n\n# Create DataLoader for test dataset\ntest_loader = DataLoader(test_dataset2,\n                         batch_size=batch_size,\n                         shuffle=False,\n                         collate_fn=lambda batch: tuple(zip(*batch)),\n                         num_workers = 8)\n\n\ntrain_dataset\n\nDataset Kitti_v2\n    Number of datapoints: 7481\n    Root location: ./Data\n\n\n\nlen(train_dataset)\n\n7481\n\n\n\nimage, targets = train_dataset[2]\ntype(targets)\n\nlist\n\n\n\nimage\n\n\n\n\n\n\n\n\n\ncar_types = ['Car', 'Van', 'DontCare',\n             'Cyclist', 'Pedestrian', 'Truck',\n             'Tram', 'Misc', 'Person_sitting']\n\n\ndef cv2_show(image_np, label):\n    image_cv2 = cv2.cvtColor((image_np * 255).astype(np.uint8), cv2.COLOR_RGB2BGR)\n    if 'scores' in label:\n        for bbox, item, score in zip(label['boxes'], label['labels'], label['scores']):\n            if score &gt; 0.2:\n                cv2.rectangle(image_cv2,\n                              (int(bbox[0]), int(bbox[1])), (int(bbox[2]), int(bbox[3])),\n                              (0, 255, 0), 2)\n            \n                # Display the label\n                font = cv2.FONT_HERSHEY_SIMPLEX\n                cv2.putText(image_cv2, str(item),\n                            (int(bbox[0]), int(bbox[1]) - 10),\n                            font, 0.5, (0, 255, 0), 2, cv2.LINE_AA)\n    else:\n        for bbox, item in zip(label['boxes'], label['labels']):\n            # Draw the bounding box\n            cv2.rectangle(image_cv2,\n                          (int(bbox[0]), int(bbox[1])), (int(bbox[2]), int(bbox[3])),\n                          (0, 255, 0), 2)\n        \n            # Display the label\n            font = cv2.FONT_HERSHEY_SIMPLEX\n            cv2.putText(image_cv2, str(item),\n                        (int(bbox[0]), int(bbox[1]) - 10),\n                        font, 0.5, (0, 255, 0), 2, cv2.LINE_AA)\n\n    # Convert the image back to RGB format for display with Matplotlib\n    image_rgb = cv2.cvtColor(image_cv2, cv2.COLOR_BGR2RGB)\n    \n    return image_rgb\n\ndef show_image(kitti_dataset):\n    # Access an image and its label from the dataset\n    image, label = kitti_dataset\n    \n    # Convert the image tensor to a NumPy array\n    image_np = image.numpy().transpose((1, 2, 0))\n    image_rgb = cv2_show(image_np, label)\n    # Display the image using Matplotlib\n    plt.imshow(image_rgb)\n    plt.axis('off')\n    plt.show()\n\n# Define a function to display images\ndef show_images(images, labels, **kwargs):\n    nrows = int(np.sqrt(len(images)))\n    ncols = int(np.floor(len(images)/nrows))\n        \n    fig, axes = plt.subplots(nrows, ncols, **kwargs)\n    # Adjust the spacing between subplots\n    plt.subplots_adjust(wspace=0.01, hspace=0.01)\n\n    # Display the image using Matplotlib\n    for ax, image, label in zip(axes.flat, images, labels):\n        # Convert image to numpy array and adjust pixel values\n        image_np = image.numpy().transpose((1, 2, 0))\n        image_rgb = cv2_show(image_np, label)\n        \n        # Display image\n        ax.imshow(image_rgb)\n        ax.axis('off')\n\n    for ax in axes.flat[len(images):]:\n        ax.axis('off')\n    plt.show()\n\n\nimage, label = train_dataset2[18]\nshow_image(train_dataset2[18])\n\n\n\n\n\n\n\n\n\nIter\n\n# Get a batch of images and labels from the data loader\nexamples = iter(train_loader)\nimages, labels = next(examples)\n\n\nshow_images(images, labels, figsize=(15, 5))",
    "crumbs": [
      "Blog",
      "Dataset and Dataloaders"
    ]
  },
  {
    "objectID": "datasets_and_dataloaders.html#coco",
    "href": "datasets_and_dataloaders.html#coco",
    "title": "Dataset and Dataloaders",
    "section": "COCO",
    "text": "COCO\n\nData download\n\nimport fiftyone as fo\nimport fiftyone.zoo as foz\n\n#\n# Only the required images will be downloaded (if necessary).\n# By default, only detections are loaded\n#\n\n\ndataset = foz.load_zoo_dataset(\n    name=\"coco-2017\",\n    dataset_dir= \"Data/coco\",\n    splits=[\"validation\",\"train\"],\n    classes=[\"person\", \"car\"],\n    max_samples=50,\n)\n\nDownloading split 'validation' to 'Data/coco/validation' if necessary\nFound annotations at 'Data/coco/raw/instances_val2017.json'\nSufficient images already downloaded\nExisting download of split 'validation' is sufficient\nDownloading split 'train' to 'Data/coco/train' if necessary\nFound annotations at 'Data/coco/raw/instances_train2017.json'\nSufficient images already downloaded\nExisting download of split 'train' is sufficient\nLoading 'coco-2017' split 'validation'\n 100% |███████████████████| 50/50 [304.1ms elapsed, 0s remaining, 164.4 samples/s]     \nLoading 'coco-2017' split 'train'\n 100% |███████████████████| 50/50 [291.6ms elapsed, 0s remaining, 171.5 samples/s]     \nDataset 'coco-2017-validation-train-50' created\n\n\n\nclasses = dataset.default_classes\n\n\n# Visualize the dataset in the FiftyOne App\nsession = fo.launch_app(dataset)\n\n\n        \n        \n\n\n\nWelcome to\n\n███████╗██╗███████╗████████╗██╗   ██╗ ██████╗ ███╗   ██╗███████╗\n██╔════╝██║██╔════╝╚══██╔══╝╚██╗ ██╔╝██╔═══██╗████╗  ██║██╔════╝\n█████╗  ██║█████╗     ██║    ╚████╔╝ ██║   ██║██╔██╗ ██║█████╗\n██╔══╝  ██║██╔══╝     ██║     ╚██╔╝  ██║   ██║██║╚██╗██║██╔══╝\n██║     ██║██║        ██║      ██║   ╚██████╔╝██║ ╚████║███████╗\n╚═╝     ╚═╝╚═╝        ╚═╝      ╚═╝    ╚═════╝ ╚═╝  ╚═══╝╚══════╝ v0.23.8\n\nIf you're finding FiftyOne helpful, here's how you can get involved:\n\n|\n|  ⭐⭐⭐ Give the project a star on GitHub ⭐⭐⭐\n|  https://github.com/voxel51/fiftyone\n|\n|  🚀🚀🚀 Join the FiftyOne Slack community 🚀🚀🚀\n|  https://slack.voxel51.com\n|\n\n\n\n\nfrom bokeh.plotting import figure, show\nfrom bokeh.io import output_notebook\n\n# Create a histogram plot\noutput_notebook()\n\n    \n    \n        \n        Loading BokehJS ...\n    \n\n\n\n\n\n\n\nDataset and DataLoaders\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom torchvision import transforms, datasets\nfrom torchvision.transforms import ToPILImage, v2\nfrom torch.utils.data import DataLoader\n\nfrom tqdm import tqdm \nimport matplotlib.pyplot as plt\nimport numpy as np\nimport cv2\n\n\nclasses = ['0', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus','train', \n           'truck', 'boat', 'traffic light', 'fire hydrant', '12', 'stop sign',\n           'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n           'elephant', 'bear', 'zebra', 'giraffe', '26', 'backpack', 'umbrella', '29',\n           '30', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard',\n           'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard',\n           'surfboard', 'tennis racket', 'bottle', '45', 'wine glass', 'cup', 'fork',\n           'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange',\n           'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch',\n           'potted plant', 'bed', '66', 'dining table', '68', '69', 'toilet', '71',\n           'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave',\n           'oven', 'toaster', 'sink', 'refrigerator', '83', 'book', 'clock', 'vase',\n           'scissors', 'teddy bear', 'hair drier', 'toothbrush']\n\n\ntrain_path = './Data/coco/train'\nval_path = './Data/coco/validation'\n\n# Define transforms\n# Define transforms for the dataset\ntransform2 = v2.Compose(\n    [\n        v2.ToImage(),\n        # v2.Resize(size = desired_size),  # Resize image\n        # v2.RandomPhotometricDistort(p=0.2),\n        # v2.RandomZoomOut(fill={tv_tensors.Image: (123, 117, 104), \"others\": 0}),\n        # # v2.RandomIoUCrop(),\n        # v2.RandomHorizontalFlip(p=0.4),\n        # # v2.SanitizeBoundingBoxes(),\n        v2.ToDtype(torch.float32, scale=True),\n    ]\n)\n\n\n# Load KITTI train dataset\ntrain_dataset = datasets.CocoDetection(root=f'{train_path}/data',\n                              annFile=f'{train_path}/labels.json', \n                              transform = transform2)\n\n# Load KITTI test dataset\ntest_dataset = datasets.CocoDetection(root=f'{val_path}/data',\n                            annFile=f'{val_path}/labels.json',\n                            transform=transform2)\n\nloading annotations into memory...\nDone (t=0.00s)\ncreating index...\nindex created!\nloading annotations into memory...\nDone (t=0.01s)\ncreating index...\nindex created!\n\n\n\nsample = train_dataset[49]\nimg, target = sample\nprint(f\"{type(img) = }\\n{type(target) = }\")\n\ntype(img) = &lt;class 'torchvision.tv_tensors._image.Image'&gt;\ntype(target) = &lt;class 'list'&gt;\n\n\n\ntrain_dataset2 = datasets.wrap_dataset_for_transforms_v2(train_dataset)\n\ntest_dataset2 = datasets.wrap_dataset_for_transforms_v2(test_dataset)\n\n\nlen(train_dataset2), len(test_dataset2)\n\n(50, 50)\n\n\n\nbatch_size = 2\n\ntrain_loader = DataLoader(train_dataset2,\n                          batch_size=batch_size,\n                          shuffle=True,\n                          collate_fn=lambda batch: tuple(zip(*batch)),\n                          num_workers = 8)\n\n# Create DataLoader for test dataset\ntest_loader = DataLoader(test_dataset2,\n                         batch_size=batch_size,\n                         shuffle=False,\n                         collate_fn=lambda batch: tuple(zip(*batch)),\n                         num_workers = 8)\n\n\nsample = train_dataset2[10]\nimg, target = sample\nprint(f\"{type(img) = }\\n{type(target) = }\\n{target.keys() = }\")\nprint(f\"{type(target['boxes']) = }\\n{type(target['labels']) = }\")\n\ntype(img) = &lt;class 'torchvision.tv_tensors._image.Image'&gt;\ntype(target) = &lt;class 'dict'&gt;\ntarget.keys() = dict_keys(['image_id', 'boxes', 'labels'])\ntype(target['boxes']) = &lt;class 'torchvision.tv_tensors._bounding_boxes.BoundingBoxes'&gt;\ntype(target['labels']) = &lt;class 'torch.Tensor'&gt;\n\n\n\ndef cv2_show(image_np, label):\n    image_cv2 = cv2.cvtColor((image_np * 255).astype(np.uint8), cv2.COLOR_RGB2BGR)\n    if 'scores' in label:\n        for bbox, item, score in zip(label['boxes'], label['labels'], label['scores']):\n            if score &gt; 0.2:\n                cv2.rectangle(image_cv2,\n                              (int(bbox[0]), int(bbox[1])), (int(bbox[2]), int(bbox[3])),\n                              (0, 255, 0), 2)\n            \n                # Display the label\n                font = cv2.FONT_HERSHEY_SIMPLEX\n                cv2.putText(image_cv2, str(item),\n                            (int(bbox[0]), int(bbox[1]) - 10),\n                            font, 0.5, (0, 255, 0), 2, cv2.LINE_AA)\n    else:\n        for bbox, item in zip(label['boxes'], label['labels']):\n            # Draw the bounding box\n            cv2.rectangle(image_cv2,\n                          (int(bbox[0]), int(bbox[1])), (int(bbox[2]), int(bbox[3])),\n                          (0, 255, 0), 2)\n        \n            # Display the label\n            font = cv2.FONT_HERSHEY_SIMPLEX\n            cv2.putText(image_cv2, classes[item],\n                        (int(bbox[0]), int(bbox[1]) - 10),\n                        font, 0.5, (0, 255, 0), 2, cv2.LINE_AA)\n\n    # Convert the image back to RGB format for display with Matplotlib\n    image_rgb = cv2.cvtColor(image_cv2, cv2.COLOR_BGR2RGB)\n    \n    return image_rgb\n\ndef show_image(kitti_dataset):\n    # Access an image and its label from the dataset\n    image, label = kitti_dataset\n    \n    # Convert the image tensor to a NumPy array\n    image_np = image.numpy().transpose((1, 2, 0))\n    image_rgb = cv2_show(image_np, label)\n    # Display the image using Matplotlib\n    plt.imshow(image_rgb)\n    plt.axis('off')\n    plt.show()\n\n# Define a function to display images\ndef show_images(images, labels, **kwargs):\n    nrows = int(np.sqrt(len(images)))\n    ncols = int(np.floor(len(images)/nrows))\n        \n    fig, axes = plt.subplots(nrows, ncols, **kwargs)\n    # Adjust the spacing between subplots\n    plt.subplots_adjust(wspace=0.01, hspace=0.01)\n\n    # Display the image using Matplotlib\n    for ax, image, label in zip(axes.flat, images, labels):\n        # Convert image to numpy array and adjust pixel values\n        image_np = np.asarray(image).transpose((1, 2, 0))\n        image_rgb = cv2_show(image_np, label)\n        \n        # Display image\n        ax.imshow(image_rgb)\n        ax.axis('off')\n\n    for ax in axes.flat[len(images):]:\n        ax.axis('off')\n    plt.show()\n\n\nshow_image(train_dataset2[18])\n\n\n\n\n\n\n\n\n\nIter\n\n# Get a batch of images and labels from the data loader\nexamples = iter(train_loader)\nimages, labels = next(examples)\n\n\nshow_images(images, labels, figsize=(15, 5))",
    "crumbs": [
      "Blog",
      "Dataset and Dataloaders"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DeepLearning",
    "section": "",
    "text": "use of any dataset - cifar, imagenet, kitti, nyu-dataset, Cityscapes\nuse of any model - resnet18, ViT, UNet, midas (depth)\nmodels for detection - faster R-CNN, YOLO, DETR, DINO\ndifferent applications - object classification, object detection, object segmentation\nmeasure change in performance",
    "crumbs": [
      "Blog",
      "DeepLearning"
    ]
  },
  {
    "objectID": "index.html#dimensions",
    "href": "index.html#dimensions",
    "title": "DeepLearning",
    "section": "",
    "text": "use of any dataset - cifar, imagenet, kitti, nyu-dataset, Cityscapes\nuse of any model - resnet18, ViT, UNet, midas (depth)\nmodels for detection - faster R-CNN, YOLO, DETR, DINO\ndifferent applications - object classification, object detection, object segmentation\nmeasure change in performance",
    "crumbs": [
      "Blog",
      "DeepLearning"
    ]
  },
  {
    "objectID": "resnet18_with_fastai.html",
    "href": "resnet18_with_fastai.html",
    "title": "Resnet 18 from FastAI",
    "section": "",
    "text": "import torch\n\n\ntorch.cuda.is_available()\n\nTrue\n\n\n\nfrom fastbook import *\nfrom fastai.vision.all import *\nimport torchvision.transforms as transforms\nfrom torchvision import datasets\n\n\nfrom fastai.vision.all import *\n\n# Load CIFAR-10 dataset\npath = untar_data(URLs.CIFAR)\n\n# Define DataBlock\ndblock = DataBlock(blocks=(ImageBlock, CategoryBlock),\n                   get_items=get_image_files,\n                   get_y=parent_label,\n                   splitter=GrandparentSplitter(train_name='train', valid_name='test'),\n                   item_tfms=Resize(32))\n\n# Create DataLoaders\ndls = dblock.dataloaders(path, num_workers=4)\ndls.show_batch(max_n=6)\n\n\n\n\n\n\n\n\n\n# Create Learner\nlearn = vision_learner(dls, resnet18, metrics=error_rate)\nlearn.fine_tune(3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n1.684787\n1.488868\n0.521500\n00:38\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.971654\n0.850866\n0.293200\n00:39\n\n\n1\n0.714961\n0.687398\n0.235600\n00:40\n\n\n2\n0.522026\n0.672022\n0.228400\n00:40\n\n\n\n\n\n\n# Create Learner\nlearn = vision_learner(dls, resnet34, metrics=error_rate)\nlearn.fine_tune(3)\n\n/home/ben/mambaforge/envs/cfast/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n1.629048\n1.444631\n0.497000\n00:41\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.899234\n0.774175\n0.266900\n00:54\n\n\n1\n0.652826\n0.635258\n0.216300\n00:57\n\n\n2\n0.414785\n0.603115\n0.205500\n00:55\n\n\n\n\n\n\nlearn.model\n\nSequential(\n  (0): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n    (4): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (2): BasicBlock(\n        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (5): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (downsample): Sequential(\n          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (2): BasicBlock(\n        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (3): BasicBlock(\n        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (6): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (downsample): Sequential(\n          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (2): BasicBlock(\n        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (3): BasicBlock(\n        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (4): BasicBlock(\n        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (5): BasicBlock(\n        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (7): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (downsample): Sequential(\n          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (2): BasicBlock(\n        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n  )\n  (1): Sequential(\n    (0): AdaptiveConcatPool2d(\n      (ap): AdaptiveAvgPool2d(output_size=1)\n      (mp): AdaptiveMaxPool2d(output_size=1)\n    )\n    (1): fastai.layers.Flatten(full=False)\n    (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (3): Dropout(p=0.25, inplace=False)\n    (4): Linear(in_features=1024, out_features=512, bias=False)\n    (5): ReLU(inplace=True)\n    (6): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (7): Dropout(p=0.5, inplace=False)\n    (8): Linear(in_features=512, out_features=10, bias=False)\n  )\n)\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Blog",
      "Resnet 18 from FastAI"
    ]
  }
]